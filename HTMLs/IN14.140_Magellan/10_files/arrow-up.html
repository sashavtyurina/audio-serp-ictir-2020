<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>images: Topics by Science.gov</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!--  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css"> -->
<!--  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script> -->
<!--  <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script> -->
  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
  
  <script>
     var lastDiv = "" ;
  </script>
  <style type="text/css" media="screen">
    .hiddenDiv {
      display: none;
    }
    .visibleDiv {
      display: block;
      border: none;
    }
    ol {
      margin-left: -20px;
    }
    span.hlt {
      color: #990000;
    }
    .result-class>li {
      margin-bottom: 22px;
    }
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2017 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */

/*!
 * Generated using the Bootstrap Customizer (http://getbootstrap.com/customize/?id=f6cb80b000bbb17f18e0769c5e4ad209)
 * Config saved to config.json and https://gist.github.com/f6cb80b000bbb17f18e0769c5e4ad209
 *//*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 *//*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{font-size:2em;margin:0.67em 0}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{color:inherit;font:inherit;margin:0}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield;-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{border:1px solid #c0c0c0;margin:0 2px;padding:0.35em 0.625em 0.75em}legend{border:0;padding:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}*{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}*:before,*:after{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}html{font-size:10px;-webkit-tap-highlight-color:rgba(0,0,0,0)}body{font-family:"Helvetica Neue",Helvetica,Arial,sans-serif;font-size:14px;line-height:1.42857143;color:#333;background-color:#fff}input,button,select,textarea{font-family:inherit;font-size:inherit;line-height:inherit}a{color:#337ab7;text-decoration:none}a:hover,a:focus{color:#23527c;text-decoration:underline}a:focus{outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}figure{margin:0}img{vertical-align:middle}.img-responsive{display:block;max-width:100%;height:auto}.img-rounded{border-radius:6px}.img-thumbnail{padding:4px;line-height:1.42857143;background-color:#fff;border:1px solid #ddd;border-radius:4px;-webkit-transition:all .2s ease-in-out;-o-transition:all .2s ease-in-out;transition:all .2s ease-in-out;display:inline-block;max-width:100%;height:auto}.img-circle{border-radius:50%}hr{margin-top:20px;margin-bottom:20px;border:0;border-top:1px solid #eee}.sr-only{position:absolute;width:1px;height:1px;margin:-1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);border:0}.sr-only-focusable:active,.sr-only-focusable:focus{position:static;width:auto;height:auto;margin:0;overflow:visible;clip:auto}[role="button"]{cursor:pointer}.container{margin-right:auto;margin-left:auto;padding-left:15px;padding-right:15px}@media (min-width:768px){.container{width:750px}}@media (min-width:992px){.container{width:970px}}@media (min-width:1200px){.container{width:1170px}}.container-fluid{margin-right:auto;margin-left:auto;padding-left:15px;padding-right:15px}.row{margin-left:-15px;margin-right:-15px}.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12{position:relative;min-height:1px;padding-left:15px;padding-right:15px}.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12{float:left}.col-xs-12{width:100%}.col-xs-11{width:91.66666667%}.col-xs-10{width:83.33333333%}.col-xs-9{width:75%}.col-xs-8{width:66.66666667%}.col-xs-7{width:58.33333333%}.col-xs-6{width:50%}.col-xs-5{width:41.66666667%}.col-xs-4{width:33.33333333%}.col-xs-3{width:25%}.col-xs-2{width:16.66666667%}.col-xs-1{width:8.33333333%}.col-xs-pull-12{right:100%}.col-xs-pull-11{right:91.66666667%}.col-xs-pull-10{right:83.33333333%}.col-xs-pull-9{right:75%}.col-xs-pull-8{right:66.66666667%}.col-xs-pull-7{right:58.33333333%}.col-xs-pull-6{right:50%}.col-xs-pull-5{right:41.66666667%}.col-xs-pull-4{right:33.33333333%}.col-xs-pull-3{right:25%}.col-xs-pull-2{right:16.66666667%}.col-xs-pull-1{right:8.33333333%}.col-xs-pull-0{right:auto}.col-xs-push-12{left:100%}.col-xs-push-11{left:91.66666667%}.col-xs-push-10{left:83.33333333%}.col-xs-push-9{left:75%}.col-xs-push-8{left:66.66666667%}.col-xs-push-7{left:58.33333333%}.col-xs-push-6{left:50%}.col-xs-push-5{left:41.66666667%}.col-xs-push-4{left:33.33333333%}.col-xs-push-3{left:25%}.col-xs-push-2{left:16.66666667%}.col-xs-push-1{left:8.33333333%}.col-xs-push-0{left:auto}.col-xs-offset-12{margin-left:100%}.col-xs-offset-11{margin-left:91.66666667%}.col-xs-offset-10{margin-left:83.33333333%}.col-xs-offset-9{margin-left:75%}.col-xs-offset-8{margin-left:66.66666667%}.col-xs-offset-7{margin-left:58.33333333%}.col-xs-offset-6{margin-left:50%}.col-xs-offset-5{margin-left:41.66666667%}.col-xs-offset-4{margin-left:33.33333333%}.col-xs-offset-3{margin-left:25%}.col-xs-offset-2{margin-left:16.66666667%}.col-xs-offset-1{margin-left:8.33333333%}.col-xs-offset-0{margin-left:0}@media (min-width:768px){.col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12{float:left}.col-sm-12{width:100%}.col-sm-11{width:91.66666667%}.col-sm-10{width:83.33333333%}.col-sm-9{width:75%}.col-sm-8{width:66.66666667%}.col-sm-7{width:58.33333333%}.col-sm-6{width:50%}.col-sm-5{width:41.66666667%}.col-sm-4{width:33.33333333%}.col-sm-3{width:25%}.col-sm-2{width:16.66666667%}.col-sm-1{width:8.33333333%}.col-sm-pull-12{right:100%}.col-sm-pull-11{right:91.66666667%}.col-sm-pull-10{right:83.33333333%}.col-sm-pull-9{right:75%}.col-sm-pull-8{right:66.66666667%}.col-sm-pull-7{right:58.33333333%}.col-sm-pull-6{right:50%}.col-sm-pull-5{right:41.66666667%}.col-sm-pull-4{right:33.33333333%}.col-sm-pull-3{right:25%}.col-sm-pull-2{right:16.66666667%}.col-sm-pull-1{right:8.33333333%}.col-sm-pull-0{right:auto}.col-sm-push-12{left:100%}.col-sm-push-11{left:91.66666667%}.col-sm-push-10{left:83.33333333%}.col-sm-push-9{left:75%}.col-sm-push-8{left:66.66666667%}.col-sm-push-7{left:58.33333333%}.col-sm-push-6{left:50%}.col-sm-push-5{left:41.66666667%}.col-sm-push-4{left:33.33333333%}.col-sm-push-3{left:25%}.col-sm-push-2{left:16.66666667%}.col-sm-push-1{left:8.33333333%}.col-sm-push-0{left:auto}.col-sm-offset-12{margin-left:100%}.col-sm-offset-11{margin-left:91.66666667%}.col-sm-offset-10{margin-left:83.33333333%}.col-sm-offset-9{margin-left:75%}.col-sm-offset-8{margin-left:66.66666667%}.col-sm-offset-7{margin-left:58.33333333%}.col-sm-offset-6{margin-left:50%}.col-sm-offset-5{margin-left:41.66666667%}.col-sm-offset-4{margin-left:33.33333333%}.col-sm-offset-3{margin-left:25%}.col-sm-offset-2{margin-left:16.66666667%}.col-sm-offset-1{margin-left:8.33333333%}.col-sm-offset-0{margin-left:0}}@media (min-width:992px){.col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12{float:left}.col-md-12{width:100%}.col-md-11{width:91.66666667%}.col-md-10{width:83.33333333%}.col-md-9{width:75%}.col-md-8{width:66.66666667%}.col-md-7{width:58.33333333%}.col-md-6{width:50%}.col-md-5{width:41.66666667%}.col-md-4{width:33.33333333%}.col-md-3{width:25%}.col-md-2{width:16.66666667%}.col-md-1{width:8.33333333%}.col-md-pull-12{right:100%}.col-md-pull-11{right:91.66666667%}.col-md-pull-10{right:83.33333333%}.col-md-pull-9{right:75%}.col-md-pull-8{right:66.66666667%}.col-md-pull-7{right:58.33333333%}.col-md-pull-6{right:50%}.col-md-pull-5{right:41.66666667%}.col-md-pull-4{right:33.33333333%}.col-md-pull-3{right:25%}.col-md-pull-2{right:16.66666667%}.col-md-pull-1{right:8.33333333%}.col-md-pull-0{right:auto}.col-md-push-12{left:100%}.col-md-push-11{left:91.66666667%}.col-md-push-10{left:83.33333333%}.col-md-push-9{left:75%}.col-md-push-8{left:66.66666667%}.col-md-push-7{left:58.33333333%}.col-md-push-6{left:50%}.col-md-push-5{left:41.66666667%}.col-md-push-4{left:33.33333333%}.col-md-push-3{left:25%}.col-md-push-2{left:16.66666667%}.col-md-push-1{left:8.33333333%}.col-md-push-0{left:auto}.col-md-offset-12{margin-left:100%}.col-md-offset-11{margin-left:91.66666667%}.col-md-offset-10{margin-left:83.33333333%}.col-md-offset-9{margin-left:75%}.col-md-offset-8{margin-left:66.66666667%}.col-md-offset-7{margin-left:58.33333333%}.col-md-offset-6{margin-left:50%}.col-md-offset-5{margin-left:41.66666667%}.col-md-offset-4{margin-left:33.33333333%}.col-md-offset-3{margin-left:25%}.col-md-offset-2{margin-left:16.66666667%}.col-md-offset-1{margin-left:8.33333333%}.col-md-offset-0{margin-left:0}}@media (min-width:1200px){.col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12{float:left}.col-lg-12{width:100%}.col-lg-11{width:91.66666667%}.col-lg-10{width:83.33333333%}.col-lg-9{width:75%}.col-lg-8{width:66.66666667%}.col-lg-7{width:58.33333333%}.col-lg-6{width:50%}.col-lg-5{width:41.66666667%}.col-lg-4{width:33.33333333%}.col-lg-3{width:25%}.col-lg-2{width:16.66666667%}.col-lg-1{width:8.33333333%}.col-lg-pull-12{right:100%}.col-lg-pull-11{right:91.66666667%}.col-lg-pull-10{right:83.33333333%}.col-lg-pull-9{right:75%}.col-lg-pull-8{right:66.66666667%}.col-lg-pull-7{right:58.33333333%}.col-lg-pull-6{right:50%}.col-lg-pull-5{right:41.66666667%}.col-lg-pull-4{right:33.33333333%}.col-lg-pull-3{right:25%}.col-lg-pull-2{right:16.66666667%}.col-lg-pull-1{right:8.33333333%}.col-lg-pull-0{right:auto}.col-lg-push-12{left:100%}.col-lg-push-11{left:91.66666667%}.col-lg-push-10{left:83.33333333%}.col-lg-push-9{left:75%}.col-lg-push-8{left:66.66666667%}.col-lg-push-7{left:58.33333333%}.col-lg-push-6{left:50%}.col-lg-push-5{left:41.66666667%}.col-lg-push-4{left:33.33333333%}.col-lg-push-3{left:25%}.col-lg-push-2{left:16.66666667%}.col-lg-push-1{left:8.33333333%}.col-lg-push-0{left:auto}.col-lg-offset-12{margin-left:100%}.col-lg-offset-11{margin-left:91.66666667%}.col-lg-offset-10{margin-left:83.33333333%}.col-lg-offset-9{margin-left:75%}.col-lg-offset-8{margin-left:66.66666667%}.col-lg-offset-7{margin-left:58.33333333%}.col-lg-offset-6{margin-left:50%}.col-lg-offset-5{margin-left:41.66666667%}.col-lg-offset-4{margin-left:33.33333333%}.col-lg-offset-3{margin-left:25%}.col-lg-offset-2{margin-left:16.66666667%}.col-lg-offset-1{margin-left:8.33333333%}.col-lg-offset-0{margin-left:0}}fieldset{padding:0;margin:0;border:0;min-width:0}legend{display:block;width:100%;padding:0;margin-bottom:20px;font-size:21px;line-height:inherit;color:#333;border:0;border-bottom:1px solid #e5e5e5}label{display:inline-block;max-width:100%;margin-bottom:5px;font-weight:bold}input[type="search"]{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}input[type="radio"],input[type="checkbox"]{margin:4px 0 0;margin-top:1px \9;line-height:normal}input[type="file"]{display:block}input[type="range"]{display:block;width:100%}select[multiple],select[size]{height:auto}input[type="file"]:focus,input[type="radio"]:focus,input[type="checkbox"]:focus{outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}output{display:block;padding-top:7px;font-size:14px;line-height:1.42857143;color:#555}.form-control{display:block;width:100%;height:34px;padding:6px 12px;font-size:14px;line-height:1.42857143;color:#555;background-color:#fff;background-image:none;border:1px solid #ccc;border-radius:4px;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,0.075);box-shadow:inset 0 1px 1px rgba(0,0,0,0.075);-webkit-transition:border-color ease-in-out .15s, -webkit-box-shadow ease-in-out .15s;-o-transition:border-color ease-in-out .15s, box-shadow ease-in-out .15s;transition:border-color ease-in-out .15s, box-shadow ease-in-out .15s}.form-control:focus{border-color:#66afe9;outline:0;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);box-shadow:inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6)}.form-control::-moz-placeholder{color:#999;opacity:1}.form-control:-ms-input-placeholder{color:#999}.form-control::-webkit-input-placeholder{color:#999}.form-control::-ms-expand{border:0;background-color:transparent}.form-control[disabled],.form-control[readonly],fieldset[disabled] .form-control{background-color:#eee;opacity:1}.form-control[disabled],fieldset[disabled] .form-control{cursor:not-allowed}textarea.form-control{height:auto}input[type="search"]{-webkit-appearance:none}@media screen and (-webkit-min-device-pixel-ratio:0){input[type="date"].form-control,input[type="time"].form-control,input[type="datetime-local"].form-control,input[type="month"].form-control{line-height:34px}input[type="date"].input-sm,input[type="time"].input-sm,input[type="datetime-local"].input-sm,input[type="month"].input-sm,.input-group-sm input[type="date"],.input-group-sm input[type="time"],.input-group-sm input[type="datetime-local"],.input-group-sm input[type="month"]{line-height:30px}input[type="date"].input-lg,input[type="time"].input-lg,input[type="datetime-local"].input-lg,input[type="month"].input-lg,.input-group-lg input[type="date"],.input-group-lg input[type="time"],.input-group-lg input[type="datetime-local"],.input-group-lg input[type="month"]{line-height:46px}}.form-group{margin-bottom:15px}.radio,.checkbox{position:relative;display:block;margin-top:10px;margin-bottom:10px}.radio label,.checkbox label{min-height:20px;padding-left:20px;margin-bottom:0;font-weight:normal;cursor:pointer}.radio input[type="radio"],.radio-inline input[type="radio"],.checkbox input[type="checkbox"],.checkbox-inline input[type="checkbox"]{position:absolute;margin-left:-20px;margin-top:4px \9}.radio+.radio,.checkbox+.checkbox{margin-top:-5px}.radio-inline,.checkbox-inline{position:relative;display:inline-block;padding-left:20px;margin-bottom:0;vertical-align:middle;font-weight:normal;cursor:pointer}.radio-inline+.radio-inline,.checkbox-inline+.checkbox-inline{margin-top:0;margin-left:10px}input[type="radio"][disabled],input[type="checkbox"][disabled],input[type="radio"].disabled,input[type="checkbox"].disabled,fieldset[disabled] input[type="radio"],fieldset[disabled] input[type="checkbox"]{cursor:not-allowed}.radio-inline.disabled,.checkbox-inline.disabled,fieldset[disabled] .radio-inline,fieldset[disabled] .checkbox-inline{cursor:not-allowed}.radio.disabled label,.checkbox.disabled label,fieldset[disabled] .radio label,fieldset[disabled] .checkbox label{cursor:not-allowed}.form-control-static{padding-top:7px;padding-bottom:7px;margin-bottom:0;min-height:34px}.form-control-static.input-lg,.form-control-static.input-sm{padding-left:0;padding-right:0}.input-sm{height:30px;padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}select.input-sm{height:30px;line-height:30px}textarea.input-sm,select[multiple].input-sm{height:auto}.form-group-sm .form-control{height:30px;padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}.form-group-sm select.form-control{height:30px;line-height:30px}.form-group-sm textarea.form-control,.form-group-sm select[multiple].form-control{height:auto}.form-group-sm .form-control-static{height:30px;min-height:32px;padding:6px 10px;font-size:12px;line-height:1.5}.input-lg{height:46px;padding:10px 16px;font-size:18px;line-height:1.3333333;border-radius:6px}select.input-lg{height:46px;line-height:46px}textarea.input-lg,select[multiple].input-lg{height:auto}.form-group-lg .form-control{height:46px;padding:10px 16px;font-size:18px;line-height:1.3333333;border-radius:6px}.form-group-lg select.form-control{height:46px;line-height:46px}.form-group-lg textarea.form-control,.form-group-lg select[multiple].form-control{height:auto}.form-group-lg .form-control-static{height:46px;min-height:38px;padding:11px 16px;font-size:18px;line-height:1.3333333}.has-feedback{position:relative}.has-feedback .form-control{padding-right:42.5px}.form-control-feedback{position:absolute;top:0;right:0;z-index:2;display:block;width:34px;height:34px;line-height:34px;text-align:center;pointer-events:none}.input-lg+.form-control-feedback,.input-group-lg+.form-control-feedback,.form-group-lg .form-control+.form-control-feedback{width:46px;height:46px;line-height:46px}.input-sm+.form-control-feedback,.input-group-sm+.form-control-feedback,.form-group-sm .form-control+.form-control-feedback{width:30px;height:30px;line-height:30px}.has-success .help-block,.has-success .control-label,.has-success .radio,.has-success .checkbox,.has-success .radio-inline,.has-success .checkbox-inline,.has-success.radio label,.has-success.checkbox label,.has-success.radio-inline label,.has-success.checkbox-inline label{color:#3c763d}.has-success .form-control{border-color:#3c763d;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,0.075);box-shadow:inset 0 1px 1px rgba(0,0,0,0.075)}.has-success .form-control:focus{border-color:#2b542c;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,0.075),0 0 6px #67b168;box-shadow:inset 0 1px 1px rgba(0,0,0,0.075),0 0 6px #67b168}.has-success .input-group-addon{color:#3c763d;border-color:#3c763d;background-color:#dff0d8}.has-success .form-control-feedback{color:#3c763d}.has-warning .help-block,.has-warning .control-label,.has-warning .radio,.has-warning .checkbox,.has-warning .radio-inline,.has-warning .checkbox-inline,.has-warning.radio label,.has-warning.checkbox label,.has-warning.radio-inline label,.has-warning.checkbox-inline label{color:#8a6d3b}.has-warning .form-control{border-color:#8a6d3b;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,0.075);box-shadow:inset 0 1px 1px rgba(0,0,0,0.075)}.has-warning .form-control:focus{border-color:#66512c;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,0.075),0 0 6px #c0a16b;box-shadow:inset 0 1px 1px rgba(0,0,0,0.075),0 0 6px #c0a16b}.has-warning .input-group-addon{color:#8a6d3b;border-color:#8a6d3b;background-color:#fcf8e3}.has-warning .form-control-feedback{color:#8a6d3b}.has-error .help-block,.has-error .control-label,.has-error .radio,.has-error .checkbox,.has-error .radio-inline,.has-error .checkbox-inline,.has-error.radio label,.has-error.checkbox label,.has-error.radio-inline label,.has-error.checkbox-inline label{color:#a94442}.has-error .form-control{border-color:#a94442;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,0.075);box-shadow:inset 0 1px 1px rgba(0,0,0,0.075)}.has-error .form-control:focus{border-color:#843534;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,0.075),0 0 6px #ce8483;box-shadow:inset 0 1px 1px rgba(0,0,0,0.075),0 0 6px #ce8483}.has-error .input-group-addon{color:#a94442;border-color:#a94442;background-color:#f2dede}.has-error .form-control-feedback{color:#a94442}.has-feedback label~.form-control-feedback{top:25px}.has-feedback label.sr-only~.form-control-feedback{top:0}.help-block{display:block;margin-top:5px;margin-bottom:10px;color:#737373}@media (min-width:768px){.form-inline .form-group{display:inline-block;margin-bottom:0;vertical-align:middle}.form-inline .form-control{display:inline-block;width:auto;vertical-align:middle}.form-inline .form-control-static{display:inline-block}.form-inline .input-group{display:inline-table;vertical-align:middle}.form-inline .input-group .input-group-addon,.form-inline .input-group .input-group-btn,.form-inline .input-group .form-control{width:auto}.form-inline .input-group>.form-control{width:100%}.form-inline .control-label{margin-bottom:0;vertical-align:middle}.form-inline .radio,.form-inline .checkbox{display:inline-block;margin-top:0;margin-bottom:0;vertical-align:middle}.form-inline .radio label,.form-inline .checkbox label{padding-left:0}.form-inline .radio input[type="radio"],.form-inline .checkbox input[type="checkbox"]{position:relative;margin-left:0}.form-inline .has-feedback .form-control-feedback{top:0}}.form-horizontal .radio,.form-horizontal .checkbox,.form-horizontal .radio-inline,.form-horizontal .checkbox-inline{margin-top:0;margin-bottom:0;padding-top:7px}.form-horizontal .radio,.form-horizontal .checkbox{min-height:27px}.form-horizontal .form-group{margin-left:-15px;margin-right:-15px}@media (min-width:768px){.form-horizontal .control-label{text-align:right;margin-bottom:0;padding-top:7px}}.form-horizontal .has-feedback .form-control-feedback{right:15px}@media (min-width:768px){.form-horizontal .form-group-lg .control-label{padding-top:11px;font-size:18px}}@media (min-width:768px){.form-horizontal .form-group-sm .control-label{padding-top:6px;font-size:12px}}.btn{display:inline-block;margin-bottom:0;font-weight:normal;text-align:center;vertical-align:middle;-ms-touch-action:manipulation;touch-action:manipulation;cursor:pointer;background-image:none;border:1px solid transparent;white-space:nowrap;padding:6px 12px;font-size:14px;line-height:1.42857143;border-radius:4px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.btn:focus,.btn:active:focus,.btn.active:focus,.btn.focus,.btn:active.focus,.btn.active.focus{outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}.btn:hover,.btn:focus,.btn.focus{color:#333;text-decoration:none}.btn:active,.btn.active{outline:0;background-image:none;-webkit-box-shadow:inset 0 3px 5px rgba(0,0,0,0.125);box-shadow:inset 0 3px 5px rgba(0,0,0,0.125)}.btn.disabled,.btn[disabled],fieldset[disabled] .btn{cursor:not-allowed;opacity:.65;filter:alpha(opacity=65);-webkit-box-shadow:none;box-shadow:none}a.btn.disabled,fieldset[disabled] a.btn{pointer-events:none}.btn-default{color:#333;background-color:#fff;border-color:#ccc}.btn-default:focus,.btn-default.focus{color:#333;background-color:#e6e6e6;border-color:#8c8c8c}.btn-default:hover{color:#333;background-color:#e6e6e6;border-color:#adadad}.btn-default:active,.btn-default.active,.open>.dropdown-toggle.btn-default{color:#333;background-color:#e6e6e6;border-color:#adadad}.btn-default:active:hover,.btn-default.active:hover,.open>.dropdown-toggle.btn-default:hover,.btn-default:active:focus,.btn-default.active:focus,.open>.dropdown-toggle.btn-default:focus,.btn-default:active.focus,.btn-default.active.focus,.open>.dropdown-toggle.btn-default.focus{color:#333;background-color:#d4d4d4;border-color:#8c8c8c}.btn-default:active,.btn-default.active,.open>.dropdown-toggle.btn-default{background-image:none}.btn-default.disabled:hover,.btn-default[disabled]:hover,fieldset[disabled] .btn-default:hover,.btn-default.disabled:focus,.btn-default[disabled]:focus,fieldset[disabled] .btn-default:focus,.btn-default.disabled.focus,.btn-default[disabled].focus,fieldset[disabled] .btn-default.focus{background-color:#fff;border-color:#ccc}.btn-default .badge{color:#fff;background-color:#333}.btn-primary{color:#fff;background-color:#337ab7;border-color:#2e6da4}.btn-primary:focus,.btn-primary.focus{color:#fff;background-color:#286090;border-color:#122b40}.btn-primary:hover{color:#fff;background-color:#286090;border-color:#204d74}.btn-primary:active,.btn-primary.active,.open>.dropdown-toggle.btn-primary{color:#fff;background-color:#286090;border-color:#204d74}.btn-primary:active:hover,.btn-primary.active:hover,.open>.dropdown-toggle.btn-primary:hover,.btn-primary:active:focus,.btn-primary.active:focus,.open>.dropdown-toggle.btn-primary:focus,.btn-primary:active.focus,.btn-primary.active.focus,.open>.dropdown-toggle.btn-primary.focus{color:#fff;background-color:#204d74;border-color:#122b40}.btn-primary:active,.btn-primary.active,.open>.dropdown-toggle.btn-primary{background-image:none}.btn-primary.disabled:hover,.btn-primary[disabled]:hover,fieldset[disabled] .btn-primary:hover,.btn-primary.disabled:focus,.btn-primary[disabled]:focus,fieldset[disabled] .btn-primary:focus,.btn-primary.disabled.focus,.btn-primary[disabled].focus,fieldset[disabled] .btn-primary.focus{background-color:#337ab7;border-color:#2e6da4}.btn-primary .badge{color:#337ab7;background-color:#fff}.btn-success{color:#fff;background-color:#5cb85c;border-color:#4cae4c}.btn-success:focus,.btn-success.focus{color:#fff;background-color:#449d44;border-color:#255625}.btn-success:hover{color:#fff;background-color:#449d44;border-color:#398439}.btn-success:active,.btn-success.active,.open>.dropdown-toggle.btn-success{color:#fff;background-color:#449d44;border-color:#398439}.btn-success:active:hover,.btn-success.active:hover,.open>.dropdown-toggle.btn-success:hover,.btn-success:active:focus,.btn-success.active:focus,.open>.dropdown-toggle.btn-success:focus,.btn-success:active.focus,.btn-success.active.focus,.open>.dropdown-toggle.btn-success.focus{color:#fff;background-color:#398439;border-color:#255625}.btn-success:active,.btn-success.active,.open>.dropdown-toggle.btn-success{background-image:none}.btn-success.disabled:hover,.btn-success[disabled]:hover,fieldset[disabled] .btn-success:hover,.btn-success.disabled:focus,.btn-success[disabled]:focus,fieldset[disabled] .btn-success:focus,.btn-success.disabled.focus,.btn-success[disabled].focus,fieldset[disabled] .btn-success.focus{background-color:#5cb85c;border-color:#4cae4c}.btn-success .badge{color:#5cb85c;background-color:#fff}.btn-info{color:#fff;background-color:#5bc0de;border-color:#46b8da}.btn-info:focus,.btn-info.focus{color:#fff;background-color:#31b0d5;border-color:#1b6d85}.btn-info:hover{color:#fff;background-color:#31b0d5;border-color:#269abc}.btn-info:active,.btn-info.active,.open>.dropdown-toggle.btn-info{color:#fff;background-color:#31b0d5;border-color:#269abc}.btn-info:active:hover,.btn-info.active:hover,.open>.dropdown-toggle.btn-info:hover,.btn-info:active:focus,.btn-info.active:focus,.open>.dropdown-toggle.btn-info:focus,.btn-info:active.focus,.btn-info.active.focus,.open>.dropdown-toggle.btn-info.focus{color:#fff;background-color:#269abc;border-color:#1b6d85}.btn-info:active,.btn-info.active,.open>.dropdown-toggle.btn-info{background-image:none}.btn-info.disabled:hover,.btn-info[disabled]:hover,fieldset[disabled] .btn-info:hover,.btn-info.disabled:focus,.btn-info[disabled]:focus,fieldset[disabled] .btn-info:focus,.btn-info.disabled.focus,.btn-info[disabled].focus,fieldset[disabled] .btn-info.focus{background-color:#5bc0de;border-color:#46b8da}.btn-info .badge{color:#5bc0de;background-color:#fff}.btn-warning{color:#fff;background-color:#f0ad4e;border-color:#eea236}.btn-warning:focus,.btn-warning.focus{color:#fff;background-color:#ec971f;border-color:#985f0d}.btn-warning:hover{color:#fff;background-color:#ec971f;border-color:#d58512}.btn-warning:active,.btn-warning.active,.open>.dropdown-toggle.btn-warning{color:#fff;background-color:#ec971f;border-color:#d58512}.btn-warning:active:hover,.btn-warning.active:hover,.open>.dropdown-toggle.btn-warning:hover,.btn-warning:active:focus,.btn-warning.active:focus,.open>.dropdown-toggle.btn-warning:focus,.btn-warning:active.focus,.btn-warning.active.focus,.open>.dropdown-toggle.btn-warning.focus{color:#fff;background-color:#d58512;border-color:#985f0d}.btn-warning:active,.btn-warning.active,.open>.dropdown-toggle.btn-warning{background-image:none}.btn-warning.disabled:hover,.btn-warning[disabled]:hover,fieldset[disabled] .btn-warning:hover,.btn-warning.disabled:focus,.btn-warning[disabled]:focus,fieldset[disabled] .btn-warning:focus,.btn-warning.disabled.focus,.btn-warning[disabled].focus,fieldset[disabled] .btn-warning.focus{background-color:#f0ad4e;border-color:#eea236}.btn-warning .badge{color:#f0ad4e;background-color:#fff}.btn-danger{color:#fff;background-color:#d9534f;border-color:#d43f3a}.btn-danger:focus,.btn-danger.focus{color:#fff;background-color:#c9302c;border-color:#761c19}.btn-danger:hover{color:#fff;background-color:#c9302c;border-color:#ac2925}.btn-danger:active,.btn-danger.active,.open>.dropdown-toggle.btn-danger{color:#fff;background-color:#c9302c;border-color:#ac2925}.btn-danger:active:hover,.btn-danger.active:hover,.open>.dropdown-toggle.btn-danger:hover,.btn-danger:active:focus,.btn-danger.active:focus,.open>.dropdown-toggle.btn-danger:focus,.btn-danger:active.focus,.btn-danger.active.focus,.open>.dropdown-toggle.btn-danger.focus{color:#fff;background-color:#ac2925;border-color:#761c19}.btn-danger:active,.btn-danger.active,.open>.dropdown-toggle.btn-danger{background-image:none}.btn-danger.disabled:hover,.btn-danger[disabled]:hover,fieldset[disabled] .btn-danger:hover,.btn-danger.disabled:focus,.btn-danger[disabled]:focus,fieldset[disabled] .btn-danger:focus,.btn-danger.disabled.focus,.btn-danger[disabled].focus,fieldset[disabled] .btn-danger.focus{background-color:#d9534f;border-color:#d43f3a}.btn-danger .badge{color:#d9534f;background-color:#fff}.btn-link{color:#337ab7;font-weight:normal;border-radius:0}.btn-link,.btn-link:active,.btn-link.active,.btn-link[disabled],fieldset[disabled] .btn-link{background-color:transparent;-webkit-box-shadow:none;box-shadow:none}.btn-link,.btn-link:hover,.btn-link:focus,.btn-link:active{border-color:transparent}.btn-link:hover,.btn-link:focus{color:#23527c;text-decoration:underline;background-color:transparent}.btn-link[disabled]:hover,fieldset[disabled] .btn-link:hover,.btn-link[disabled]:focus,fieldset[disabled] .btn-link:focus{color:#777;text-decoration:none}.btn-lg{padding:10px 16px;font-size:18px;line-height:1.3333333;border-radius:6px}.btn-sm{padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}.btn-xs{padding:1px 5px;font-size:12px;line-height:1.5;border-radius:3px}.btn-block{display:block;width:100%}.btn-block+.btn-block{margin-top:5px}input[type="submit"].btn-block,input[type="reset"].btn-block,input[type="button"].btn-block{width:100%}.nav{margin-bottom:0;padding-left:0;list-style:none}.nav>li{position:relative;display:block}.nav>li>a{position:relative;display:block;padding:10px 15px}.nav>li>a:hover,.nav>li>a:focus{text-decoration:none;background-color:#eee}.nav>li.disabled>a{color:#777}.nav>li.disabled>a:hover,.nav>li.disabled>a:focus{color:#777;text-decoration:none;background-color:transparent;cursor:not-allowed}.nav .open>a,.nav .open>a:hover,.nav .open>a:focus{background-color:#eee;border-color:#337ab7}.nav .nav-divider{height:1px;margin:9px 0;overflow:hidden;background-color:#e5e5e5}.nav>li>a>img{max-width:none}.nav-tabs{border-bottom:1px solid #ddd}.nav-tabs>li{float:left;margin-bottom:-1px}.nav-tabs>li>a{margin-right:2px;line-height:1.42857143;border:1px solid transparent;border-radius:4px 4px 0 0}.nav-tabs>li>a:hover{border-color:#eee #eee #ddd}.nav-tabs>li.active>a,.nav-tabs>li.active>a:hover,.nav-tabs>li.active>a:focus{color:#555;background-color:#fff;border:1px solid #ddd;border-bottom-color:transparent;cursor:default}.nav-tabs.nav-justified{width:100%;border-bottom:0}.nav-tabs.nav-justified>li{float:none}.nav-tabs.nav-justified>li>a{text-align:center;margin-bottom:5px}.nav-tabs.nav-justified>.dropdown .dropdown-menu{top:auto;left:auto}@media (min-width:768px){.nav-tabs.nav-justified>li{display:table-cell;width:1%}.nav-tabs.nav-justified>li>a{margin-bottom:0}}.nav-tabs.nav-justified>li>a{margin-right:0;border-radius:4px}.nav-tabs.nav-justified>.active>a,.nav-tabs.nav-justified>.active>a:hover,.nav-tabs.nav-justified>.active>a:focus{border:1px solid #ddd}@media (min-width:768px){.nav-tabs.nav-justified>li>a{border-bottom:1px solid #ddd;border-radius:4px 4px 0 0}.nav-tabs.nav-justified>.active>a,.nav-tabs.nav-justified>.active>a:hover,.nav-tabs.nav-justified>.active>a:focus{border-bottom-color:#fff}}.nav-pills>li{float:left}.nav-pills>li>a{border-radius:4px}.nav-pills>li+li{margin-left:2px}.nav-pills>li.active>a,.nav-pills>li.active>a:hover,.nav-pills>li.active>a:focus{color:#fff;background-color:#337ab7}.nav-stacked>li{float:none}.nav-stacked>li+li{margin-top:2px;margin-left:0}.nav-justified{width:100%}.nav-justified>li{float:none}.nav-justified>li>a{text-align:center;margin-bottom:5px}.nav-justified>.dropdown .dropdown-menu{top:auto;left:auto}@media (min-width:768px){.nav-justified>li{display:table-cell;width:1%}.nav-justified>li>a{margin-bottom:0}}.nav-tabs-justified{border-bottom:0}.nav-tabs-justified>li>a{margin-right:0;border-radius:4px}.nav-tabs-justified>.active>a,.nav-tabs-justified>.active>a:hover,.nav-tabs-justified>.active>a:focus{border:1px solid #ddd}@media (min-width:768px){.nav-tabs-justified>li>a{border-bottom:1px solid #ddd;border-radius:4px 4px 0 0}.nav-tabs-justified>.active>a,.nav-tabs-justified>.active>a:hover,.nav-tabs-justified>.active>a:focus{border-bottom-color:#fff}}.tab-content>.tab-pane{display:none}.tab-content>.active{display:block}.nav-tabs .dropdown-menu{margin-top:-1px;border-top-right-radius:0;border-top-left-radius:0}.navbar{position:relative;min-height:50px;margin-bottom:20px;border:1px solid transparent}@media (min-width:768px){.navbar{border-radius:4px}}@media (min-width:768px){.navbar-header{float:left}}.navbar-collapse{overflow-x:visible;padding-right:15px;padding-left:15px;border-top:1px solid transparent;-webkit-box-shadow:inset 0 1px 0 rgba(255,255,255,0.1);box-shadow:inset 0 1px 0 rgba(255,255,255,0.1);-webkit-overflow-scrolling:touch}.navbar-collapse.in{overflow-y:auto}@media (min-width:768px){.navbar-collapse{width:auto;border-top:0;-webkit-box-shadow:none;box-shadow:none}.navbar-collapse.collapse{display:block !important;height:auto !important;padding-bottom:0;overflow:visible !important}.navbar-collapse.in{overflow-y:visible}.navbar-fixed-top .navbar-collapse,.navbar-static-top .navbar-collapse,.navbar-fixed-bottom .navbar-collapse{padding-left:0;padding-right:0}}.navbar-fixed-top .navbar-collapse,.navbar-fixed-bottom .navbar-collapse{max-height:340px}@media (max-device-width:480px) and (orientation:landscape){.navbar-fixed-top .navbar-collapse,.navbar-fixed-bottom .navbar-collapse{max-height:200px}}.container>.navbar-header,.container-fluid>.navbar-header,.container>.navbar-collapse,.container-fluid>.navbar-collapse{margin-right:-15px;margin-left:-15px}@media (min-width:768px){.container>.navbar-header,.container-fluid>.navbar-header,.container>.navbar-collapse,.container-fluid>.navbar-collapse{margin-right:0;margin-left:0}}.navbar-static-top{z-index:1000;border-width:0 0 1px}@media (min-width:768px){.navbar-static-top{border-radius:0}}.navbar-fixed-top,.navbar-fixed-bottom{position:fixed;right:0;left:0;z-index:1030}@media (min-width:768px){.navbar-fixed-top,.navbar-fixed-bottom{border-radius:0}}.navbar-fixed-top{top:0;border-width:0 0 1px}.navbar-fixed-bottom{bottom:0;margin-bottom:0;border-width:1px 0 0}.navbar-brand{float:left;padding:15px 15px;font-size:18px;line-height:20px;height:50px}.navbar-brand:hover,.navbar-brand:focus{text-decoration:none}.navbar-brand>img{display:block}@media (min-width:768px){.navbar>.container .navbar-brand,.navbar>.container-fluid .navbar-brand{margin-left:-15px}}.navbar-toggle{position:relative;float:right;margin-right:15px;padding:9px 10px;margin-top:8px;margin-bottom:8px;background-color:transparent;background-image:none;border:1px solid transparent;border-radius:4px}.navbar-toggle:focus{outline:0}.navbar-toggle .icon-bar{display:block;width:22px;height:2px;border-radius:1px}.navbar-toggle .icon-bar+.icon-bar{margin-top:4px}@media (min-width:768px){.navbar-toggle{display:none}}.navbar-nav{margin:7.5px -15px}.navbar-nav>li>a{padding-top:10px;padding-bottom:10px;line-height:20px}@media (max-width:767px){.navbar-nav .open .dropdown-menu{position:static;float:none;width:auto;margin-top:0;background-color:transparent;border:0;-webkit-box-shadow:none;box-shadow:none}.navbar-nav .open .dropdown-menu>li>a,.navbar-nav .open .dropdown-menu .dropdown-header{padding:5px 15px 5px 25px}.navbar-nav .open .dropdown-menu>li>a{line-height:20px}.navbar-nav .open .dropdown-menu>li>a:hover,.navbar-nav .open .dropdown-menu>li>a:focus{background-image:none}}@media (min-width:768px){.navbar-nav{float:left;margin:0}.navbar-nav>li{float:left}.navbar-nav>li>a{padding-top:15px;padding-bottom:15px}}.navbar-form{margin-left:-15px;margin-right:-15px;padding:10px 15px;border-top:1px solid transparent;border-bottom:1px solid transparent;-webkit-box-shadow:inset 0 1px 0 rgba(255,255,255,0.1),0 1px 0 rgba(255,255,255,0.1);box-shadow:inset 0 1px 0 rgba(255,255,255,0.1),0 1px 0 rgba(255,255,255,0.1);margin-top:8px;margin-bottom:8px}@media (min-width:768px){.navbar-form .form-group{display:inline-block;margin-bottom:0;vertical-align:middle}.navbar-form .form-control{display:inline-block;width:auto;vertical-align:middle}.navbar-form .form-control-static{display:inline-block}.navbar-form .input-group{display:inline-table;vertical-align:middle}.navbar-form .input-group .input-group-addon,.navbar-form .input-group .input-group-btn,.navbar-form .input-group .form-control{width:auto}.navbar-form .input-group>.form-control{width:100%}.navbar-form .control-label{margin-bottom:0;vertical-align:middle}.navbar-form .radio,.navbar-form .checkbox{display:inline-block;margin-top:0;margin-bottom:0;vertical-align:middle}.navbar-form .radio label,.navbar-form .checkbox label{padding-left:0}.navbar-form .radio input[type="radio"],.navbar-form .checkbox input[type="checkbox"]{position:relative;margin-left:0}.navbar-form .has-feedback .form-control-feedback{top:0}}@media (max-width:767px){.navbar-form .form-group{margin-bottom:5px}.navbar-form .form-group:last-child{margin-bottom:0}}@media (min-width:768px){.navbar-form{width:auto;border:0;margin-left:0;margin-right:0;padding-top:0;padding-bottom:0;-webkit-box-shadow:none;box-shadow:none}}.navbar-nav>li>.dropdown-menu{margin-top:0;border-top-right-radius:0;border-top-left-radius:0}.navbar-fixed-bottom .navbar-nav>li>.dropdown-menu{margin-bottom:0;border-top-right-radius:4px;border-top-left-radius:4px;border-bottom-right-radius:0;border-bottom-left-radius:0}.navbar-btn{margin-top:8px;margin-bottom:8px}.navbar-btn.btn-sm{margin-top:10px;margin-bottom:10px}.navbar-btn.btn-xs{margin-top:14px;margin-bottom:14px}.navbar-text{margin-top:15px;margin-bottom:15px}@media (min-width:768px){.navbar-text{float:left;margin-left:15px;margin-right:15px}}@media (min-width:768px){.navbar-left{float:left !important}.navbar-right{float:right !important;margin-right:-15px}.navbar-right~.navbar-right{margin-right:0}}.navbar-default{background-color:#f8f8f8;border-color:#e7e7e7}.navbar-default .navbar-brand{color:#777}.navbar-default .navbar-brand:hover,.navbar-default .navbar-brand:focus{color:#5e5e5e;background-color:transparent}.navbar-default .navbar-text{color:#777}.navbar-default .navbar-nav>li>a{color:#777}.navbar-default .navbar-nav>li>a:hover,.navbar-default .navbar-nav>li>a:focus{color:#333;background-color:transparent}.navbar-default .navbar-nav>.active>a,.navbar-default .navbar-nav>.active>a:hover,.navbar-default .navbar-nav>.active>a:focus{color:#555;background-color:#e7e7e7}.navbar-default .navbar-nav>.disabled>a,.navbar-default .navbar-nav>.disabled>a:hover,.navbar-default .navbar-nav>.disabled>a:focus{color:#ccc;background-color:transparent}.navbar-default .navbar-toggle{border-color:#ddd}.navbar-default .navbar-toggle:hover,.navbar-default .navbar-toggle:focus{background-color:#ddd}.navbar-default .navbar-toggle .icon-bar{background-color:#888}.navbar-default .navbar-collapse,.navbar-default .navbar-form{border-color:#e7e7e7}.navbar-default .navbar-nav>.open>a,.navbar-default .navbar-nav>.open>a:hover,.navbar-default .navbar-nav>.open>a:focus{background-color:#e7e7e7;color:#555}@media (max-width:767px){.navbar-default .navbar-nav .open .dropdown-menu>li>a{color:#777}.navbar-default .navbar-nav .open .dropdown-menu>li>a:hover,.navbar-default .navbar-nav .open .dropdown-menu>li>a:focus{color:#333;background-color:transparent}.navbar-default .navbar-nav .open .dropdown-menu>.active>a,.navbar-default .navbar-nav .open .dropdown-menu>.active>a:hover,.navbar-default .navbar-nav .open .dropdown-menu>.active>a:focus{color:#555;background-color:#e7e7e7}.navbar-default .navbar-nav .open .dropdown-menu>.disabled>a,.navbar-default .navbar-nav .open .dropdown-menu>.disabled>a:hover,.navbar-default .navbar-nav .open .dropdown-menu>.disabled>a:focus{color:#ccc;background-color:transparent}}.navbar-default .navbar-link{color:#777}.navbar-default .navbar-link:hover{color:#333}.navbar-default .btn-link{color:#777}.navbar-default .btn-link:hover,.navbar-default .btn-link:focus{color:#333}.navbar-default .btn-link[disabled]:hover,fieldset[disabled] .navbar-default .btn-link:hover,.navbar-default .btn-link[disabled]:focus,fieldset[disabled] .navbar-default .btn-link:focus{color:#ccc}.navbar-inverse{background-color:#222;border-color:#080808}.navbar-inverse .navbar-brand{color:#9d9d9d}.navbar-inverse .navbar-brand:hover,.navbar-inverse .navbar-brand:focus{color:#fff;background-color:transparent}.navbar-inverse .navbar-text{color:#9d9d9d}.navbar-inverse .navbar-nav>li>a{color:#9d9d9d}.navbar-inverse .navbar-nav>li>a:hover,.navbar-inverse .navbar-nav>li>a:focus{color:#fff;background-color:transparent}.navbar-inverse .navbar-nav>.active>a,.navbar-inverse .navbar-nav>.active>a:hover,.navbar-inverse .navbar-nav>.active>a:focus{color:#fff;background-color:#080808}.navbar-inverse .navbar-nav>.disabled>a,.navbar-inverse .navbar-nav>.disabled>a:hover,.navbar-inverse .navbar-nav>.disabled>a:focus{color:#444;background-color:transparent}.navbar-inverse .navbar-toggle{border-color:#333}.navbar-inverse .navbar-toggle:hover,.navbar-inverse .navbar-toggle:focus{background-color:#333}.navbar-inverse .navbar-toggle .icon-bar{background-color:#fff}.navbar-inverse .navbar-collapse,.navbar-inverse .navbar-form{border-color:#101010}.navbar-inverse .navbar-nav>.open>a,.navbar-inverse .navbar-nav>.open>a:hover,.navbar-inverse .navbar-nav>.open>a:focus{background-color:#080808;color:#fff}@media (max-width:767px){.navbar-inverse .navbar-nav .open .dropdown-menu>.dropdown-header{border-color:#080808}.navbar-inverse .navbar-nav .open .dropdown-menu .divider{background-color:#080808}.navbar-inverse .navbar-nav .open .dropdown-menu>li>a{color:#9d9d9d}.navbar-inverse .navbar-nav .open .dropdown-menu>li>a:hover,.navbar-inverse .navbar-nav .open .dropdown-menu>li>a:focus{color:#fff;background-color:transparent}.navbar-inverse .navbar-nav .open .dropdown-menu>.active>a,.navbar-inverse .navbar-nav .open .dropdown-menu>.active>a:hover,.navbar-inverse .navbar-nav .open .dropdown-menu>.active>a:focus{color:#fff;background-color:#080808}.navbar-inverse .navbar-nav .open .dropdown-menu>.disabled>a,.navbar-inverse .navbar-nav .open .dropdown-menu>.disabled>a:hover,.navbar-inverse .navbar-nav .open .dropdown-menu>.disabled>a:focus{color:#444;background-color:transparent}}.navbar-inverse .navbar-link{color:#9d9d9d}.navbar-inverse .navbar-link:hover{color:#fff}.navbar-inverse .btn-link{color:#9d9d9d}.navbar-inverse .btn-link:hover,.navbar-inverse .btn-link:focus{color:#fff}.navbar-inverse .btn-link[disabled]:hover,fieldset[disabled] .navbar-inverse .btn-link:hover,.navbar-inverse .btn-link[disabled]:focus,fieldset[disabled] .navbar-inverse .btn-link:focus{color:#444}.pagination{display:inline-block;padding-left:0;margin:20px 0;border-radius:4px}.pagination>li{display:inline}.pagination>li>a,.pagination>li>span{position:relative;float:left;padding:6px 12px;line-height:1.42857143;text-decoration:none;color:#337ab7;background-color:#fff;border:1px solid #ddd;margin-left:-1px}.pagination>li:first-child>a,.pagination>li:first-child>span{margin-left:0;border-bottom-left-radius:4px;border-top-left-radius:4px}.pagination>li:last-child>a,.pagination>li:last-child>span{border-bottom-right-radius:4px;border-top-right-radius:4px}.pagination>li>a:hover,.pagination>li>span:hover,.pagination>li>a:focus,.pagination>li>span:focus{z-index:2;color:#23527c;background-color:#eee;border-color:#ddd}.pagination>.active>a,.pagination>.active>span,.pagination>.active>a:hover,.pagination>.active>span:hover,.pagination>.active>a:focus,.pagination>.active>span:focus{z-index:3;color:#fff;background-color:#337ab7;border-color:#337ab7;cursor:default}.pagination>.disabled>span,.pagination>.disabled>span:hover,.pagination>.disabled>span:focus,.pagination>.disabled>a,.pagination>.disabled>a:hover,.pagination>.disabled>a:focus{color:#777;background-color:#fff;border-color:#ddd;cursor:not-allowed}.pagination-lg>li>a,.pagination-lg>li>span{padding:10px 16px;font-size:18px;line-height:1.3333333}.pagination-lg>li:first-child>a,.pagination-lg>li:first-child>span{border-bottom-left-radius:6px;border-top-left-radius:6px}.pagination-lg>li:last-child>a,.pagination-lg>li:last-child>span{border-bottom-right-radius:6px;border-top-right-radius:6px}.pagination-sm>li>a,.pagination-sm>li>span{padding:5px 10px;font-size:12px;line-height:1.5}.pagination-sm>li:first-child>a,.pagination-sm>li:first-child>span{border-bottom-left-radius:3px;border-top-left-radius:3px}.pagination-sm>li:last-child>a,.pagination-sm>li:last-child>span{border-bottom-right-radius:3px;border-top-right-radius:3px}.clearfix:before,.clearfix:after,.container:before,.container:after,.container-fluid:before,.container-fluid:after,.row:before,.row:after,.form-horizontal .form-group:before,.form-horizontal .form-group:after,.nav:before,.nav:after,.navbar:before,.navbar:after,.navbar-header:before,.navbar-header:after,.navbar-collapse:before,.navbar-collapse:after{content:" ";display:table}.clearfix:after,.container:after,.container-fluid:after,.row:after,.form-horizontal .form-group:after,.nav:after,.navbar:after,.navbar-header:after,.navbar-collapse:after{clear:both}.center-block{display:block;margin-left:auto;margin-right:auto}.pull-right{float:right !important}.pull-left{float:left !important}.hide{display:none !important}.show{display:block !important}.invisible{visibility:hidden}.text-hide{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;border:0}.hidden{display:none !important}.affix{position:fixed}
  </style>
</head>

<body>
  <div class="container">
    <img width="100%" border="0" alt="Science.gov" src="//science.gov/images/SciGov_banner640.png">
    <nav class="navbar navbar-default">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Topics by Science.gov</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="//www.science.gov/">Home</a></li>
            <li><a href="//www.science.gov/about.html">About</a></li>
	    <li><a href="//www.science.gov/STEM_Opportunities.html">STEM Opportunities</a></li>
	    <li><a href="//www.science.gov/members/">Sign In</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div><!--/.container-fluid -->
    </nav>
<center><h4>Sample records for <strong>images</strong></h4></center>
<div id="page_1" class="visibleDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li class="active"><span>1</span></li>
   <li><a href="#" onclick='return showDiv("page_2");'>2</a></li>
      <li><a href="#" onclick='return showDiv("page_3");'>3</a></li>
      <li><a href="#" onclick='return showDiv("page_4");'>4</a></li>
      <li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="1">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.northcom.mil/Images.aspx','SCIGOVWS'); return false;" href="http://www.northcom.mil/Images.aspx"><span><span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.science.gov/aboutsearch.html">Science.gov Websites</a></p>
      <p></p>
         <p></p>
         <p>: Upload Date Photo Date 1 2 3 4 5 Next Arctic Edge 2018 Download Full <em><span class="hlt">Image</span></em> Photo Details Arctic Edge 2018 Download Full <em><span class="hlt">Image</span></em> Photo Details Arctic Edge 2018 Download Full <em><span class="hlt">Image</span></em> Photo Details Arctic Edge 2018 Download Full <em><span class="hlt">Image</span></em> Photo Details Arctic Edge 2018 Download Full <em><span class="hlt">Image</span></em> Photo Details Arctic Edge 2018</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=Any+AND+prize&pg=3&id=ED264991','ERIC'); return false;" href="https://eric.ed.gov/?q=Any+AND+prize&pg=3&id=ED264991"><span><span class="hlt">Images</span>.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Christensen, Rosemary Ackley</p>
         <p></p>
         <p>The packet of visual <span class="hlt">images</span>, designed by Ojibwe artist Steven Premo, is intended to provide teachers of Indian students with contemporary, positive, non-stereotypical <span class="hlt">images</span> of native cultures, particularly Indian women, that can be used in all classes for any aged student to assist in increasing the self-esteem of Indian children and help raise…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/1250847-image','SCIGOV-STC'); return false;" href="https://www.osti.gov/biblio/1250847-image"><span><span class="hlt">Image</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Marsh, Amber; Harsch, Tim; Pitt, Julie</p>
         <p>2007-08-31</p>
         <p>The computer side of the <span class="hlt">IMAGE</span> project consists of a collection of Perl scripts that perform a variety of tasks; scripts are available to insert, update and delete data from the underlying Oracle database, download data from NCBI's Genbank and other sources, and generate data files for download by interested parties. Web scripts make up the tracking interface, and various tools available on the project web-site (<span class="hlt">image</span>.llnl.gov) that provide a search interface to the database.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://nccih.nih.gov/news/multimedia/gallery','NIH-MEDLINEPLUS'); return false;" href="https://nccih.nih.gov/news/multimedia/gallery"><span><span class="hlt">Image</span> Gallery</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>... R S T U V W X Y Z <span class="hlt">Image</span> Gallery Share: The <span class="hlt">Image</span> Gallery contains high-quality digital photographs available from ... Select a category below to view additional thumbnail <span class="hlt">images</span>. <span class="hlt">Images</span> are available for direct download in 2 ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.epa.gov/web-policies-and-procedures/image-guidance','PESTICIDES'); return false;" href="https://www.epa.gov/web-policies-and-procedures/image-guidance"><span><span class="hlt">Image</span> Guidance</span></a></p>
      <p><a target="_blank" href="http://www.epa.gov/pesticides/search.htm">EPA Pesticide Factsheets</a></p>
      <p></p>
         <p></p>
         <p>Guidance that explains the process for getting <span class="hlt">images</span> approved in One EPA Web microsites and resource directories. includes an appendix that shows examples of what makes some <span class="hlt">images</span> better than others, how some <span class="hlt">images</span> convey meaning more than others</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020090905','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020090905"><span>Digital <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1986-01-01</p>
         <p>Digital <span class="hlt">Imaging</span> is the computer processed numerical representation of physical <span class="hlt">images</span>. Enhancement of <span class="hlt">images</span> results in easier interpretation. Quantitative digital <span class="hlt">image</span> analysis by Perceptive Scientific Instruments, locates objects within an <span class="hlt">image</span> and measures them to extract quantitative information. Applications are CAT scanners, radiography, microscopy in medicine as well as various industrial and manufacturing uses. The PSICOM 327 performs all digital <span class="hlt">image</span> analysis functions. It is based on Jet Propulsion Laboratory technology, is accurate and cost efficient.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/19301663','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/19301663"><span><span class="hlt">Imaging</span> angiogenesis.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Charnley, Natalie; Donaldson, Stephanie; Price, Pat</p>
         <p>2009-01-01</p>
         <p>There is a need for direct <span class="hlt">imaging</span> of effects on tumor vasculature in assessment of response to antiangiogenic drugs and vascular disrupting agents. <span class="hlt">Imaging</span> tumor vasculature depends on differences in permeability of vasculature of tumor and normal tissue, which cause changes in penetration of contrast agents. Angiogenesis <span class="hlt">imaging</span> may be defined in terms of measurement of tumor perfusion and direct <span class="hlt">imaging</span> of the molecules involved in angiogenesis. In addition, assessment of tumor hypoxia will give an indication of tumor vasculature. The range of <span class="hlt">imaging</span> techniques available for these processes includes positron emission tomography (PET), dynamic contrast-enhanced magnetic resonance <span class="hlt">imaging</span> (DCE-MRI), perfusion computed tomography (CT), and ultrasound (US).</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=ultrasound+AND+imaging&pg=3&id=EJ522191','ERIC'); return false;" href="https://eric.ed.gov/?q=ultrasound+AND+imaging&pg=3&id=EJ522191"><span>Medical <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Barker, M. C. J.</p>
         <p>1996-01-01</p>
         <p>Discusses four main types of medical <span class="hlt">imaging</span> (x-ray, radionuclide, ultrasound, and magnetic resonance) and considers their relative merits. Describes important recent and possible future developments in <span class="hlt">image</span> processing. (Author/MKR)</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/diagnosticimaging.html','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/diagnosticimaging.html"><span>Diagnostic <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>Diagnostic <span class="hlt">imaging</span> lets doctors look inside your body for clues about a medical condition. A variety of machines and ... and activities inside your body. The type of <span class="hlt">imaging</span> your doctor uses depends on your symptoms and ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=moving+AND+stress&pg=4&id=EJ781149','ERIC'); return false;" href="https://eric.ed.gov/?q=moving+AND+stress&pg=4&id=EJ781149"><span>Canonical <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Hewitt, Dave</p>
         <p>2007-01-01</p>
         <p>In this article, the author offers two well-known mathematical <span class="hlt">images</span>--that of a dot moving around a circle; and that of the tens chart--and considers their power for developing mathematical thinking. In his opinion, these <span class="hlt">images</span> each contain the essence of a particular topic of mathematics. They are contrasting <span class="hlt">images</span> in the sense that they deal…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1129377','SCIGOV-STC'); return false;" href="https://www.osti.gov/servlets/purl/1129377"><span><span class="hlt">Image</span> alignment</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Dowell, Larry Jonathan</p>
         <p></p>
         <p>Disclosed is a method and device for aligning at least two digital <span class="hlt">images</span>. An embodiment may use frequency-domain transforms of small tiles created from each <span class="hlt">image</span> to identify substantially similar, "distinguishing" features within each of the <span class="hlt">images</span>, and then align the <span class="hlt">images</span> together based on the location of the distinguishing features. To accomplish this, an embodiment may create equal sized tile sub-<span class="hlt">images</span> for each <span class="hlt">image</span>. A "key" for each tile may be created by performing a frequency-domain transform calculation on each tile. A information-distance difference between each possible pair of tiles on each <span class="hlt">image</span> may be calculated to identify distinguishingmore » features. From analysis of the information-distance differences of the pairs of tiles, a subset of tiles with high discrimination metrics in relation to other tiles may be located for each <span class="hlt">image</span>. The subset of distinguishing tiles for each <span class="hlt">image</span> may then be compared to locate tiles with substantially similar keys and/or information-distance metrics to other tiles of other <span class="hlt">images</span>. Once similar tiles are located for each <span class="hlt">image</span>, the <span class="hlt">images</span> may be aligned in relation to the identified similar tiles.« less</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020080941','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020080941"><span><span class="hlt">Image</span> Processing</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1993-01-01</p>
         <p>Electronic Imagery, Inc.'s <span class="hlt">Image</span>Scale Plus software, developed through a Small Business Innovation Research (SBIR) contract with Kennedy Space Flight Center for use on space shuttle Orbiter in 1991, enables astronauts to conduct <span class="hlt">image</span> processing, prepare electronic still camera <span class="hlt">images</span> in orbit, display them and downlink <span class="hlt">images</span> to ground based scientists for evaluation. Electronic Imagery, Inc.'s <span class="hlt">Image</span>Count, a spin-off product of <span class="hlt">Image</span>Scale Plus, is used to count trees in Florida orange groves. Other applications include x-ray and MRI imagery, textile designs and special effects for movies. As of 1/28/98, company could not be located, therefore contact/product information is no longer valid.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA246373','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA246373"><span>Electronic <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1991-11-01</p>
         <p>Tilted Rough Disc,&#34 Donald J. Schertler and Nicholas George &#34<span class="hlt">Image</span> Deblurring for Multiple-Point Impulse Responses,&#34 Bryan J. Stossel and Nicholas George...Rough Disc Donald J. Schertler Nicholas George <span class="hlt">Image</span> Deblurring for Multiple-Point Impulse Bryan J. Stossel Responses Nicholas George z 0 zw V) w LU 0...number of impulses present in the degradation. <span class="hlt">IMAGE</span> DEBLURRING FOR MULTIPLE-POINT IMPULSE RESPONSESt Bryan J. Stossel Nicholas George Institute of Optics</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2003SPIE.5008..493D','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2003SPIE.5008..493D"><span><span class="hlt">Image</span> barcodes</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Damera-Venkata, Niranjan; Yen, Jonathan</p>
         <p>2003-01-01</p>
         <p>A Visually significant two-dimensional barcode (VSB) developed by Shaked et. al. is a method used to design an information carrying two-dimensional barcode, which has the appearance of a given graphical entity such as a company logo. The encoding and decoding of information using the VSB, uses a base <span class="hlt">image</span> with very few graylevels (typically only two). This typically requires the <span class="hlt">image</span> histogram to be bi-modal. For continuous-tone <span class="hlt">images</span> such as digital photographs of individuals, the representation of tone or "shades of gray" is not only important to obtain a pleasing rendition of the face, but in most cases, the VSB renders these <span class="hlt">images</span> unrecognizable due to its inability to represent true gray-tone variations. This paper extends the concept of a VSB to an <span class="hlt">image</span> bar code (IBC). We enable the encoding and subsequent decoding of information embedded in the hardcopy version of continuous-tone base-<span class="hlt">images</span> such as those acquired with a digital camera. The encoding-decoding process is modeled by robust data transmission through a noisy print-scan channel that is explicitly modeled. The IBC supports a high information capacity that differentiates it from common hardcopy watermarks. The reason for the improved <span class="hlt">image</span> quality over the VSB is a joint encoding/halftoning strategy based on a modified version of block error diffusion. Encoder stability, <span class="hlt">image</span> quality vs. information capacity tradeoffs and decoding issues with and without explicit knowledge of the base-<span class="hlt">image</span> are discussed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020087015','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020087015"><span>Body <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1990-01-01</p>
         <p>Magnetic Resonance <span class="hlt">Imaging</span> (MRI) and Computer-aided Tomography (CT) <span class="hlt">images</span> are often complementary. In most cases, MRI is good for viewing soft tissue but not bone, while CT <span class="hlt">images</span> are good for bone but not always good for soft tissue discrimination. Physicians and engineers in the Department of Radiology at the University of Michigan Hospitals are developing a technique for combining the best features of MRI and CT scans to increase the accuracy of discriminating one type of body tissue from another. One of their research tools is a computer program called HICAP. The program can be used to distinguish between healthy and diseased tissue in body <span class="hlt">images</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4756468','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4756468"><span><span class="hlt">Imaging</span> Atherosclerosis</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Tarkin, Jason M.; Dweck, Marc R.; Evans, Nicholas R.; Takx, Richard A.P.; Brown, Adam J.; Tawakol, Ahmed; Fayad, Zahi A.</p>
         <p>2016-01-01</p>
         <p>Advances in atherosclerosis <span class="hlt">imaging</span> technology and research have provided a range of diagnostic tools to characterize high-risk plaque in vivo; however, these important vascular <span class="hlt">imaging</span> methods additionally promise great scientific and translational applications beyond this quest. When combined with conventional anatomic- and hemodynamic-based assessments of disease severity, cross-sectional multimodal <span class="hlt">imaging</span> incorporating molecular probes and other novel noninvasive techniques can add detailed interrogation of plaque composition, activity, and overall disease burden. In the catheterization laboratory, intravascular <span class="hlt">imaging</span> provides unparalleled access to the world beneath the plaque surface, allowing tissue characterization and measurement of cap thickness with micrometer spatial resolution. Atherosclerosis <span class="hlt">imaging</span> captures key data that reveal snapshots into underlying biology, which can test our understanding of fundamental research questions and shape our approach toward patient management. <span class="hlt">Imaging</span> can also be used to quantify response to therapeutic interventions and ultimately help predict cardiovascular risk. Although there are undeniable barriers to clinical translation, many of these hold-ups might soon be surpassed by rapidly evolving innovations to improve <span class="hlt">image</span> acquisition, coregistration, motion correction, and reduce radiation exposure. This article provides a comprehensive review of current and experimental atherosclerosis <span class="hlt">imaging</span> methods and their uses in research and potential for translation to the clinic. PMID:26892971</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19940021009','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19940021009"><span><span class="hlt">Image</span> fusion</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Pavel, M.</p>
         <p>1993-01-01</p>
         <p>The topics covered include the following: a system overview of the basic components of a system designed to improve the ability of a pilot to fly through low-visibility conditions such as fog; the role of visual sciences; fusion issues; sensor characterization; sources of information; <span class="hlt">image</span> processing; and <span class="hlt">image</span> fusion.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=molecular+AND+genetics&pg=2&id=EJ840752','ERIC'); return false;" href="https://eric.ed.gov/?q=molecular+AND+genetics&pg=2&id=EJ840752"><span><span class="hlt">Imaging</span> Genetics</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Munoz, Karen E.; Hyde, Luke W.; Hariri, Ahmad R.</p>
         <p>2009-01-01</p>
         <p><span class="hlt">Imaging</span> genetics is an experimental strategy that integrates molecular genetics and neuroimaging technology to examine biological mechanisms that mediate differences in behavior and the risks for psychiatric disorder. The basic principles in <span class="hlt">imaging</span> genetics and the development of the field are discussed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3131209','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3131209"><span>Retinal <span class="hlt">Imaging</span> and <span class="hlt">Image</span> Analysis</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Abràmoff, Michael D.; Garvin, Mona K.; Sonka, Milan</p>
         <p>2011-01-01</p>
         <p>Many important eye diseases as well as systemic diseases manifest themselves in the retina. While a number of other anatomical structures contribute to the process of vision, this review focuses on retinal <span class="hlt">imaging</span> and <span class="hlt">image</span> analysis. Following a brief overview of the most prevalent causes of blindness in the industrialized world that includes age-related macular degeneration, diabetic retinopathy, and glaucoma, the review is devoted to retinal <span class="hlt">imaging</span> and <span class="hlt">image</span> analysis methods and their clinical implications. Methods for 2-D fundus <span class="hlt">imaging</span> and techniques for 3-D optical coherence tomography (OCT) <span class="hlt">imaging</span> are reviewed. Special attention is given to quantitative techniques for analysis of fundus photographs with a focus on clinically relevant assessment of retinal vasculature, identification of retinal lesions, assessment of optic nerve head (ONH) shape, building retinal atlases, and to automated methods for population screening for retinal diseases. A separate section is devoted to 3-D analysis of OCT <span class="hlt">images</span>, describing methods for segmentation and analysis of retinal layers, retinal vasculature, and 2-D/3-D detection of symptomatic exudate-associated derangements, as well as to OCT-based analysis of ONH morphology and shape. Throughout the paper, aspects of <span class="hlt">image</span> acquisition, <span class="hlt">image</span> analysis, and clinical relevance are treated together considering their mutually interlinked relationships. PMID:22275207</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/22275207','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/22275207"><span>Retinal <span class="hlt">imaging</span> and <span class="hlt">image</span> analysis.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Abràmoff, Michael D; Garvin, Mona K; Sonka, Milan</p>
         <p>2010-01-01</p>
         <p>Many important eye diseases as well as systemic diseases manifest themselves in the retina. While a number of other anatomical structures contribute to the process of vision, this review focuses on retinal <span class="hlt">imaging</span> and <span class="hlt">image</span> analysis. Following a brief overview of the most prevalent causes of blindness in the industrialized world that includes age-related macular degeneration, diabetic retinopathy, and glaucoma, the review is devoted to retinal <span class="hlt">imaging</span> and <span class="hlt">image</span> analysis methods and their clinical implications. Methods for 2-D fundus <span class="hlt">imaging</span> and techniques for 3-D optical coherence tomography (OCT) <span class="hlt">imaging</span> are reviewed. Special attention is given to quantitative techniques for analysis of fundus photographs with a focus on clinically relevant assessment of retinal vasculature, identification of retinal lesions, assessment of optic nerve head (ONH) shape, building retinal atlases, and to automated methods for population screening for retinal diseases. A separate section is devoted to 3-D analysis of OCT <span class="hlt">images</span>, describing methods for segmentation and analysis of retinal layers, retinal vasculature, and 2-D/3-D detection of symptomatic exudate-associated derangements, as well as to OCT-based analysis of ONH morphology and shape. Throughout the paper, aspects of <span class="hlt">image</span> acquisition, <span class="hlt">image</span> analysis, and clinical relevance are treated together considering their mutually interlinked relationships.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li class="active"><span>1</span></li>
   <li><a href="#" onclick='return showDiv("page_2");'>2</a></li>
      <li><a href="#" onclick='return showDiv("page_3");'>3</a></li>
      <li><a href="#" onclick='return showDiv("page_4");'>4</a></li>
      <li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_1 -->
   <div id="page_2" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_1");'>1</a></li>
      <li class="active"><span>2</span></li>
   <li><a href="#" onclick='return showDiv("page_3");'>3</a></li>
      <li><a href="#" onclick='return showDiv("page_4");'>4</a></li>
      <li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="21">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=applications+AND+nuclear+AND+technology&pg=2&id=EJ273242','ERIC'); return false;" href="https://eric.ed.gov/?q=applications+AND+nuclear+AND+technology&pg=2&id=EJ273242"><span>Medical <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Jaffe, C. Carl</p>
         <p>1982-01-01</p>
         <p>Describes principle <span class="hlt">imaging</span> techniques, their applications, and their limitations in terms of diagnostic capability and possible adverse biological effects. Techniques include film radiography, computed tomography, nuclear medicine, positron emission tomography (PET), ultrasonography, nuclear magnetic resonance, and digital radiography. PET has…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020078337','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020078337"><span><span class="hlt">Imaging</span> System</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1995-01-01</p>
         <p>The 1100C Virtual Window is based on technology developed under NASA Small Business Innovation (SBIR) contracts to Ames Research Center. For example, under one contract Dimension Technologies, Inc. developed a large autostereoscopic display for scientific visualization applications. The Virtual Window employs an innovative illumination system to deliver the depth and color of true 3D <span class="hlt">imaging</span>. Its applications include surgery and Magnetic Resonance <span class="hlt">Imaging</span> scans, viewing for teleoperated robots, training, and in aviation cockpit displays.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1986MIzRS.........V','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1986MIzRS.........V"><span><span class="hlt">Image</span> reconstruction</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Vasilenko, Georgii Ivanovich; Taratorin, Aleksandr Markovich</p>
         <p></p>
         <p>Linear, nonlinear, and iterative <span class="hlt">image</span>-reconstruction (IR) algorithms are reviewed. Theoretical results are presented concerning controllable linear filters, the solution of ill-posed functional minimization problems, and the regularization of iterative IR algorithms. Attention is also given to the problem of superresolution and analytical spectrum continuation, the solution of the phase problem, and the reconstruction of <span class="hlt">images</span> distorted by turbulence. IR in optical and optical-digital systems is discussed with emphasis on holographic techniques.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.sil.si.edu/imagegalaxy/imageGalaxy_FeaturedImage.cfm','SCIGOVWS'); return false;" href="http://www.sil.si.edu/imagegalaxy/imageGalaxy_FeaturedImage.cfm"><span>Featured <span class="hlt">Image</span> | Galaxy of <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.science.gov/aboutsearch.html">Science.gov Websites</a></p>
      <p></p>
         <p></p>
         <p>our most popular <span class="hlt">images</span> is that of renowned <em>female</em> scientist (and the first recipient of two Nobel cameras as the perfect way to capture summer memories. This adventurous <em>female</em> copilot attempts to</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20070034890','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20070034890"><span>Stellar <span class="hlt">Imager</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Carpenter, Kenneth</p>
         <p>2007-01-01</p>
         <p>The Stellar <span class="hlt">Imager</span> (SI) is one of NASA's "Vision Missions" - concepts for future, space-based, strategic missions that could enormously increase our capabilities for observing the Cosmos. SI is designed as a UV/Optical Interferometer which will enable 0.1 milli-arcsecond (mas) spectral <span class="hlt">imaging</span> of stellar surfaces and, via asteroseismology, stellar interiors and of the Universe in general. The ultra-sharp <span class="hlt">images</span> of the Stellar <span class="hlt">Imager</span> will revolutionize our view of many dynamic astrophysical processes by transforming point sources into extended sources, and snapshots into evolving views. SI, with a characteristic angular resolution of 0.1 milli-arcseconds at 2000 Angstroms, represents an advance in <span class="hlt">image</span> detail of several hundred times over that provided by the Hubble Space Telescope. The Stellar <span class="hlt">Imager</span> will zoom in on what today-with few exceptions - we only know as point sources, revealing processes never before seen, thus providing a tool as fundamental to astrophysics as the microscope is to the study of life on Earth. SI's science focuses on the role of magnetism in the Universe, particularly on magnetic activity on the surfaces of stars like the Sun. It's prime goal is to enable long-term forecasting of solar activity and the space weather that it drives, in support of the Living With a Star program in the Exploration Era. SI will also revolutionize our understanding of the formation of planetary systems, of the habitability and climatology of distant planets, and of many magneto-hydrodynamically controlled processes in the Universe. Stellar <span class="hlt">Imager</span> is included as a "Flagship and Landmark Discovery Mission" in the 2005 Sun Solar System Connection (SSSC) Roadmap and as a candidate for a "Pathways to Life Observatory" in the Exploration of the Universe Division (EUD) Roadmap (May, 2005) and as such is a candidate mission for the 2025-2030 timeframe. An artist's drawing of the current "baseline" concept for SI is presented.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2013lema.book..115G','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2013lema.book..115G"><span>Narrowband <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Goldman, Don S.</p>
         <p></p>
         <p>The Hubble Space Telescope (HST) captured the attention of the world when it released its astounding <span class="hlt">image</span> in 1995 of the Eagle Nebula (Messier 16) often called "The Pillars of Creation" (Fig. 1). It contained dark, billowing towers of gas and dust rising majestically into a background of glowing radiation. It told a story of new star formation.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/321302','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/biblio/321302"><span><span class="hlt">Imaging</span> bolometer</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Wurden, G.A.</p>
         <p>1999-01-19</p>
         <p>Radiation-hard, steady-state <span class="hlt">imaging</span> bolometer is disclosed. A bolometer employing infrared (IR) <span class="hlt">imaging</span> of a segmented-matrix absorber of plasma radiation in a cooled-pinhole camera geometry is described. The bolometer design parameters are determined by modeling the temperature of the foils from which the absorbing matrix is fabricated by using a two-dimensional time-dependent solution of the heat conduction equation. The resulting design will give a steady-state bolometry capability, with approximately 100 Hz time resolution, while simultaneously providing hundreds of channels of spatial information. No wiring harnesses will be required, as the temperature-rise data will be measured via an IR camera. The resulting spatial data may be used to tomographically investigate the profile of plasmas. 2 figs.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/872108','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/872108"><span><span class="hlt">Imaging</span> bolometer</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Wurden, Glen A.</p>
         <p>1999-01-01</p>
         <p>Radiation-hard, steady-state <span class="hlt">imaging</span> bolometer. A bolometer employing infrared (IR) <span class="hlt">imaging</span> of a segmented-matrix absorber of plasma radiation in a cooled-pinhole camera geometry is described. The bolometer design parameters are determined by modeling the temperature of the foils from which the absorbing matrix is fabricated by using a two-dimensional time-dependent solution of the heat conduction equation. The resulting design will give a steady-state bolometry capability, with approximately 100 Hz time resolution, while simultaneously providing hundreds of channels of spatial information. No wiring harnesses will be required, as the temperature-rise data will be measured via an IR camera. The resulting spatial data may be used to tomographically investigate the profile of plasmas.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020086393','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020086393"><span><span class="hlt">Image</span> Processing</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1991-01-01</p>
         <p>The Computer Graphics Center of North Carolina State University uses LAS, a COSMIC program, to analyze and manipulate data from Landsat and SPOT providing information for government and commercial land resource application projects. LAS is used to interpret aircraft/satellite data and enables researchers to improve <span class="hlt">image</span>-based classification accuracies. The system is easy to use and has proven to be a valuable remote sensing training tool.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA282221','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA282221"><span>Biomedical <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1994-04-01</p>
         <p>distribution unlimited. United States Army Aeromedical Research Laboratory Fort Rucker, Alabama 36362-0577 Qualified recuesters Qualified requesters may...FUNDING NUMBER5 I PROGRAM zfJECT TASK WORK UNIT ELEMENT NO. NO. ACCESSION NO. 62787A 30162787A87$ EA 138 Biomedical <span class="hlt">Imaging</span> 12. PERSONAL AUTHOR(S...times larger. Usually they are expensive with commercially available units starting at around $100,000. Triangulation sensors are capable of range</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA437378','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA437378"><span><span class="hlt">Image</span> Inpainting</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2005-01-01</p>
         <p>more legible and to restore its unity [2]. The need to retouch the <span class="hlt">image</span> in an unobtrusive way extended naturally from paintings to photography and...to software tools that allow a sophisticated but mostly manual process [7]. In this article we introduce a novel algorithm for automatic digi- tal...This is done only for a didactic purpose, since our algorithm was devised for 2D, and there are other techniques (such as splines) that might yield</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://pubs.er.usgs.gov/publication/70027377','USGSPUBS'); return false;" href="https://pubs.er.usgs.gov/publication/70027377"><span><span class="hlt">Imaging</span> Borrelly</span></a></p>
      <p><a target="_blank" href="http://pubs.er.usgs.gov/pubs/index.jsp?view=adv">USGS Publications Warehouse</a></p>
      <p>Soderblom, L.A.; Boice, D.C.; Britt, D.T.; Brown, R.H.; Buratti, B.J.; Kirk, R.L.; Lee, M.; Nelson, R.M.; Oberst, J.; Sandel, B.R.; Stern, S.A.; Thomas, N.; Yelle, R.V.</p>
         <p>2004-01-01</p>
         <p>The nucleus, coma, and dust jets of short-period Comet 19P/Borrelly were <span class="hlt">imaged</span> from the Deep Space 1 spacecraft during its close flyby in September 2001. A prominent jet dominated the near-nucleus coma and emanated roughly normal to the long axis of nucleus from a broad central cavity. We show it to have remained fixed in position for more than 34 hr, much longer than the 26-hr rotation period. This confirms earlier suggestions that it is co-aligned with the rotation axis. From a combination of fitting the nucleus light curve from approach <span class="hlt">images</span> and the nucleus' orientation from stereo <span class="hlt">images</span> at encounter, we conclude that the sense of rotation is right-handed around the main jet vector. The inferred rotation pole is approximately perpendicular to the long axis of the nucleus, consistent with a simple rotational state. Lacking an existing IAU comet-specific convention but applying a convention provisionally adopted for asteroids, we label this the north pole. This places the sub-solar latitude at ???60?? N at the time of the perihelion with the north pole in constant sunlight and thus receiving maximum average insolation. ?? 2003 Elsevier Inc. All rights reserved.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-GSFC_20171208_Archive_e000971.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-GSFC_20171208_Archive_e000971.html"><span>Combined <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2017-12-08</p>
         <p>Four different instruments on SOHO show a large CME on Nov. 6, 1997. The sun is at the center, with three coronagraph <span class="hlt">images</span> of different sizes around it. The streaks of white light are from protons hitting the SOHO cameras producing a snowy effect typical of a significant flare. ..Credit: NASA/SOHO..---..CME WEEK: What To See in CME <span class="hlt">Images</span> Two main types of explosions occur on the sun: solar flares and coronal mass ejections. Unlike the energy and x-rays produced in a solar flare – which can reach Earth at the speed of light in eight minutes – coronal mass ejections are giant, expanding clouds of solar material that take one to three days to reach Earth. Once at Earth, these ejections, also called CMEs, can impact satellites in space or interfere with radio communications. During CME WEEK from Sept. 22 to 26, 2014, we explore different aspects of these giant eruptions that surge out from the star we live with. When a coronal mass ejection blasts off the sun, scientists rely on instruments called coronagraphs to track their progress. Coronagraphs block out the bright light of the sun, so that the much fainter material in the solar atmosphere -- including CMEs -- can be seen in the surrounding space. CMEs appear in these <span class="hlt">images</span> as expanding shells of material from the sun's atmosphere -- sometimes a core of colder, solar material (called a filament) from near the sun's surface moves in the center. But mapping out such three-dimensional components from a two-dimensional <span class="hlt">image</span> isn't easy. Watch the slideshow to find out how scientists interpret what they see in CME pictures. The <span class="hlt">images</span> in the slideshow are from the three sets of coronagraphs NASA currently has in space. One is on the joint European Space Agency and NASA Solar and Heliospheric Observatory, or SOHO. SOHO launched in 1995, and sits between Earth and the sun about a million miles away from Earth. The other two coronagraphs are on the two spacecraft of the NASA Solar Terrestrial Relations Observatory</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-jsc2001e26680.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-jsc2001e26680.html"><span>PIRS <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2001-01-01</p>
         <p>JSC2001-E-26680 --- One of a series of three photos of the next station module that will launch--the Russian Docking Compartment, named Pirs, the Russian word for pier. The module is planned for launch from Baikonur Sept. 14, and to dock with the station on Sept. 16. It will serve as a Russian airlock for the station and also will provide a docking port for Soyuz or Progress craft arriving at the station. This <span class="hlt">image</span> shows the Pirs under construction at Energia in Moscow.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-jsc2001e26679.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-jsc2001e26679.html"><span>PIRS <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2001-01-01</p>
         <p>JSC2001-E-26679 --- One of a series of three photos of the next station module that will launch--the Russian Docking Compartment, named Pirs, the Russian word for pier. The module is planned for launch from Baikonur Sept. 14, and to dock with the station on Sept. 16. It will serve as a Russian airlock for the station and also will provide a docking port for Soyuz or Progress craft arriving at the station. This <span class="hlt">image</span> shows the Pirs under construction at Energia in Moscow.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/28760235','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/28760235"><span>Pituitary <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Pressman, Barry D</p>
         <p>2017-09-01</p>
         <p>Modern pituitary <span class="hlt">imaging</span> is MRI. However, computed tomography (CT) still has limited usefulness. In addition, because CT offers much better bone detail and calcium detection, there are some cases in which such additional information is necessary. Before the advent of CT, plain radiography, pneumoencephalography, and angiography were used to diagnose pituitary masses. More recently, CT, and then especially MRI, made it possible to primarily delineate lesions within and around the pituitary gland rather than depend on secondary information that could only suggest their presence. Copyright © 2017 Elsevier Inc. All rights reserved.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020087606','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020087606"><span><span class="hlt">Image</span> Processor</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1989-01-01</p>
         <p>Texas Instruments Programmable Remapper is a research tool used to determine how to best utilize the part of a patient's visual field still usable by mapping onto his field of vision with manipulated imagery. It is an offshoot of a NASA program for speeding up, improving the accuracy of pattern recognition in video imagery. The Remapper enables an <span class="hlt">image</span> to be "pushed around" so more of it falls into the functional portions in the retina of a low vision person. It works at video rates, and researchers hope to significantly reduce its size and cost, creating a wearable prosthesis for visually impaired people.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA096025','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA096025"><span>Radar <span class="hlt">Image</span> Interpretability Analysis.</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1981-01-01</p>
         <p>the measured <span class="hlt">image</span> properties with respect to <span class="hlt">image</span> utility changed with <span class="hlt">image</span> application. This study has provided useful information as to how...Eneea.d) ABSTRACT The utility of radar <span class="hlt">images</span> with respect to trained <span class="hlt">image</span> inter - preter ability to identify, classify and detect specific terrain... changed with <span class="hlt">image</span> applica- tion. This study has provided useful information as to how certain <span class="hlt">image</span> characteristics relate to radar <span class="hlt">image</span> utility as</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.sil.si.edu/imagegalaxy/imageGalaxy_UseFees.cfm','SCIGOVWS'); return false;" href="http://www.sil.si.edu/imagegalaxy/imageGalaxy_UseFees.cfm"><span><span class="hlt">Image</span> Use Fees | Galaxy of <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.science.gov/aboutsearch.html">Science.gov Websites</a></p>
      <p></p>
         <p></p>
         <p>This site has moved! Please go to our new <em><span class="hlt">Image</span></em> Gallery site! dot header <em><span class="hlt">Image</span></em> Use Fees Licensing , research and study purposes only. For current pricing, please download our <em><span class="hlt">Image</span></em> Use Fee Schedule See our Frequently Asked Questions (FAQ) list for additional information. Purchase an <em><span class="hlt">image</span></em> now Contact Information</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020086381','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020086381"><span>Medical <span class="hlt">Imaging</span> System</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1991-01-01</p>
         <p>The MD <span class="hlt">Image</span> System, a true-color <span class="hlt">image</span> processing system that serves as a diagnostic aid and tool for storage and distribution of <span class="hlt">images</span>, was developed by Medical <span class="hlt">Image</span> Management Systems, Huntsville, AL, as a "spinoff from a spinoff." The original spinoff, Geostar 8800, developed by Crystal <span class="hlt">Image</span> Technologies, Huntsville, incorporates advanced UNIX versions of ELAS (developed by NASA's Earth Resources Laboratory for analysis of Landsat <span class="hlt">images</span>) for general purpose <span class="hlt">image</span> processing. The MD <span class="hlt">Image</span> System is an application of this technology to a medical system that aids in the diagnosis of cancer, and can accept, store and analyze <span class="hlt">images</span> from other sources such as Magnetic Resonance <span class="hlt">Imaging</span>.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_1");'>1</a></li>
      <li class="active"><span>2</span></li>
   <li><a href="#" onclick='return showDiv("page_3");'>3</a></li>
      <li><a href="#" onclick='return showDiv("page_4");'>4</a></li>
      <li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_2 -->
   <div id="page_3" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_1");'>1</a></li>
      <li><a href="#" onclick='return showDiv("page_2");'>2</a></li>
      <li class="active"><span>3</span></li>
   <li><a href="#" onclick='return showDiv("page_4");'>4</a></li>
      <li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="41">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/29801355','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/29801355"><span>Twin <span class="hlt">imaging</span> phenomenon of integral <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Hu, Juanmei; Lou, Yimin; Wu, Fengmin; Chen, Aixi</p>
         <p>2018-05-14</p>
         <p>The <span class="hlt">imaging</span> principles and phenomena of integral <span class="hlt">imaging</span> technique have been studied in detail using geometrical optics, wave optics, or light filed theory. However, most of the conclusions are only suit for the integral <span class="hlt">imaging</span> systems using diffused illumination. In this work, a kind of twin <span class="hlt">imaging</span> phenomenon and mechanism has been observed in a non-diffused illumination reflective integral <span class="hlt">imaging</span> system. Interactive twin <span class="hlt">images</span> including a real and a virtual 3D <span class="hlt">image</span> of one object can be activated in the system. The <span class="hlt">imaging</span> phenomenon is similar to the conjugate <span class="hlt">imaging</span> effect of hologram, but it base on the refraction and reflection instead of diffraction. The <span class="hlt">imaging</span> characteristics and mechanisms different from traditional integral <span class="hlt">imaging</span> are deduced analytically. Thin film integral <span class="hlt">imaging</span> systems with 80μm thickness have also been made to verify the <span class="hlt">imaging</span> phenomenon. Vivid lighting interactive twin 3D <span class="hlt">images</span> have been realized using a light-emitting diode (LED) light source. When the LED is moving, the twin 3D <span class="hlt">images</span> are moving synchronously. This interesting phenomenon shows a good application prospect in interactive 3D display, argument reality, and security authentication.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2398810','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2398810"><span>Medical <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Kreel, L.</p>
         <p>1991-01-01</p>
         <p>There is now a wide choice of medical <span class="hlt">imaging</span> to show both focal and diffuse pathologies in various organs. Conventional radiology with plain films, fluoroscopy and contrast medium have many advantages, being readily available with low-cost apparatus and a familiarity that almost leads to contempt. The use of plain films in chest disease and in trauma does not need emphasizing, yet there are still too many occasions when the answer obtainable from a plain radiograph has not been available. The film may have been mislaid, or the examination was not requested, or the radiograph had been misinterpreted. The converse is also quite common. Examinations are performed that add nothing to patient management, such as skull films when CT will in any case be requested or views of the internal auditory meatus and heal pad thickness in acromegaly, to quote some examples. Other issues are more complicated. Should the patient who clinically has gall-bladder disease have more than a plain film that shows gall-stones? If the answer is yes, then why request a plain film if sonography will in any case be required to 'exclude' other pathologies especially of the liver or pancreas? But then should cholecystography, CT or scintigraphy be added for confirmation? Quite clearly there will be individual circumstances to indicate further <span class="hlt">imaging</span> after sonography but in the vast majority of patients little or no extra information will be added. Statistics on accuracy and specificity will, in the case of gall-bladder pathology, vary widely if adenomyomatosis is considered by some to be a cause of symptoms or if sonographic examinations 'after fatty meals' are performed. The arguments for or against routine contrast urography rather than sonography are similar but the possibility of contrast reactions and the need to limit ionizing radiation must be borne in mind. These diagnostic strategies are also being influenced by their cost and availability; purely pragmatic considerations are not</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/25024921','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/25024921"><span>scikit-<span class="hlt">image</span>: <span class="hlt">image</span> processing in Python.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>van der Walt, Stéfan; Schönberger, Johannes L; Nunez-Iglesias, Juan; Boulogne, François; Warner, Joshua D; Yager, Neil; Gouillart, Emmanuelle; Yu, Tony</p>
         <p>2014-01-01</p>
         <p>scikit-<span class="hlt">image</span> is an <span class="hlt">image</span> processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal Modified BSD open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-<span class="hlt">image</span> library, and we showcase several real-world <span class="hlt">image</span> processing applications that use scikit-<span class="hlt">image</span>. More information can be found on the project homepage, http://scikit-<span class="hlt">image</span>.org.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4081273','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4081273"><span>scikit-<span class="hlt">image</span>: <span class="hlt">image</span> processing in Python</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Schönberger, Johannes L.; Nunez-Iglesias, Juan; Boulogne, François; Warner, Joshua D.; Yager, Neil; Gouillart, Emmanuelle; Yu, Tony</p>
         <p>2014-01-01</p>
         <p>scikit-<span class="hlt">image</span> is an <span class="hlt">image</span> processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal Modified BSD open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-<span class="hlt">image</span> library, and we showcase several real-world <span class="hlt">image</span> processing applications that use scikit-<span class="hlt">image</span>. More information can be found on the project homepage, http://scikit-<span class="hlt">image</span>.org. PMID:25024921</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/1524979','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/1524979"><span>Pediatric chest <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Gross, G W</p>
         <p>1992-10-01</p>
         <p>The highlight of recent articles published on pediatric chest <span class="hlt">imaging</span> is the potential advantage of digital <span class="hlt">imaging</span> of the infant's chest. Digital chest <span class="hlt">imaging</span> allows accurate determination of functional residual capacity as well as manipulation of the <span class="hlt">image</span> to highlight specific anatomic features. Reusable photostimulable phosphor <span class="hlt">imaging</span> systems provide wide <span class="hlt">imaging</span> latitude and lower patient dose. In addition, digital radiology permits multiple remote-site viewing on monitor displays. Several excellent reviews of the <span class="hlt">imaging</span> features of various thoracic abnormalities and the application of newer <span class="hlt">imaging</span> modalities, such as ultrafast CT and MR <span class="hlt">imaging</span> to the pediatric chest, are additional highlights.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1994IJMPC...5..151H','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1994IJMPC...5..151H"><span>Prospects for <span class="hlt">Image</span> Restoration</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Hunt, B. R.</p>
         <p></p>
         <p><span class="hlt">Image</span> restoration is the theory and practice of processing an <span class="hlt">image</span> to correct it for distortions caused by the <span class="hlt">image</span> formation process. The first efforts in <span class="hlt">image</span> restoration appeared more than 25 years ago. In this article we review the more recent trends in <span class="hlt">image</span> restoration and discuss the main directions that are expected to influence the continued evolution of this technology.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1175365','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1175365"><span>Split <span class="hlt">image</span> optical display</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Veligdan, James T.</p>
         <p>2005-05-31</p>
         <p>A video <span class="hlt">image</span> is displayed from an optical panel by splitting the <span class="hlt">image</span> into a plurality of <span class="hlt">image</span> components, and then projecting the <span class="hlt">image</span> components through corresponding portions of the panel to collectively form the <span class="hlt">image</span>. Depth of the display is correspondingly reduced.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/909422','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/909422"><span>Split <span class="hlt">image</span> optical display</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Veligdan, James T [Manorville, NY</p>
         <p>2007-05-29</p>
         <p>A video <span class="hlt">image</span> is displayed from an optical panel by splitting the <span class="hlt">image</span> into a plurality of <span class="hlt">image</span> components, and then projecting the <span class="hlt">image</span> components through corresponding portions of the panel to collectively form the <span class="hlt">image</span>. Depth of the display is correspondingly reduced.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20120002953','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20120002953"><span>Smart <span class="hlt">Image</span> Enhancement Process</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Jobson, Daniel J. (Inventor); Rahman, Zia-ur (Inventor); Woodell, Glenn A. (Inventor)</p>
         <p>2012-01-01</p>
         <p>Contrast and lightness measures are used to first classify the <span class="hlt">image</span> as being one of non-turbid and turbid. If turbid, the original <span class="hlt">image</span> is enhanced to generate a first enhanced <span class="hlt">image</span>. If non-turbid, the original <span class="hlt">image</span> is classified in terms of a merged contrast/lightness score based on the contrast and lightness measures. The non-turbid <span class="hlt">image</span> is enhanced to generate a second enhanced <span class="hlt">image</span> when a poor contrast/lightness score is associated therewith. When the second enhanced <span class="hlt">image</span> has a poor contrast/lightness score associated therewith, this <span class="hlt">image</span> is enhanced to generate a third enhanced <span class="hlt">image</span>. A sharpness measure is computed for one <span class="hlt">image</span> that is selected from (i) the non-turbid <span class="hlt">image</span>, (ii) the first enhanced <span class="hlt">image</span>, (iii) the second enhanced <span class="hlt">image</span> when a good contrast/lightness score is associated therewith, and (iv) the third enhanced <span class="hlt">image</span>. If the selected <span class="hlt">image</span> is not-sharp, it is sharpened to generate a sharpened <span class="hlt">image</span>. The final <span class="hlt">image</span> is selected from the selected <span class="hlt">image</span> and the sharpened <span class="hlt">image</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.merckmanuals.com/home/special-subjects/common-imaging-tests/overview-of-imaging-tests','NIH-MEDLINEPLUS'); return false;" href="https://www.merckmanuals.com/home/special-subjects/common-imaging-tests/overview-of-imaging-tests"><span>Overview of <span class="hlt">Imaging</span> Tests</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>... Overview of <span class="hlt">Imaging</span> Tests Angiography Computed Tomography (CT) Magnetic Resonance <span class="hlt">Imaging</span> (MRI) Plain X-Rays Radionuclide Scanning ... and radionuclide scanning Sound waves, as in ultrasonography Magnetic fields, as in magnetic resonance <span class="hlt">imaging</span> (MRI) Substances ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19780000601&hterms=elec&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D40%26Ntt%3Delec','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19780000601&hterms=elec&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D40%26Ntt%3Delec"><span>Postprocessing classification <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Kan, E. P.</p>
         <p>1979-01-01</p>
         <p>Program cleans up remote-sensing maps. It can be used with existing <span class="hlt">image</span>-processing software. Remapped <span class="hlt">images</span> closely resemble familiar resource information maps and can replace or supplement classification <span class="hlt">images</span> not postprocessed by this program.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.ismrm.org/resources/information-for-patients/','NIH-MEDLINEPLUS'); return false;" href="http://www.ismrm.org/resources/information-for-patients/"><span>Magnetic Resonance <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>... specific information about your own examination. What is magnetic resonance <span class="hlt">imaging</span> (MRI)? What is MRI used for? How safe ... What is the MRI examination like? What is magnetic resonance <span class="hlt">imaging</span> (MRI)? MRI, or magnetic resonance <span class="hlt">imaging</span>, is a ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=image+AND+j&pg=2&id=EJ790321','ERIC'); return false;" href="https://eric.ed.gov/?q=image+AND+j&pg=2&id=EJ790321"><span>What Is an <span class="hlt">Image</span>?</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Gerber, Andrew J.; Peterson, Bradley S.</p>
         <p>2008-01-01</p>
         <p>The article helps to understand the interpretation of an <span class="hlt">image</span> by presenting as to what constitutes an <span class="hlt">image</span>. A common feature in all <span class="hlt">images</span> is the basic physical structure that can be described with a common set of terms.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=electronic+AND+document+AND+management&pg=2&id=EJ531046','ERIC'); return false;" href="https://eric.ed.gov/?q=electronic+AND+document+AND+management&pg=2&id=EJ531046"><span>To <span class="hlt">Image</span>...or Not to <span class="hlt">Image</span>?</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Bruley, Karina</p>
         <p>1996-01-01</p>
         <p>Provides a checklist of considerations for installing document <span class="hlt">image</span> processing with an electronic document management system. Other topics include scanning; indexing; the <span class="hlt">image</span> file life cycle; benefits of <span class="hlt">imaging</span>; document-driven workflow; and planning for workplace changes like postsorting, creating a scanning room, redeveloping job tasks and…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2680925','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2680925"><span><span class="hlt">Imaging</span> Oncogene Expression</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Mukherjee, Archana; Wickstrom, Eric</p>
         <p>2009-01-01</p>
         <p>This review briefly outlines the importance of molecular <span class="hlt">imaging</span>, particularly <span class="hlt">imaging</span> of endogenous gene expression for noninvasive genetic analysis of radiographic masses. The concept of antisense <span class="hlt">imaging</span> agents and the advantages and challenges in the development of hybridization probes for in vivo <span class="hlt">imaging</span> are described. An overview of the investigations on oncogene expression <span class="hlt">imaging</span> is given. Finally, the need for further improvement in antisense-based <span class="hlt">imaging</span> agents and directions to improve oncogene mRNA targeting is stated. PMID:19264436</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=20000040110&hterms=etc+stock&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D40%26Ntt%3Detc%2Bstock','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=20000040110&hterms=etc+stock&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D40%26Ntt%3Detc%2Bstock"><span>Far Ultraviolet <span class="hlt">Imaging</span> from the <span class="hlt">Image</span> Spacecraft</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Mende, S. B.; Heetderks, H.; Frey, H. U.; Lampton, M.; Geller, S. P.; Stock, J. M.; Abiad, R.; Siegmund, O. H. W.; Tremsin, A. S.; Habraken, S.</p>
         <p>2000-01-01</p>
         <p>Direct <span class="hlt">imaging</span> of the magnetosphere by the <span class="hlt">IMAGE</span> spacecraft will be supplemented by observation of the global aurora. The <span class="hlt">IMAGE</span> satellite instrument complement includes three Far Ultraviolet (FUV) instruments. The Wideband <span class="hlt">Imaging</span> Camera (WIC) will provide broad band ultraviolet <span class="hlt">images</span> of the aurora for maximum spatial and temporal resolution by <span class="hlt">imaging</span> the LBH N2 bands of the aurora. The Spectrographic <span class="hlt">Imager</span> (SI), a novel form of monochromatic <span class="hlt">imager</span>, will <span class="hlt">image</span> the aurora, filtered by wavelength. The proton-induced component of the aurora will be <span class="hlt">imaged</span> separately by measuring the Doppler-shifted Lyman-a. Finally, the GEO instrument will observe the distribution of the geocoronal emission to obtain the neutral background density source for charge exchange in the magnetosphere. The FUV instrument complement looks radially outward from the rotating <span class="hlt">IMAGE</span> satellite and, therefore, it spends only a short time observing the aurora and the Earth during each spin. To maximize photon collection efficiency and use efficiently the short time available for exposures the FUV auroral <span class="hlt">imagers</span> WIC and SI both have wide fields of view and take data continuously as the auroral region proceeds through the field of view. To minimize data volume, the set of multiple <span class="hlt">images</span> are electronically co-added by suitably shifting each <span class="hlt">image</span> to compensate for the spacecraft rotation. In order to minimize resolution loss, the <span class="hlt">images</span> have to be distort ion-corrected in real time. The distortion correction is accomplished using high speed look up tables that are pre-generated by least square fitting to polynomial functions by the on-orbit processor. The instruments were calibrated individually while on stationary platforms, mostly in vacuum chambers. Extensive ground-based testing was performed with visible and near UV simulators mounted on a rotating platform to emulate their performance on a rotating spacecraft.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3746120','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3746120"><span><span class="hlt">Image</span> processing and recognition for biological <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Uchida, Seiichi</p>
         <p>2013-01-01</p>
         <p>This paper reviews <span class="hlt">image</span> processing and pattern recognition techniques, which will be useful to analyze bioimages. Although this paper does not provide their technical details, it will be possible to grasp their main tasks and typical tools to handle the tasks. <span class="hlt">Image</span> processing is a large research area to improve the visibility of an input <span class="hlt">image</span> and acquire some valuable information from it. As the main tasks of <span class="hlt">image</span> processing, this paper introduces gray-level transformation, binarization, <span class="hlt">image</span> filtering, <span class="hlt">image</span> segmentation, visual object tracking, optical flow and <span class="hlt">image</span> registration. <span class="hlt">Image</span> pattern recognition is the technique to classify an input <span class="hlt">image</span> into one of the predefined classes and also has a large research area. This paper overviews its two main modules, that is, feature extraction module and classification module. Throughout the paper, it will be emphasized that bioimage is a very difficult target for even state-of-the-art <span class="hlt">image</span> processing and pattern recognition techniques due to noises, deformations, etc. This paper is expected to be one tutorial guide to bridge biology and <span class="hlt">image</span> processing researchers for their further collaboration to tackle such a difficult target. PMID:23560739</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/23560739','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/23560739"><span><span class="hlt">Image</span> processing and recognition for biological <span class="hlt">images</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Uchida, Seiichi</p>
         <p>2013-05-01</p>
         <p>This paper reviews <span class="hlt">image</span> processing and pattern recognition techniques, which will be useful to analyze bioimages. Although this paper does not provide their technical details, it will be possible to grasp their main tasks and typical tools to handle the tasks. <span class="hlt">Image</span> processing is a large research area to improve the visibility of an input <span class="hlt">image</span> and acquire some valuable information from it. As the main tasks of <span class="hlt">image</span> processing, this paper introduces gray-level transformation, binarization, <span class="hlt">image</span> filtering, <span class="hlt">image</span> segmentation, visual object tracking, optical flow and <span class="hlt">image</span> registration. <span class="hlt">Image</span> pattern recognition is the technique to classify an input <span class="hlt">image</span> into one of the predefined classes and also has a large research area. This paper overviews its two main modules, that is, feature extraction module and classification module. Throughout the paper, it will be emphasized that bioimage is a very difficult target for even state-of-the-art <span class="hlt">image</span> processing and pattern recognition techniques due to noises, deformations, etc. This paper is expected to be one tutorial guide to bridge biology and <span class="hlt">image</span> processing researchers for their further collaboration to tackle such a difficult target. © 2013 The Author Development, Growth & Differentiation © 2013 Japanese Society of Developmental Biologists.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19880014766','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19880014766"><span><span class="hlt">Image</span> management research</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Watson, Andrew B.</p>
         <p>1988-01-01</p>
         <p>Two types of research issues are involved in <span class="hlt">image</span> management systems with space station applications: <span class="hlt">image</span> processing research and <span class="hlt">image</span> perception research. The <span class="hlt">image</span> processing issues are the traditional ones of digitizing, coding, compressing, storing, analyzing, and displaying, but with a new emphasis on the constraints imposed by the human perceiver. Two <span class="hlt">image</span> coding algorithms have been developed that may increase the efficiency of <span class="hlt">image</span> management systems (IMS). <span class="hlt">Image</span> perception research involves a study of the theoretical and practical aspects of visual perception of electronically displayed <span class="hlt">images</span>. Issues include how rapidly a user can search through a library of <span class="hlt">images</span>, how to make this search more efficient, and how to present <span class="hlt">images</span> in terms of resolution and split screens. Other issues include optimal interface to an IMS and how to code <span class="hlt">images</span> in a way that is optimal for the human perceiver. A test-bed within which such issues can be addressed has been designed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2001AGUFMSH31B0714Y','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2001AGUFMSH31B0714Y"><span>Multiscale <span class="hlt">Image</span> Processing of Solar <span class="hlt">Image</span> Data</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Young, C.; Myers, D. C.</p>
         <p>2001-12-01</p>
         <p>It is often said that the blessing and curse of solar physics is too much data. Solar missions such as Yohkoh, SOHO and TRACE have shown us the Sun with amazing clarity but have also increased the amount of highly complex data. We have improved our view of the Sun yet we have not improved our analysis techniques. The standard techniques used for analysis of solar <span class="hlt">images</span> generally consist of observing the evolution of features in a sequence of byte scaled <span class="hlt">images</span> or a sequence of byte scaled difference <span class="hlt">images</span>. The determination of features and structures in the <span class="hlt">images</span> are done qualitatively by the observer. There is little quantitative and objective analysis done with these <span class="hlt">images</span>. Many advances in <span class="hlt">image</span> processing techniques have occured in the past decade. Many of these methods are possibly suited for solar <span class="hlt">image</span> analysis. Multiscale/Multiresolution methods are perhaps the most promising. These methods have been used to formulate the human ability to view and comprehend phenomena on different scales. So these techniques could be used to quantitify the <span class="hlt">imaging</span> processing done by the observers eyes and brains. In this work we present several applications of multiscale techniques applied to solar <span class="hlt">image</span> data. Specifically, we discuss uses of the wavelet, curvelet, and related transforms to define a multiresolution support for EIT, LASCO and TRACE <span class="hlt">images</span>.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_1");'>1</a></li>
      <li><a href="#" onclick='return showDiv("page_2");'>2</a></li>
      <li class="active"><span>3</span></li>
   <li><a href="#" onclick='return showDiv("page_4");'>4</a></li>
      <li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_3 -->
   <div id="page_4" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_2");'>2</a></li>
      <li><a href="#" onclick='return showDiv("page_3");'>3</a></li>
      <li class="active"><span>4</span></li>
   <li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li><a href="#" onclick='return showDiv("page_6");'>6</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="61">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.epa.gov/drupaltraining/adding-and-deleting-images','PESTICIDES'); return false;" href="https://www.epa.gov/drupaltraining/adding-and-deleting-images"><span>Adding and Deleting <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.epa.gov/pesticides/search.htm">EPA Pesticide Factsheets</a></p>
      <p></p>
         <p></p>
         <p><span class="hlt">Images</span> are added via the Drupal WebCMS Editor. Once an <span class="hlt">image</span> is uploaded onto a page, it is available via the Library and your files. You can edit the metadata, delete the <span class="hlt">image</span> permanently, and/or replace <span class="hlt">images</span> on the Files tab.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=dysmorphic&pg=2&id=EJ651394','ERIC'); return false;" href="https://eric.ed.gov/?q=dysmorphic&pg=2&id=EJ651394"><span>Adolescence and Body <span class="hlt">Image</span>.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Weinshenker, Naomi</p>
         <p>2002-01-01</p>
         <p>Discusses body <span class="hlt">image</span> among adolescents, explaining that today's adolescents are more prone to body <span class="hlt">image</span> distortions and dissatisfaction than ever and examining the historical context; how self-<span class="hlt">image</span> develops; normative discontent; body <span class="hlt">image</span> distortions; body dysmorphic disorder (BDD); vulnerability of boys (muscle dysmorphia); who is at risk;…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.sil.si.edu/imagegalaxy/imageGalaxy_Contacts.cfm','SCIGOVWS'); return false;" href="http://www.sil.si.edu/imagegalaxy/imageGalaxy_Contacts.cfm"><span>Contacts | Galaxy of <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.science.gov/aboutsearch.html">Science.gov Websites</a></p>
      <p></p>
         <p></p>
         <p>This site has moved! Please go to our new <em><span class="hlt">Image</span></em> Gallery site! dot header Contact Us About the <em><span class="hlt">Image</span></em> Galaxy For licensing and other usage questions, please contact: <em><span class="hlt">Image</span></em> use and licensing ! Enter a search term and hit the search button to quickly find an <em><span class="hlt">image</span></em> Go The above "Quick Search</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1404931','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1404931"><span>Hyperspectral <span class="hlt">imaging</span> flow cytometer</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Sinclair, Michael B.; Jones, Howland D. T.</p>
         <p>2017-10-25</p>
         <p>A hyperspectral <span class="hlt">imaging</span> flow cytometer can acquire high-resolution hyperspectral <span class="hlt">images</span> of particles, such as biological cells, flowing through a microfluidic system. The hyperspectral <span class="hlt">imaging</span> flow cytometer can provide detailed spatial maps of multiple emitting species, cell morphology information, and state of health. An optimized system can <span class="hlt">image</span> about 20 cells per second. The hyperspectral <span class="hlt">imaging</span> flow cytometer enables many thousands of cells to be characterized in a single session.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020086960','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020086960"><span><span class="hlt">Image</span> Processing Software</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1990-01-01</p>
         <p>The Ames digital <span class="hlt">image</span> velocimetry technology has been incorporated in a commercially available <span class="hlt">image</span> processing software package that allows motion measurement of <span class="hlt">images</span> on a PC alone. The software, manufactured by Werner Frei Associates, is IMAGELAB FFT. IMAGELAB FFT is a general purpose <span class="hlt">image</span> processing system with a variety of other applications, among them <span class="hlt">image</span> enhancement of fingerprints and use by banks and law enforcement agencies for analysis of videos run during robberies.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA023633','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA023633"><span><span class="hlt">Image</span> Analysis and Modeling</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1976-03-01</p>
         <p>This report summarizes the results of the research program on <span class="hlt">Image</span> Analysis and Modeling supported by the Defense Advanced Research Projects Agency...The objective is to achieve a better understanding of <span class="hlt">image</span> structure and to use this knowledge to develop improved <span class="hlt">image</span> models for use in <span class="hlt">image</span> ... analysis and processing tasks such as information extraction, <span class="hlt">image</span> enhancement and restoration, and coding. The ultimate objective of this research is</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=MSFC-0601300&hterms=ultrasound&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D40%26Ntt%3Dultrasound','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=MSFC-0601300&hterms=ultrasound&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D40%26Ntt%3Dultrasound"><span>Ultrasound <span class="hlt">Imaging</span> System Video</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>2002-01-01</p>
         <p>In this video, astronaut Peggy Whitson uses the Human Research Facility (HRF) Ultrasound <span class="hlt">Imaging</span> System in the Destiny Laboratory of the International Space Station (ISS) to <span class="hlt">image</span> her own heart. The Ultrasound <span class="hlt">Imaging</span> System provides three-dimension <span class="hlt">image</span> enlargement of the heart and other organs, muscles, and blood vessels. It is capable of high resolution <span class="hlt">imaging</span> in a wide range of applications, both research and diagnostic, such as Echocardiography (ultrasound of the heart), abdominal, vascular, gynecological, muscle, tendon, and transcranial ultrasound.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.sil.si.edu/imagegalaxy/imageGalaxy_About_About.cfm','SCIGOVWS'); return false;" href="http://www.sil.si.edu/imagegalaxy/imageGalaxy_About_About.cfm"><span>About Galaxy of <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.science.gov/aboutsearch.html">Science.gov Websites</a></p>
      <p></p>
         <p></p>
         <p>This site has moved! Please go to our new <em><span class="hlt">Image</span></em> Gallery site! dot header About the <em><span class="hlt">Image</span></em> Galaxy are added regularly. Statistics about the Galaxy of <span class="hlt">Images</span> Frequently Asked Questions <em><span class="hlt">Image</span></em> Use Fees Quick Search! Enter a search term and hit the search button to quickly find an <em><span class="hlt">image</span></em> Go The above &quot</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016SPIE.9820E..16W','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016SPIE.9820E..16W"><span><span class="hlt">Image</span> based performance analysis of thermal <span class="hlt">imagers</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Wegner, D.; Repasi, E.</p>
         <p>2016-05-01</p>
         <p>Due to advances in technology, modern thermal <span class="hlt">imagers</span> resemble sophisticated <span class="hlt">image</span> processing systems in functionality. Advanced signal and <span class="hlt">image</span> processing tools enclosed into the camera body extend the basic <span class="hlt">image</span> capturing capability of thermal cameras. This happens in order to enhance the display presentation of the captured scene or specific scene details. Usually, the implemented methods are proprietary company expertise, distributed without extensive documentation. This makes the comparison of thermal <span class="hlt">imagers</span> especially from different companies a difficult task (or at least a very time consuming/expensive task - e.g. requiring the execution of a field trial and/or an observer trial). For example, a thermal camera equipped with turbulence mitigation capability stands for such a closed system. The Fraunhofer IOSB has started to build up a system for testing thermal <span class="hlt">imagers</span> by <span class="hlt">image</span> based methods in the lab environment. This will extend our capability of measuring the classical IR-system parameters (e.g. MTF, MTDP, etc.) in the lab. The system is set up around the IR- scene projector, which is necessary for the thermal display (projection) of an <span class="hlt">image</span> sequence for the IR-camera under test. The same set of thermal test sequences might be presented to every unit under test. For turbulence mitigation tests, this could be e.g. the same turbulence sequence. During system tests, gradual variation of input parameters (e. g. thermal contrast) can be applied. First ideas of test scenes selection and how to assembly an <span class="hlt">imaging</span> suite (a set of <span class="hlt">image</span> sequences) for the analysis of <span class="hlt">imaging</span> thermal systems containing such black boxes in the <span class="hlt">image</span> forming path is discussed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA473125','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA473125"><span>Super Resolution <span class="hlt">Imaging</span> Applied to Scientific <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2007-05-01</p>
         <p>norm has found favor in the <span class="hlt">image</span> restoration community because it allows discontinuities in its solution. As opposed to the L2 norm it does not...Oxford University Press. 31) Malay Kumar Nema , S.Rakshit and S.Chaudhuri,”Edge Model Based High Resolution <span class="hlt">Image</span> Genration”Indian Conference on...Society of America, vol. 11, no. 2, pp. 572- 579, February 1994 37) M. Nema , S. Rakshit and S. Chaudhuri, ``Edge Model Based High Resolution <span class="hlt">Image</span></p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1083015','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1083015"><span><span class="hlt">Image</span> registration method for medical <span class="hlt">image</span> sequences</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Gee, Timothy F.; Goddard, James S.</p>
         <p>2013-03-26</p>
         <p><span class="hlt">Image</span> registration of low contrast <span class="hlt">image</span> sequences is provided. In one aspect, a desired region of an <span class="hlt">image</span> is automatically segmented and only the desired region is registered. Active contours and adaptive thresholding of intensity or edge information may be used to segment the desired regions. A transform function is defined to register the segmented region, and sub-pixel information may be determined using one or more interpolation methods.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19930015360','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19930015360"><span>Fast <span class="hlt">image</span> decompression for telebrowsing of <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Miaou, Shaou-Gang; Tou, Julius T.</p>
         <p>1993-01-01</p>
         <p>Progressive <span class="hlt">image</span> transmission (PIT) is often used to reduce the transmission time of an <span class="hlt">image</span> telebrowsing system. A side effect of the PIT is the increase of computational complexity at the viewer's site. This effect is more serious in transform domain techniques than in other techniques. Recent attempts to reduce the side effect are futile as they create another side effect, namely, the discontinuous and unpleasant <span class="hlt">image</span> build-up. Based on a practical assumption that <span class="hlt">image</span> blocks to be inverse transformed are generally sparse, this paper presents a method to minimize both side effects simultaneously.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2018SPIE10498E..37C','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2018SPIE10498E..37C"><span>Novel snapshot hyperspectral <span class="hlt">imager</span> for fluorescence <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Chandler, Lynn; Chandler, Andrea; Periasamy, Ammasi</p>
         <p>2018-02-01</p>
         <p>Hyperspectral <span class="hlt">imaging</span> has emerged as a new technique for the identification and classification of biological tissue1. Benefitting recent developments in sensor technology, the new class of hyperspectral <span class="hlt">imagers</span> can capture entire hypercubes with single shot operation and it shows great potential for real-time <span class="hlt">imaging</span> in biomedical sciences. This paper explores the use of a SnapShot <span class="hlt">imager</span> in fluorescence <span class="hlt">imaging</span> via microscope for the very first time. Utilizing the latest <span class="hlt">imaging</span> sensor, the Snapshot <span class="hlt">imager</span> is both compact and attachable via C-mount to any commercially available light microscope. Using this setup, fluorescence hypercubes of several cells were generated, containing both spatial and spectral information. The fluorescence <span class="hlt">images</span> were acquired with one shot operation for all the emission range from visible to near infrared (VIS-IR). The paper will present the hypercubes obtained <span class="hlt">images</span> from example tissues (475-630nm). This study demonstrates the potential of application in cell biology or biomedical applications for real time monitoring.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19830009703&hterms=principles+information+systems&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D60%26Ntt%3Dprinciples%2Binformation%2Bsystems','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19830009703&hterms=principles+information+systems&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D60%26Ntt%3Dprinciples%2Binformation%2Bsystems"><span><span class="hlt">IMAGES</span>: An interactive <span class="hlt">image</span> processing system</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Jensen, J. R.</p>
         <p>1981-01-01</p>
         <p>The <span class="hlt">IMAGES</span> interactive <span class="hlt">image</span> processing system was created specifically for undergraduate remote sensing education in geography. The system is interactive, relatively inexpensive to operate, almost hardware independent, and responsive to numerous users at one time in a time-sharing mode. Most important, it provides a medium whereby theoretical remote sensing principles discussed in lecture may be reinforced in laboratory as students perform computer-assisted <span class="hlt">image</span> processing. In addition to its use in academic and short course environments, the system has also been used extensively to conduct basic <span class="hlt">image</span> processing research. The flow of information through the system is discussed including an overview of the programs.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2005SPIE.5694...90R','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2005SPIE.5694...90R"><span>Multispectral <span class="hlt">imaging</span> for biometrics</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Rowe, Robert K.; Corcoran, Stephen P.; Nixon, Kristin A.; Ostrom, Robert E.</p>
         <p>2005-03-01</p>
         <p>Automated identification systems based on fingerprint <span class="hlt">images</span> are subject to two significant types of error: an incorrect decision about the identity of a person due to a poor quality fingerprint <span class="hlt">image</span> and incorrectly accepting a fingerprint <span class="hlt">image</span> generated from an artificial sample or altered finger. This paper discusses the use of multispectral sensing as a means to collect additional information about a finger that significantly augments the information collected using a conventional fingerprint <span class="hlt">imager</span> based on total internal reflectance. In the context of this paper, "multispectral sensing" is used broadly to denote a collection of <span class="hlt">images</span> taken under different polarization conditions and illumination configurations, as well as using multiple wavelengths. Background information is provided on conventional fingerprint <span class="hlt">imaging</span>. A multispectral <span class="hlt">imager</span> for fingerprint <span class="hlt">imaging</span> is then described and a means to combine the two <span class="hlt">imaging</span> systems into a single unit is discussed. Results from an early-stage prototype of such a system are shown.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA13281.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA13281.html"><span>Test <span class="hlt">Image</span> by Mars Descent <span class="hlt">Imager</span></span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2010-07-19</p>
         <p>Ken Edgett, deputy principal investigator for NASA Mars Descent <span class="hlt">Imager</span>, holds a ruler used as a depth-of-field test target. The instrument took this <span class="hlt">image</span> inside the Malin Space Science Systems clean room in San Diego, CA, during calibration testing.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/29852945','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/29852945"><span>Innovations in Nuclear <span class="hlt">Imaging</span> Instrumentation: Cerenkov <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Tamura, Ryo; Pratt, Edwin C; Grimm, Jan</p>
         <p>2018-07-01</p>
         <p>Cerenkov luminescence (CL) is blue glow light produced by charged subatomic particles travelling faster than the phase velocity of light in a dielectric medium such as water or tissue. CL was first discovered in 1934, but for biomedical research it was recognized only in 2009 after advances in optical camera sensors brought the required high sensitivity. Recently, applications of CL from clinical radionuclides have been rapidly expanding to include not only preclinical and clinical biomedical <span class="hlt">imaging</span> but also an approach to therapy. Cerenkov Luminescence <span class="hlt">Imaging</span> (CLI) utilizes CL generated from clinically relevant radionuclides alongside optical <span class="hlt">imaging</span> instrumentation. CLI is advantageous over traditional nuclear <span class="hlt">imaging</span> methods in terms of infrastructure cost, resolution, and <span class="hlt">imaging</span> time. Furthermore, CLI is a truly multimodal <span class="hlt">imaging</span> method where the same agent can be detected by two independent modalities, with optical (CL) <span class="hlt">imaging</span> and with positron emission tomography (PET) <span class="hlt">imaging</span>. CL has been combined with small molecules, biomolecules and nanoparticles to improve diagnosis and therapy in cancer research. Here, we cover the fundamental breakthroughs and recent advances in reagents and instrumentation methods for CLI as well as therapeutic application of CL. Copyright © 2018 Elsevier Inc. All rights reserved.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://pubs.er.usgs.gov/publication/70021779','USGSPUBS'); return false;" href="https://pubs.er.usgs.gov/publication/70021779"><span><span class="hlt">Imager</span> for Mars Pathfinder (IMP) <span class="hlt">image</span> calibration</span></a></p>
      <p><a target="_blank" href="http://pubs.er.usgs.gov/pubs/index.jsp?view=adv">USGS Publications Warehouse</a></p>
      <p>Reid, R.J.; Smith, P.H.; Lemmon, M.; Tanner, R.; Burkland, M.; Wegryn, E.; Weinberg, J.; Marcialis, R.; Britt, D.T.; Thomas, N.; Kramm, R.; Dummel, A.; Crowe, D.; Bos, B.J.; Bell, J.F.; Rueffer, P.; Gliem, F.; Johnson, J. R.; Maki, J.N.; Herkenhoff, K. E.; Singer, Robert B.</p>
         <p>1999-01-01</p>
         <p>The <span class="hlt">Imager</span> for Mars Pathfinder returned over 16,000 high-quality <span class="hlt">images</span> from the surface of Mars. The camera was well-calibrated in the laboratory, with <5% radiometric uncertainty. The photometric properties of two radiometric targets were also measured with 3% uncertainty. Several data sets acquired during the cruise and on Mars confirm that the system operated nominally throughout the course of the mission. <span class="hlt">Image</span> calibration algorithms were developed for landed operations to correct instrumental sources of noise and to calibrate <span class="hlt">images</span> relative to observations of the radiometric targets. The uncertainties associated with these algorithms as well as current improvements to <span class="hlt">image</span> calibration are discussed. Copyright 1999 by the American Geophysical Union.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2007SPIE.6565E..0CR','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2007SPIE.6565E..0CR"><span>A hyperspectral <span class="hlt">image</span> projector for hyperspectral <span class="hlt">imagers</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Rice, Joseph P.; Brown, Steven W.; Neira, Jorge E.; Bousquet, Robert R.</p>
         <p>2007-04-01</p>
         <p>We have developed and demonstrated a Hyperspectral <span class="hlt">Image</span> Projector (HIP) intended for system-level validation testing of hyperspectral <span class="hlt">imagers</span>, including the instrument and any associated spectral unmixing algorithms. HIP, based on the same digital micromirror arrays used in commercial digital light processing (DLP*) displays, is capable of projecting any combination of many different arbitrarily programmable basis spectra into each <span class="hlt">image</span> pixel at up to video frame rates. We use a scheme whereby one micromirror array is used to produce light having the spectra of endmembers (i.e. vegetation, water, minerals, etc.), and a second micromirror array, optically in series with the first, projects any combination of these arbitrarily-programmable spectra into the pixels of a 1024 x 768 element spatial <span class="hlt">image</span>, thereby producing temporally-integrated <span class="hlt">images</span> having spectrally mixed pixels. HIP goes beyond conventional DLP projectors in that each spatial pixel can have an arbitrary spectrum, not just arbitrary color. As such, the resulting spectral and spatial content of the projected <span class="hlt">image</span> can simulate realistic scenes that a hyperspectral <span class="hlt">imager</span> will measure during its use. Also, the spectral radiance of the projected scenes can be measured with a calibrated spectroradiometer, such that the spectral radiance projected into each pixel of the hyperspectral <span class="hlt">imager</span> can be accurately known. Use of such projected scenes in a controlled laboratory setting would alleviate expensive field testing of instruments, allow better separation of environmental effects from instrument effects, and enable system-level performance testing and validation of hyperspectral <span class="hlt">imagers</span> as used with analysis algorithms. For example, known mixtures of relevant endmember spectra could be projected into arbitrary spatial pixels in a hyperspectral <span class="hlt">imager</span>, enabling tests of how well a full system, consisting of the instrument + calibration + analysis algorithm, performs in unmixing (i.e. de-convolving) the</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1996SPIE.2739..341R','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1996SPIE.2739..341R"><span><span class="hlt">Image</span> registration of naval IR <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Rodland, Arne J.</p>
         <p>1996-06-01</p>
         <p>In a real world application an <span class="hlt">image</span> from a stabilized sensor on a moving platform will not be 100 percent stabilized. There will always be a small unknown error in the stabilization due to factors such as dynamic deformations in the structure between sensor and reference Inertial Navigation Unit, servo inaccuracies, etc. For a high resolution <span class="hlt">imaging</span> sensor this stabilization error causes the <span class="hlt">image</span> to move several pixels in unknown direction between frames. TO be able to detect and track small moving objects from such a sensor, this unknown movement of the sensor <span class="hlt">image</span> must be estimated. An algorithm that searches for land contours in the <span class="hlt">image</span> has been evaluated. The algorithm searches for high contrast points distributed over the whole <span class="hlt">image</span>. As long as moving objects in the scene only cover a small area of the scene, most of the points are located on solid ground. By matching the list of points from frame to frame, the movement of the <span class="hlt">image</span> due to stabilization errors can be estimated and compensated. The point list is searched for points with diverging movement from the estimated stabilization error. These points are then assumed to be located on moving objects. Points assumed to be located on moving objects are gradually exchanged with new points located in the same area. Most of the processing is performed on the list of points and not on the complete <span class="hlt">image</span>. The algorithm is therefore very fast and well suited for real time implementation. The algorithm has been tested on <span class="hlt">images</span> from an experimental IR scanner. Stabilization errors were added artificially to the <span class="hlt">image</span> such that the output from the algorithm could be compared with the artificially added stabilization errors.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_2");'>2</a></li>
      <li><a href="#" onclick='return showDiv("page_3");'>3</a></li>
      <li class="active"><span>4</span></li>
   <li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li><a href="#" onclick='return showDiv("page_6");'>6</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_4 -->
   <div id="page_5" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_3");'>3</a></li>
      <li><a href="#" onclick='return showDiv("page_4");'>4</a></li>
      <li class="active"><span>5</span></li>
   <li><a href="#" onclick='return showDiv("page_6");'>6</a></li>
      <li><a href="#" onclick='return showDiv("page_7");'>7</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="81">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1986PhDT........41L','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1986PhDT........41L"><span>Radiological <span class="hlt">Image</span> Compression</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Lo, Shih-Chung Benedict</p>
         <p></p>
         <p>The movement toward digital <span class="hlt">images</span> in radiology presents the problem of how to conveniently and economically store, retrieve, and transmit the volume of digital <span class="hlt">images</span>. Basic research into <span class="hlt">image</span> data compression is necessary in order to move from a film-based department to an efficient digital -based department. Digital data compression technology consists of two types of compression technique: error-free and irreversible. Error -free <span class="hlt">image</span> compression is desired; however, present techniques can only achieve compression ratio of from 1.5:1 to 3:1, depending upon the <span class="hlt">image</span> characteristics. Irreversible <span class="hlt">image</span> compression can achieve a much higher compression ratio; however, the <span class="hlt">image</span> reconstructed from the compressed data shows some difference from the original <span class="hlt">image</span>. This dissertation studies both error-free and irreversible <span class="hlt">image</span> compression techniques. In particular, some modified error-free techniques have been tested and the recommended strategies for various radiological <span class="hlt">images</span> are discussed. A full-frame bit-allocation irreversible compression technique has been derived. A total of 76 <span class="hlt">images</span> which include CT head and body, and radiographs digitized to 2048 x 2048, 1024 x 1024, and 512 x 512 have been used to test this algorithm. The normalized mean -square-error (NMSE) on the difference <span class="hlt">image</span>, defined as the difference between the original and the reconstructed <span class="hlt">image</span> from a given compression ratio, is used as a global measurement on the quality of the reconstructed <span class="hlt">image</span>. The NMSE's of total of 380 reconstructed and 380 difference <span class="hlt">images</span> are measured and the results tabulated. Three complex compression methods are also suggested to compress <span class="hlt">images</span> with special characteristics. Finally, various parameters which would effect the quality of the reconstructed <span class="hlt">images</span> are discussed. A proposed hardware compression module is given in the last chapter.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1999SPIE.3726..416C','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1999SPIE.3726..416C"><span>Simpler <span class="hlt">images</span>, better results</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Chance, Britton</p>
         <p>1999-03-01</p>
         <p>The very rapid development of optical technology has followed a pattern similar to that of nuclear magnetic resonance: first, spectroscopy and then <span class="hlt">imaging</span>. The accomplishments in spectroscopy have been significant--among them, early detection of hematomas and quantitative oximetry (assuming that time and frequency domain instruments are used). <span class="hlt">Imaging</span> has progressed somewhat later. The first <span class="hlt">images</span> were obtained in Japan and USA a few years ago, particularly of parietal stimulation of the human brain. Since then, rapid applications to breast and limb, together with higher resolution of the brain now make NIR <span class="hlt">imaging</span> of functional activation and tumor detection readily available, reliable and affordable devices. The lecture has to do with the applications of <span class="hlt">imaging</span> to these three areas, particularly to prefrontal <span class="hlt">imaging</span> of cognitive function, of breast tumor detection, and of localized muscle activation in exercise. The <span class="hlt">imaging</span> resolution achievable in functional activation appears to be FWHM of 4 mm. The time required for an <span class="hlt">image</span> is a few seconds or even much less. Breast <span class="hlt">image</span> detection at 50 microsecond(s) ec/pixel results in <span class="hlt">images</span> obtainable in a few seconds or shorter times (bandwidths of the kHz are available). Finally, <span class="hlt">imaging</span> of the body organs is under study in this laboratory, particularly in the in utero fetus. It appears that the photon migration theory now leads to the development of a wide number of <span class="hlt">images</span> for human subject tissue spectroscopy and <span class="hlt">imaging</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1419767','SCIGOV-STC'); return false;" href="https://www.osti.gov/servlets/purl/1419767"><span><span class="hlt">Image</span> registration via optimization over disjoint <span class="hlt">image</span> regions</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Pitts, Todd; Hathaway, Simon; Karelitz, David B.</p>
         <p></p>
         <p>Technologies pertaining to registering a target <span class="hlt">image</span> with a base <span class="hlt">image</span> are described. In a general embodiment, the base <span class="hlt">image</span> is selected from a set of <span class="hlt">images</span>, and the target <span class="hlt">image</span> is an <span class="hlt">image</span> in the set of <span class="hlt">images</span> that is to be registered to the base <span class="hlt">image</span>. A set of disjoint regions of the target <span class="hlt">image</span> is selected, and a transform to be applied to the target <span class="hlt">image</span> is computed based on the optimization of a metric over the selected set of disjoint regions. The transform is applied to the target <span class="hlt">image</span> so as to register the target imagemore » with the base <span class="hlt">image</span>.« less</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19950010051','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19950010051"><span><span class="hlt">Image</span> tools for UNIX</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Banks, David C.</p>
         <p>1994-01-01</p>
         <p>This talk features two simple and useful tools for digital <span class="hlt">image</span> processing in the UNIX environment. They are xv and pbmplus. The xv <span class="hlt">image</span> viewer which runs under the X window system reads <span class="hlt">images</span> in a number of different file formats and writes them out in different formats. The view area supports a pop-up control panel. The 'algorithms' menu lets you blur an <span class="hlt">image</span>. The xv control panel also activates the color editor which displays the <span class="hlt">image</span>'s color map (if one exists). The xv <span class="hlt">image</span> viewer is available through the internet. The pbmplus package is a set of tools designed to perform <span class="hlt">image</span> processing from within a UNIX shell. The acronym 'pbm' stands for portable bit map. Like xv, the pbm plus tool can convert <span class="hlt">images</span> from and to many different file formats. The source code and manual pages for pbmplus are also available through the internet. This software is in the public domain.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.glaucoma.org/glaucoma/optic-nerve-imaging.php','NIH-MEDLINEPLUS'); return false;" href="https://www.glaucoma.org/glaucoma/optic-nerve-imaging.php"><span>Optic Nerve <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>... About Us Donate In This Section Optic Nerve <span class="hlt">Imaging</span> email Send this article to a friend by ... may use one of these optic nerve computer <span class="hlt">imaging</span> techniques as part of your glaucoma examination. By ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=20000027711&hterms=images+mars&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D40%26Ntt%3Dimages%2Bmars','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=20000027711&hterms=images+mars&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D40%26Ntt%3Dimages%2Bmars"><span>Mars Global Surveyor <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1999-01-01</p>
         <p>High resolution <span class="hlt">images</span> that help scientists fine tune the landing site for NASA's Mars Surveyor lander mission are shown. These <span class="hlt">images</span> reveal a smooth surface in the southern cratered highlands near the Nepenthes Mensae.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20040129681','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20040129681"><span>Video <span class="hlt">Image</span> Tracking Engine</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Howard, Richard T. (Inventor); Bryan, ThomasC. (Inventor); Book, Michael L. (Inventor)</p>
         <p>2004-01-01</p>
         <p>A method and system for processing an <span class="hlt">image</span> including capturing an <span class="hlt">image</span> and storing the <span class="hlt">image</span> as <span class="hlt">image</span> pixel data. Each <span class="hlt">image</span> pixel datum is stored in a respective memory location having a corresponding address. Threshold pixel data is selected from the <span class="hlt">image</span> pixel data and linear spot segments are identified from the threshold pixel data selected.. Ihe positions of only a first pixel and a last pixel for each linear segment are saved. Movement of one or more objects are tracked by comparing the positions of fust and last pixels of a linear segment present in the captured <span class="hlt">image</span> with respective first and last pixel positions in subsequent captured <span class="hlt">images</span>. Alternatively, additional data for each linear data segment is saved such as sum of pixels and the weighted sum of pixels i.e., each threshold pixel value is multiplied by that pixel's x-location).</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.fda.gov/Radiation-EmittingProducts/RadiationEmittingProductsandProcedures/MedicalImaging/MRI/default.htm','NIH-MEDLINEPLUS'); return false;" href="https://www.fda.gov/Radiation-EmittingProducts/RadiationEmittingProductsandProcedures/MedicalImaging/MRI/default.htm"><span>MRI (Magnetic Resonance <span class="hlt">Imaging</span>)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>... IV in the arm. MRI Research Programs at FDA Magnetic Resonance <span class="hlt">Imaging</span> (MRI) Safety Electromagnetic Modeling Related ... Resonance <span class="hlt">Imaging</span> Equipment in Clinical Use (March 2015) FDA/CDER: Information on Gadolinium-Based Contrast Agents Safety ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/867987','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/867987"><span>Spectrographic <span class="hlt">imaging</span> system</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Morris, Michael D.; Treado, Patrick J.</p>
         <p>1991-01-01</p>
         <p>An <span class="hlt">imaging</span> system for providing spectrographically resolved <span class="hlt">images</span>. The system incorporates a one-dimensional spatial encoding mask which enables an <span class="hlt">image</span> to be projected onto a two-dimensional <span class="hlt">image</span> detector after spectral dispersion of the <span class="hlt">image</span>. The dimension of the <span class="hlt">image</span> which is lost due to spectral dispersion on the two-dimensional detector is recovered through employing a reverse transform based on presenting a multiplicity of different spatial encoding patterns to the <span class="hlt">image</span>. The system is especially adapted for detecting Raman scattering of monochromatic light transmitted through or reflected from physical samples. Preferably, spatial encoding is achieved through the use of Hadamard mask which selectively transmits or blocks portions of the <span class="hlt">image</span> from the sample being evaluated.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=transpersonal&pg=5&id=EJ166876','ERIC'); return false;" href="https://eric.ed.gov/?q=transpersonal&pg=5&id=EJ166876"><span>The Power of <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Sherman, Vivian</p>
         <p>1977-01-01</p>
         <p>The role played by <span class="hlt">images</span> in the course of human development is considered in this article; personal growth is defined at three different levels of imagery: the producer/consumer <span class="hlt">image</span>, the humanistic, and the transpersonal. (JD)</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=61234','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=61234"><span>Medical <span class="hlt">Image</span> Databases</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Tagare, Hemant D.; Jaffe, C. Carl; Duncan, James</p>
         <p>1997-01-01</p>
         <p>Abstract Information contained in medical <span class="hlt">images</span> differs considerably from that residing in alphanumeric format. The difference can be attributed to four characteristics: (1) the semantics of medical knowledge extractable from <span class="hlt">images</span> is imprecise; (2) <span class="hlt">image</span> information contains form and spatial data, which are not expressible in conventional language; (3) a large part of <span class="hlt">image</span> information is geometric; (4) diagnostic inferences derived from <span class="hlt">images</span> rest on an incomplete, continuously evolving model of normality. This paper explores the differentiating characteristics of text versus <span class="hlt">images</span> and their impact on design of a medical <span class="hlt">image</span> database intended to allow content-based indexing and retrieval. One strategy for implementing medical <span class="hlt">image</span> databases is presented, which employs object-oriented iconic queries, semantics by association with prototypes, and a generic schema. PMID:9147338</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/ency/imagepages/1031.htm','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/ency/imagepages/1031.htm"><span>Hepatitis B virus (<span class="hlt">image</span>)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>Hepatitis B is also known as serum hepatitis and is spread through blood and sexual contact. It is ... population. This photograph is an electronmicroscopic <span class="hlt">image</span> of hepatitis B virus particles. (<span class="hlt">Image</span> courtesy of the Centers for ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19900006898','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19900006898"><span>High compression <span class="hlt">image</span> and <span class="hlt">image</span> sequence coding</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Kunt, Murat</p>
         <p>1989-01-01</p>
         <p>The digital representation of an <span class="hlt">image</span> requires a very large number of bits. This number is even larger for an <span class="hlt">image</span> sequence. The goal of <span class="hlt">image</span> coding is to reduce this number, as much as possible, and reconstruct a faithful duplicate of the original picture or <span class="hlt">image</span> sequence. Early efforts in <span class="hlt">image</span> coding, solely guided by information theory, led to a plethora of methods. The compression ratio reached a plateau around 10:1 a couple of years ago. Recent progress in the study of the brain mechanism of vision and scene analysis has opened new vistas in picture coding. Directional sensitivity of the neurones in the visual pathway combined with the separate processing of contours and textures has led to a new class of coding methods capable of achieving compression ratios as high as 100:1 for <span class="hlt">images</span> and around 300:1 for <span class="hlt">image</span> sequences. Recent progress on some of the main avenues of object-based methods is presented. These second generation techniques make use of contour-texture modeling, new results in neurophysiology and psychophysics and scene analysis.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3262268','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3262268"><span>Biomedical photoacoustic <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Beard, Paul</p>
         <p>2011-01-01</p>
         <p>Photoacoustic (PA) <span class="hlt">imaging</span>, also called optoacoustic <span class="hlt">imaging</span>, is a new biomedical <span class="hlt">imaging</span> modality based on the use of laser-generated ultrasound that has emerged over the last decade. It is a hybrid modality, combining the high-contrast and spectroscopic-based specificity of optical <span class="hlt">imaging</span> with the high spatial resolution of ultrasound <span class="hlt">imaging</span>. In essence, a PA <span class="hlt">image</span> can be regarded as an ultrasound <span class="hlt">image</span> in which the contrast depends not on the mechanical and elastic properties of the tissue, but its optical properties, specifically optical absorption. As a consequence, it offers greater specificity than conventional ultrasound <span class="hlt">imaging</span> with the ability to detect haemoglobin, lipids, water and other light-absorbing chomophores, but with greater penetration depth than purely optical <span class="hlt">imaging</span> modalities that rely on ballistic photons. As well as visualizing anatomical structures such as the microvasculature, it can also provide functional information in the form of blood oxygenation, blood flow and temperature. All of this can be achieved over a wide range of length scales from micrometres to centimetres with scalable spatial resolution. These attributes lend PA <span class="hlt">imaging</span> to a wide variety of applications in clinical medicine, preclinical research and basic biology for studying cancer, cardiovascular disease, abnormalities of the microcirculation and other conditions. With the emergence of a variety of truly compelling in vivo <span class="hlt">images</span> obtained by a number of groups around the world in the last 2–3 years, the technique has come of age and the promise of PA <span class="hlt">imaging</span> is now beginning to be realized. Recent highlights include the demonstration of whole-body small-animal <span class="hlt">imaging</span>, the first demonstrations of molecular <span class="hlt">imaging</span>, the introduction of new microscopy modes and the first steps towards clinical breast <span class="hlt">imaging</span> being taken as well as a myriad of in vivo preclinical <span class="hlt">imaging</span> studies. In this article, the underlying physical principles of the technique, its practical</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3654484','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3654484"><span>Ghost <span class="hlt">Imaging</span> without Discord</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Shapiro, Jeffrey H.; Venkatraman, Dheera; Wong, Franco N. C.</p>
         <p>2013-01-01</p>
         <p>Ragy and Adesso argue that quantum discord is involved in the formation of a pseudothermal ghost <span class="hlt">image</span>. We show that quantum discord plays no role in spatial light modulator ghost <span class="hlt">imaging</span>, i.e., ghost-<span class="hlt">image</span> formation based on structured illumination realized with laser light that has undergone spatial light modulation by the output from a pseudorandom number generator. Our analysis thus casts doubt on the degree to which quantum discord is necessary for ghost <span class="hlt">imaging</span>. PMID:23673426</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADP011348','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADP011348"><span>Internet Color <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2000-07-01</p>
         <p>UNCLASSIFIED Defense Technical Information Center Compilation Part Notice ADPO1 1348 TITLE: Internet Color <span class="hlt">Imaging</span> DISTRIBUTION: Approved for public...Paper Internet Color <span class="hlt">Imaging</span> Hsien-Che Lee <span class="hlt">Imaging</span> Science and Technology Laboratory Eastman Kodak Company, Rochester, New York 14650-1816 USA...ABSTRACT The sharing and exchange of color <span class="hlt">images</span> over the Internet pose very challenging problems to color science and technology . Emerging color standards</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.sil.si.edu/imagegalaxy/imageGalaxy_SearchBasic.cfm','SCIGOVWS'); return false;" href="http://www.sil.si.edu/imagegalaxy/imageGalaxy_SearchBasic.cfm"><span>Galaxy of <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.science.gov/aboutsearch.html">Science.gov Websites</a></p>
      <p></p>
         <p></p>
         <p>This site has moved! Please go to our new <span class="hlt">Image</span> Gallery site! dot header Basic <span class="hlt">Image</span> <em>Search</em> Options dot header <em>Search</em> Tips Enter a keyword term below: Submit Use this <em>search</em> to find ANY words you Irish Lion Cubs Taxonomic (Scientific) Keyword <em>Search</em>: Submit Many of the <span class="hlt">images</span> in the Galaxy of <span class="hlt">Images</span></p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/868020','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/868020"><span>Video <span class="hlt">image</span> position determination</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Christensen, Wynn; Anderson, Forrest L.; Kortegaard, Birchard L.</p>
         <p>1991-01-01</p>
         <p>An optical beam position controller in which a video camera captures an <span class="hlt">image</span> of the beam in its video frames, and conveys those <span class="hlt">images</span> to a processing board which calculates the centroid coordinates for the <span class="hlt">image</span>. The <span class="hlt">image</span> coordinates are used by motor controllers and stepper motors to position the beam in a predetermined alignment. In one embodiment, system noise, used in conjunction with Bernoulli trials, yields higher resolution centroid coordinates.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016SPIE.9871E..0CZ','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016SPIE.9871E..0CZ"><span><span class="hlt">Image</span> quality (IQ) guided multispectral <span class="hlt">image</span> compression</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Zheng, Yufeng; Chen, Genshe; Wang, Zhonghai; Blasch, Erik</p>
         <p>2016-05-01</p>
         <p><span class="hlt">Image</span> compression is necessary for data transportation, which saves both transferring time and storage space. In this paper, we focus on our discussion on lossy compression. There are many standard <span class="hlt">image</span> formats and corresponding compression algorithms, for examples, JPEG (DCT -- discrete cosine transform), JPEG 2000 (DWT -- discrete wavelet transform), BPG (better portable graphics) and TIFF (LZW -- Lempel-Ziv-Welch). The <span class="hlt">image</span> quality (IQ) of decompressed <span class="hlt">image</span> will be measured by numerical metrics such as root mean square error (RMSE), peak signal-to-noise ratio (PSNR), and structural Similarity (SSIM) Index. Given an <span class="hlt">image</span> and a specified IQ, we will investigate how to select a compression method and its parameters to achieve an expected compression. Our scenario consists of 3 steps. The first step is to compress a set of interested <span class="hlt">images</span> by varying parameters and compute their IQs for each compression method. The second step is to create several regression models per compression method after analyzing the IQ-measurement versus compression-parameter from a number of compressed <span class="hlt">images</span>. The third step is to compress the given <span class="hlt">image</span> with the specified IQ using the selected compression method (JPEG, JPEG2000, BPG, or TIFF) according to the regressed models. The IQ may be specified by a compression ratio (e.g., 100), then we will select the compression method of the highest IQ (SSIM, or PSNR). Or the IQ may be specified by a IQ metric (e.g., SSIM = 0.8, or PSNR = 50), then we will select the compression method of the highest compression ratio. Our experiments tested on thermal (long-wave infrared) <span class="hlt">images</span> (in gray scales) showed very promising results.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20090032078','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20090032078"><span>XVD <span class="hlt">Image</span> Display Program</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Deen, Robert G.; Andres, Paul M.; Mortensen, Helen B.; Parizher, Vadim; McAuley, Myche; Bartholomew, Paul</p>
         <p>2009-01-01</p>
         <p>The XVD [X-Windows VICAR (video <span class="hlt">image</span> communication and retrieval) Display] computer program offers an interactive display of VICAR and PDS (planetary data systems) <span class="hlt">images</span>. It is designed to efficiently display multiple-GB <span class="hlt">images</span> and runs on Solaris, Linux, or Mac OS X systems using X-Windows.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_3");'>3</a></li>
      <li><a href="#" onclick='return showDiv("page_4");'>4</a></li>
      <li class="active"><span>5</span></li>
   <li><a href="#" onclick='return showDiv("page_6");'>6</a></li>
      <li><a href="#" onclick='return showDiv("page_7");'>7</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_5 -->
   <div id="page_6" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_4");'>4</a></li>
      <li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li class="active"><span>6</span></li>
   <li><a href="#" onclick='return showDiv("page_7");'>7</a></li>
      <li><a href="#" onclick='return showDiv("page_8");'>8</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="101">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=photography&pg=6&id=EJ1131118','ERIC'); return false;" href="https://eric.ed.gov/?q=photography&pg=6&id=EJ1131118"><span>What Is an <span class="hlt">Image</span>?</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Zetie, K. P.</p>
         <p>2017-01-01</p>
         <p>In basic physics, often in their first year of study of the subject, students meet the concept of an <span class="hlt">image</span>, for example when using pinhole cameras and finding the position of an <span class="hlt">image</span> in a mirror. They are also familiar with the term in photography and design, through software which allows <span class="hlt">image</span> manipulation, even "in-camera" on most…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.sil.si.edu/imagegalaxy/imageGalaxy_About_FAQ.cfm','SCIGOVWS'); return false;" href="http://www.sil.si.edu/imagegalaxy/imageGalaxy_About_FAQ.cfm"><span>About | Galaxy of <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.science.gov/aboutsearch.html">Science.gov Websites</a></p>
      <p></p>
         <p></p>
         <p>-resolution <span class="hlt">images</span> directly from the web site for personal, research or study purposes for <em>free</em>. This includes , promotional material, etc. The usage fee is not a copyright fee. You are <em>free</em> to obtain a copy of these <span class="hlt">images</span> and how our <span class="hlt">images</span> may be used. Smithsonian Libraries provides <em>free</em> and open access to its digital</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.ars.usda.gov/research/publications/publication/?seqNo115=279739','TEKTRAN'); return false;" href="http://www.ars.usda.gov/research/publications/publication/?seqNo115=279739"><span>Basics of <span class="hlt">image</span> analysis</span></a></p>
      <p><a target="_blank" href="https://www.ars.usda.gov/research/publications/find-a-publication/">USDA-ARS?s Scientific Manuscript database</a></p>
      <p></p>
         <p></p>
         <p>Hyperspectral <span class="hlt">imaging</span> technology has emerged as a powerful tool for quality and safety inspection of food and agricultural products and in precision agriculture over the past decade. <span class="hlt">Image</span> analysis is a critical step in implementing hyperspectral <span class="hlt">imaging</span> technology; it is aimed to improve the qualit...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/24663142','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/24663142"><span>Noninvasive cardiovascular <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Hartman, Robert J</p>
         <p>2014-01-01</p>
         <p>Over the past 2 decades, use of noninvasive cardiovascular <span class="hlt">imaging</span> has increased dramatically. This article provides a brief synopsis of the current state of several technologies-- echocardiography, cardiac magnetic resonance <span class="hlt">imaging</span>, and cardiac computed tomography--as well as a glimpse at future possibilities in cardiac <span class="hlt">imaging</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.ars.usda.gov/research/publications/publication/?seqNo115=308622','TEKTRAN'); return false;" href="http://www.ars.usda.gov/research/publications/publication/?seqNo115=308622"><span>Hyperspectral <span class="hlt">image</span> processing methods</span></a></p>
      <p><a target="_blank" href="https://www.ars.usda.gov/research/publications/find-a-publication/">USDA-ARS?s Scientific Manuscript database</a></p>
      <p></p>
         <p></p>
         <p>Hyperspectral <span class="hlt">image</span> processing refers to the use of computer algorithms to extract, store and manipulate both spatial and spectral information contained in hyperspectral <span class="hlt">images</span> across the visible and near-infrared portion of the electromagnetic spectrum. A typical hyperspectral <span class="hlt">image</span> processing work...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/872942','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/872942"><span>Near-electrode <span class="hlt">imager</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Rathke, Jerome W.; Klingler, Robert J.; Woelk, Klaus; Gerald, II, Rex E.</p>
         <p>2000-01-01</p>
         <p>An apparatus, near-electrode <span class="hlt">imager</span>, for employing nuclear magnetic resonance <span class="hlt">imaging</span> to provide in situ measurements of electrochemical properties of a sample as a function of distance from a working electrode. The near-electrode <span class="hlt">imager</span> uses the radio frequency field gradient within a cylindrical toroid cavity resonator to provide high-resolution nuclear magnetic resonance spectral information on electrolyte materials.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=DRONE+OR+QUADRONE&pg=2&id=EJ249563','ERIC'); return false;" href="https://eric.ed.gov/?q=DRONE+OR+QUADRONE&pg=2&id=EJ249563"><span>Nursing's <span class="hlt">Image</span> on Campus.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Woolley, Alma S.</p>
         <p>1981-01-01</p>
         <p>In studying the nurse's <span class="hlt">image</span> at a liberal arts college, it was found that faculty and administrators view nurses as long-suffering drones. On the whole, the <span class="hlt">image</span> of nursing was positive, with those who had the most contact with the nursing program having a more enlightened <span class="hlt">image</span>. (CT)</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=61345','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=61345"><span><span class="hlt">Image</span> Acquisition Context</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Bidgood, W. Dean; Bray, Bruce; Brown, Nicolas; Mori, Angelo Rossi; Spackman, Kent A.; Golichowski, Alan; Jones, Robert H.; Korman, Louis; Dove, Brent; Hildebrand, Lloyd; Berg, Michael</p>
         <p>1999-01-01</p>
         <p>Objective: To support clinically relevant indexing of biomedical <span class="hlt">images</span> and <span class="hlt">image</span>-related information based on the attributes of <span class="hlt">image</span> acquisition procedures and the judgments (observations) expressed by observers in the process of <span class="hlt">image</span> interpretation. Design: The authors introduce the notion of “<span class="hlt">image</span> acquisition context,” the set of attributes that describe <span class="hlt">image</span> acquisition procedures, and present a standards-based strategy for utilizing the attributes of <span class="hlt">image</span> acquisition context as indexing and retrieval keys for digital <span class="hlt">image</span> libraries. Methods: The authors' indexing strategy is based on an interdependent message/terminology architecture that combines the Digital <span class="hlt">Imaging</span> and Communication in Medicine (DICOM) standard, the SNOMED (Systematized Nomenclature of Human and Veterinary Medicine) vocabulary, and the SNOMED DICOM microglossary. The SNOMED DICOM microglossary provides context-dependent mapping of terminology to DICOM data elements. Results: The capability of embedding standard coded descriptors in DICOM <span class="hlt">image</span> headers and <span class="hlt">image</span>-interpretation reports improves the potential for selective retrieval of <span class="hlt">image</span>-related information. This favorably affects information management in digital libraries. PMID:9925229</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA134943','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA134943"><span><span class="hlt">Image</span> Processing Research</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1976-09-30</p>
         <p>Estimation and Detection of <span class="hlt">Images</span> Degraded by Film Grain Noise - Firouz Naderi 200 5. 3 <span class="hlt">Image</span> Restoration by Spline Functions...given for the choice of this number: (a) Higher order terms correspond to noise in the <span class="hlt">image</span> and should be ignored. (b) An analytical...expansion ate sufficient to characterize the signal exactly. Results of experiaental evaluation signals containing noise are presented next</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/21953049','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/21953049"><span>Digital <span class="hlt">imaging</span> mass spectrometry.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Bamberger, Casimir; Renz, Uwe; Bamberger, Andreas</p>
         <p>2011-06-01</p>
         <p>Methods to visualize the two-dimensional (2D) distribution of molecules by mass spectrometric <span class="hlt">imaging</span> evolve rapidly and yield novel applications in biology, medicine, and material surface sciences. Most mass spectrometric <span class="hlt">imagers</span> acquire high mass resolution spectra spot-by-spot and thereby scan the object's surface. Thus, <span class="hlt">imaging</span> is slow and <span class="hlt">image</span> reconstruction remains cumbersome. Here we describe an <span class="hlt">imaging</span> mass spectrometer that exploits the true <span class="hlt">imaging</span> capabilities by ion optical means for the time of flight mass separation. The mass spectrometer is equipped with the ASIC Timepix chip as an array detector to acquire the position, mass, and intensity of ions that are <span class="hlt">imaged</span> by matrix-assisted laser desorption/ionization (MALDI) directly from the target sample onto the detector. This <span class="hlt">imaging</span> mass spectrometer has a spatial resolving power at the specimen of (84 ± 35) μm with a mass resolution of 45 and locates atoms or organic compounds on a surface area up to ~2 cm(2). Extended laser spots of ~5 mm(2) on structured specimens allows parallel <span class="hlt">imaging</span> of selected masses. The digital <span class="hlt">imaging</span> mass spectrometer proves high hit-multiplicity, straightforward <span class="hlt">image</span> reconstruction, and potential for high-speed readout at 4 kHz or more. This device demonstrates a simple way of true <span class="hlt">image</span> acquisition like a digital photographic camera. The technology may enable a fast analysis of biomolecular samples in near future.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1086945','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1086945"><span>Medical <span class="hlt">imaging</span> systems</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Frangioni, John V</p>
         <p>2013-06-25</p>
         <p>A medical <span class="hlt">imaging</span> system provides simultaneous rendering of visible light and diagnostic or functional <span class="hlt">images</span>. The system may be portable, and may include adapters for connecting various light sources and cameras in open surgical environments or laparascopic or endoscopic environments. A user interface provides control over the functionality of the integrated <span class="hlt">imaging</span> system. In one embodiment, the system provides a tool for surgical pathology.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1993LNP...413..193J','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1993LNP...413..193J"><span>Methods in Astronomical <span class="hlt">Image</span> Processing</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Jörsäter, S.</p>
         <p></p>
         <p>A Brief Introductory Note History of Astronomical <span class="hlt">Imaging</span> Astronomical <span class="hlt">Image</span> Data <span class="hlt">Images</span> in Various Formats Digitized <span class="hlt">Image</span> Data Digital <span class="hlt">Image</span> Data Philosophy of Astronomical <span class="hlt">Image</span> Processing Properties of Digital Astronomical <span class="hlt">Images</span> Human <span class="hlt">Image</span> Processing Astronomical vs. Computer Science <span class="hlt">Image</span> Processing Basic Tools of Astronomical <span class="hlt">Image</span> Processing Display Applications Calibration of Intensity Scales Calibration of Length Scales <span class="hlt">Image</span> Re-shaping Feature Enhancement Noise Suppression Noise and Error Analysis <span class="hlt">Image</span> Processing Packages: Design of AIPS and MIDAS AIPS MIDAS Reduction of CCD Data Bias Subtraction Clipping Preflash Subtraction Dark Subtraction Flat Fielding Sky Subtraction Extinction Correction Deconvolution Methods Rebinning/Combining Summary and Prospects for the Future</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/27429142','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/27429142"><span>Hip <span class="hlt">Imaging</span> in Athletes: Sports <span class="hlt">Imaging</span> Series.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Agten, Christoph A; Sutter, Reto; Buck, Florian M; Pfirrmann, Christian W A</p>
         <p>2016-08-01</p>
         <p>Hip or groin pain in athletes is common and clinical presentation is often nonspecific. <span class="hlt">Imaging</span> is a very important diagnostic step in the work-up of athletes with hip pain. This review article provides an overview on hip biomechanics and discusses strategies for hip <span class="hlt">imaging</span> modalities such as radiography, ultrasonography, computed tomography, and magnetic resonance (MR) <span class="hlt">imaging</span> (MR arthrography and traction MR arthrography). The authors explain current concepts of femoroacetabular impingement and the problem of high prevalence of cam- and pincer-type morphology in asymptomatic persons. With the main focus on MR <span class="hlt">imaging</span>, the authors present abnormalities of the hip joint and the surrounding soft tissues that can occur in athletes: intraarticular and extraarticular hip impingement syndromes, labral and cartilage disease, microinstability of the hip, myotendinous injuries, and athletic pubalgia. (©) RSNA, 2016.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1996SPIE.2711..396G','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1996SPIE.2711..396G"><span>Implementing desktop <span class="hlt">image</span> access of GI <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Grevera, George J.; Feingold, Eric R.; Horii, Steven C.; Laufer, Igor</p>
         <p>1996-05-01</p>
         <p>In this paper we present a specific example of the current state-of-the-art in desktop <span class="hlt">image</span> access in the GI section of the Department of Radiology at the Hospital of the University of Pennsylvania. We describe a system which allows physicians to view and manipulate <span class="hlt">images</span> from a Philips digital fluoroscopy system at the workstations in their offices. Typically they manipulate and view these <span class="hlt">images</span> on their desktop Macs and then submit the results for slide making or save the <span class="hlt">images</span> in digital teaching files. In addition to a discussion of the current state-of-the-art here at HUP, we also discuss some future directions that we are pursuing.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2002OptEn..41.2083M','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2002OptEn..41.2083M"><span>Satellite <span class="hlt">image</span> collection optimization</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Martin, William</p>
         <p>2002-09-01</p>
         <p><span class="hlt">Imaging</span> satellite systems represent a high capital cost. Optimizing the collection of <span class="hlt">images</span> is critical for both satisfying customer orders and building a sustainable satellite operations business. We describe the functions of an operational, multivariable, time dynamic optimization system that maximizes the daily collection of satellite <span class="hlt">images</span>. A graphical user interface allows the operator to quickly see the results of what if adjustments to an <span class="hlt">image</span> collection plan. Used for both long range planning and daily collection scheduling of Space <span class="hlt">Imaging</span>'s IKONOS satellite, the satellite control and tasking (SCT) software allows collection commands to be altered up to 10 min before upload to the satellite.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/21942063','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/21942063"><span>[Advance in <span class="hlt">imaging</span> spectropolarimeter].</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Wang, Xin-quan; Xiangli, Bin; Huang, Min; Hu, Liang; Zhou, Jin-song; Jing, Juan-juan</p>
         <p>2011-07-01</p>
         <p><span class="hlt">Imaging</span> spectropolarimeter (ISP) is a type of novel photoelectric sensor which integrated the functions of <span class="hlt">imaging</span>, spectrometry and polarimetry. In the present paper, the concept of the ISP is introduced, and the advances in ISP at home and abroad in recent years is reviewed. The principles of ISPs based on novel devices, such as acousto-optic tunable filter (AOTF) and liquid crystal tunable filter (LCTF), are illustrated. In addition, the principles of ISPs developed by adding polarized components to the dispersing-type <span class="hlt">imaging</span> spectrometer, spatially modulated Fourier transform <span class="hlt">imaging</span> spectrometer, and computer tomography <span class="hlt">imaging</span> spectrometer are introduced. Moreover, the trends of ISP are discussed too.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19810053344&hterms=image+alignment&qs=N%3D0%26Ntk%3DAll%26Ntx%3Dmode%2Bmatchall%26Ntt%3Dimage%2Balignment','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19810053344&hterms=image+alignment&qs=N%3D0%26Ntk%3DAll%26Ntx%3Dmode%2Bmatchall%26Ntt%3Dimage%2Balignment"><span>Experimental <span class="hlt">image</span> alignment system</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Moyer, A. L.; Kowel, S. T.; Kornreich, P. G.</p>
         <p>1980-01-01</p>
         <p>A microcomputer-based instrument for <span class="hlt">image</span> alignment with respect to a reference <span class="hlt">image</span> is described which uses the DEFT sensor (Direct Electronic Fourier Transform) for <span class="hlt">image</span> sensing and preprocessing. The instrument alignment algorithm which uses the two-dimensional Fourier transform as input is also described. It generates signals used to steer the stage carrying the test <span class="hlt">image</span> into the correct orientation. This algorithm has computational advantages over algorithms which use <span class="hlt">image</span> intensity data as input and is suitable for a microcomputer-based instrument since the two-dimensional Fourier transform is provided by the DEFT sensor.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1407410','SCIGOV-STC'); return false;" href="https://www.osti.gov/servlets/purl/1407410"><span><span class="hlt">Imaging</span>SIMS</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p></p>
         <p>2017-11-06</p>
         <p><span class="hlt">Imaging</span>SIMS is an open source application for loading, processing, manipulating and visualizing secondary ion mass spectrometry (SIMS) data. At PNNL, a separate branch has been further developed to incorporate application specific features for dynamic SIMS data sets. These include loading CAMECA IMS-1280, NanoSIMS and modified IMS-4f raw data, creating isotopic ratio <span class="hlt">images</span> and stitching together <span class="hlt">images</span> from adjacent interrogation regions. In addition to other modifications of the parent open source version, this version is equipped with a point-by-point <span class="hlt">image</span> registration tool to assist with streamlining the <span class="hlt">image</span> fusion process.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/1334621-responsive-image-inline-filter','SCIGOV-STC'); return false;" href="https://www.osti.gov/biblio/1334621-responsive-image-inline-filter"><span>Responsive <span class="hlt">Image</span> Inline Filter</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Freeman, Ian</p>
         <p>2016-10-20</p>
         <p>RIIF is a contributed module for the Drupal php web application framework (drupal.org). It is written as a helper or sub-module of other code which is part of version 8 "core Drupal" and is intended to extend its functionality. It allows Drupal to resize <span class="hlt">images</span> uploaded through the user-facing text editor within the Drupal GUI (a.k.a. "inline <span class="hlt">images</span>") for various browser widths. This resizing is already done foe other <span class="hlt">images</span> through the parent "Responsive <span class="hlt">Image</span>" core module. This code extends that functionality to inline <span class="hlt">images</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/22306226-fourier-plane-imaging-microscopy','SCIGOV-STC'); return false;" href="https://www.osti.gov/biblio/22306226-fourier-plane-imaging-microscopy"><span>Fourier plane <span class="hlt">imaging</span> microscopy</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Dominguez, Daniel, E-mail: daniel.dominguez@ttu.edu; Peralta, Luis Grave de; Nano Tech Center, Texas Tech University, Lubbock, Texas 79409</p>
         <p></p>
         <p>We show how the <span class="hlt">image</span> of an unresolved photonic crystal can be reconstructed using a single Fourier plane (FP) <span class="hlt">image</span> obtained with a second camera that was added to a traditional compound microscope. We discuss how Fourier plane <span class="hlt">imaging</span> microscopy is an application of a remarkable property of the obtained FP <span class="hlt">images</span>: they contain more information about the photonic crystals than the <span class="hlt">images</span> recorded by the camera commonly placed at the real plane of the microscope. We argue that the experimental results support the hypothesis that surface waves, contributing to enhanced resolution abilities, were optically excited in the studied photonicmore » crystals.« less</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_4");'>4</a></li>
      <li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li class="active"><span>6</span></li>
   <li><a href="#" onclick='return showDiv("page_7");'>7</a></li>
      <li><a href="#" onclick='return showDiv("page_8");'>8</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_6 -->
   <div id="page_7" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li><a href="#" onclick='return showDiv("page_6");'>6</a></li>
      <li class="active"><span>7</span></li>
   <li><a href="#" onclick='return showDiv("page_8");'>8</a></li>
      <li><a href="#" onclick='return showDiv("page_9");'>9</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="121">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/27314718','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/27314718"><span>Correlation Plenoptic <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>D'Angelo, Milena; Pepe, Francesco V; Garuccio, Augusto; Scarcelli, Giuliano</p>
         <p>2016-06-03</p>
         <p>Plenoptic <span class="hlt">imaging</span> is a promising optical modality that simultaneously captures the location and the propagation direction of light in order to enable three-dimensional <span class="hlt">imaging</span> in a single shot. However, in standard plenoptic <span class="hlt">imaging</span> systems, the maximum spatial and angular resolutions are fundamentally linked; thereby, the maximum achievable depth of field is inversely proportional to the spatial resolution. We propose to take advantage of the second-order correlation properties of light to overcome this fundamental limitation. In this Letter, we demonstrate that the correlation in both momentum and position of chaotic light leads to the enhanced refocusing power of correlation plenoptic <span class="hlt">imaging</span> with respect to standard plenoptic <span class="hlt">imaging</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016PhRvL.116v3602D','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016PhRvL.116v3602D"><span>Correlation Plenoptic <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>D'Angelo, Milena; Pepe, Francesco V.; Garuccio, Augusto; Scarcelli, Giuliano</p>
         <p>2016-06-01</p>
         <p>Plenoptic <span class="hlt">imaging</span> is a promising optical modality that simultaneously captures the location and the propagation direction of light in order to enable three-dimensional <span class="hlt">imaging</span> in a single shot. However, in standard plenoptic <span class="hlt">imaging</span> systems, the maximum spatial and angular resolutions are fundamentally linked; thereby, the maximum achievable depth of field is inversely proportional to the spatial resolution. We propose to take advantage of the second-order correlation properties of light to overcome this fundamental limitation. In this Letter, we demonstrate that the correlation in both momentum and position of chaotic light leads to the enhanced refocusing power of correlation plenoptic <span class="hlt">imaging</span> with respect to standard plenoptic <span class="hlt">imaging</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/26510142','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/26510142"><span>Guide to thoracic <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Skinner, Sarah</p>
         <p>2015-08-01</p>
         <p>Thoracic <span class="hlt">imaging</span> is commonly ordered in general practice. Guidelines exist for ordering thoracic <span class="hlt">imaging</span> but few are specific for general practice. This article summarises current indications for <span class="hlt">imaging</span> the thorax with chest X-ray and computed tomography. A simple frame-work for interpretation of the chest X-ray, suitable for trainees and practitioners providing primary care <span class="hlt">imaging</span> in rural and remote locations, is presented. Interpretation of thoracic <span class="hlt">imaging</span> is best done using a systematic approach. Radiological investigation is not warranted in un-complicated upper respiratory tract infections or asthma, minor trauma or acute-on-chronic chest pain.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/18787241','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/18787241"><span>Annotating <span class="hlt">images</span> by mining <span class="hlt">image</span> search results.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Wang, Xin-Jing; Zhang, Lei; Li, Xirong; Ma, Wei-Ying</p>
         <p>2008-11-01</p>
         <p>Although it has been studied for years by the computer vision and machine learning communities, <span class="hlt">image</span> annotation is still far from practical. In this paper, we propose a novel attempt at model-free <span class="hlt">image</span> annotation, which is a data-driven approach that annotates <span class="hlt">images</span> by mining their search results. Some 2.4 million <span class="hlt">images</span> with their surrounding text are collected from a few photo forums to support this approach. The entire process is formulated in a divide-and-conquer framework where a query keyword is provided along with the uncaptioned <span class="hlt">image</span> to improve both the effectiveness and efficiency. This is helpful when the collected data set is not dense everywhere. In this sense, our approach contains three steps: 1) the search process to discover visually and semantically similar search results, 2) the mining process to identify salient terms from textual descriptions of the search results, and 3) the annotation rejection process to filter out noisy terms yielded by Step 2. To ensure real-time annotation, two key techniques are leveraged-one is to map the high-dimensional <span class="hlt">image</span> visual features into hash codes, the other is to implement it as a distributed system, of which the search and mining processes are provided as Web services. As a typical result, the entire process finishes in less than 1 second. Since no training data set is required, our approach enables annotating with unlimited vocabulary and is highly scalable and robust to outliers. Experimental results on both real Web <span class="hlt">images</span> and a benchmark <span class="hlt">image</span> data set show the effectiveness and efficiency of the proposed algorithm. It is also worth noting that, although the entire approach is illustrated within the divide-and conquer framework, a query keyword is not crucial to our current implementation. We provide experimental results to prove this.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/17844820','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/17844820"><span><span class="hlt">Image</span> dissemination and archiving.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Robertson, Ian</p>
         <p>2007-08-01</p>
         <p><span class="hlt">Images</span> generated as part of the sonographic examination are an integral part of the medical record and must be retained according to local regulations. The standard medical <span class="hlt">image</span> format, known as DICOM (Digital <span class="hlt">Imaging</span> and COmmunications in Medicine) makes it possible for <span class="hlt">images</span> from many different <span class="hlt">imaging</span> modalities, including ultrasound, to be distributed via a standard internet network to distant viewing workstations and a central archive in an almost seamless fashion. The DICOM standard is a truly universal standard for the dissemination of medical <span class="hlt">images</span>. When purchasing an ultrasound unit, the consumer should research the unit's capacity to generate <span class="hlt">images</span> in a DICOM format, especially if one wishes interconnectivity with viewing workstations and an <span class="hlt">image</span> archive that stores other medical <span class="hlt">images</span>. PACS, an acronym for Picture Archive and Communication System refers to the infrastructure that links modalities, workstations, the <span class="hlt">image</span> archive, and the medical record information system into an integrated system, allowing for efficient electronic distribution and storage of medical <span class="hlt">images</span> and access to medical record data.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/23717792','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/23717792"><span><span class="hlt">Imaging</span> of brain metastases.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Fink, Kathleen R; Fink, James R</p>
         <p>2013-01-01</p>
         <p><span class="hlt">Imaging</span> plays a key role in the diagnosis of central nervous system (CNS) metastasis. <span class="hlt">Imaging</span> is used to detect metastases in patients with known malignancies and new neurological signs or symptoms, as well as to screen for CNS involvement in patients with known cancer. Computed tomography (CT) and magnetic resonance <span class="hlt">imaging</span> (MRI) are the key <span class="hlt">imaging</span> modalities used in the diagnosis of brain metastases. In difficult cases, such as newly diagnosed solitary enhancing brain lesions in patients without known malignancy, advanced <span class="hlt">imaging</span> techniques including proton magnetic resonance spectroscopy (MRS), contrast enhanced magnetic resonance perfusion (MRP), diffusion weighted <span class="hlt">imaging</span> (DWI), and diffusion tensor <span class="hlt">imaging</span> (DTI) may aid in arriving at the correct diagnosis. This <span class="hlt">image</span>-rich review discusses the <span class="hlt">imaging</span> evaluation of patients with suspected intracranial involvement and malignancy, describes typical <span class="hlt">imaging</span> findings of parenchymal brain metastasis on CT and MRI, and provides clues to specific histological diagnoses such as the presence of hemorrhage. Additionally, the role of advanced <span class="hlt">imaging</span> techniques is reviewed, specifically in the context of differentiating metastasis from high-grade glioma and other solitary enhancing brain lesions. Extra-axial CNS involvement by metastases, including pachymeningeal and leptomeningeal metastases is also briefly reviewed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2013biot.book..351I','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2013biot.book..351I"><span>Multimodal Diffuse Optical <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Intes, Xavier; Venugopal, Vivek; Chen, Jin; Azar, Fred S.</p>
         <p></p>
         <p>Diffuse optical <span class="hlt">imaging</span>, particularly diffuse optical tomography (DOT), is an emerging clinical modality capable of providing unique functional information, at a relatively low cost, and with nonionizing radiation. Multimodal diffuse optical <span class="hlt">imaging</span> has enabled a synergistic combination of functional and anatomical information: the quality of DOT reconstructions has been significantly improved by incorporating the structural information derived by the combined anatomical modality. In this chapter, we will review the basic principles of diffuse optical <span class="hlt">imaging</span>, including instrumentation and reconstruction algorithm design. We will also discuss the approaches for multimodal <span class="hlt">imaging</span> strategies that integrate DOI with clinically established modalities. The merit of the multimodal <span class="hlt">imaging</span> approaches is demonstrated in the context of optical mammography, but the techniques described herein can be translated to other clinical scenarios such as brain functional <span class="hlt">imaging</span> or muscle functional <span class="hlt">imaging</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/26280647','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/26280647"><span>[Fundus Autofluorescence <span class="hlt">Imaging</span>].</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Schmitz-Valckenberg, S</p>
         <p>2015-09-01</p>
         <p>Fundus autofluorescence (FAF) <span class="hlt">imaging</span> allows for non-invasive mapping of changes at the level of the retinal pigment epithelium/photoreceptor complex and of alterations of macular pigment distribution. This <span class="hlt">imaging</span> method is based on the visualisation of intrinsic fluorophores and may be easily and rapidly used in routine patient care. Main applications include degenerative disorders of the outer retina such as age-related macular degeneration, hereditary and acquired retinal diseases. FAF <span class="hlt">imaging</span> is particularly helpful for differential diagnosis, detection and extent of involved retinal areas, structural-functional correlations and monitoring of changes over time. Recent developments include - in addition to the original application of short wavelength light for excitation ("blue" FAF <span class="hlt">imaging</span>) - the use of other wavelength ranges ("green" or "near-infrared" FAF <span class="hlt">imaging</span>), widefield <span class="hlt">imaging</span> for visualisation of peripheral retinal areas and quantitative FAF <span class="hlt">imaging</span>. Georg Thieme Verlag KG Stuttgart · New York.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20110011972','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20110011972"><span>GOATS <span class="hlt">Image</span> Projection Component</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Haber, Benjamin M.; Green, Joseph J.</p>
         <p>2011-01-01</p>
         <p>When doing mission analysis and design of an <span class="hlt">imaging</span> system in orbit around the Earth, answering the fundamental question of <span class="hlt">imaging</span> performance requires an understanding of the <span class="hlt">image</span> products that will be produced by the <span class="hlt">imaging</span> system. GOATS software represents a series of MATLAB functions to provide for geometric <span class="hlt">image</span> projections. Unique features of the software include function modularity, a standard MATLAB interface, easy-to-understand first-principles-based analysis, and the ability to perform geometric <span class="hlt">image</span> projections of framing type <span class="hlt">imaging</span> systems. The software modules are created for maximum analysis utility, and can all be used independently for many varied analysis tasks, or used in conjunction with other orbit analysis tools.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4327770','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4327770"><span>Applications of Molecular <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Galbán, Craig; Galbán, Stefanie; Van Dort, Marcian; Luker, Gary D.; Bhojani, Mahaveer S.; Rehemtualla, Alnawaz; Ross, Brian D.</p>
         <p>2015-01-01</p>
         <p>Today molecular <span class="hlt">imaging</span> technologies play a central role in clinical oncology. The use of <span class="hlt">imaging</span> techniques in early cancer detection, treatment response and new therapy development is steadily growing and has already significantly impacted clinical management of cancer. In this chapter we will overview three different molecular <span class="hlt">imaging</span> technologies used for the understanding of disease biomarkers, drug development, or monitoring therapeutic outcome. They are (1) optical <span class="hlt">imaging</span> (bioluminescence and fluorescence <span class="hlt">imaging</span>) (2) magnetic resonance <span class="hlt">imaging</span> (MRI), and (3) nuclear <span class="hlt">imaging</span> (e.g, single photon emission computed tomography (SPECT) and positron emission tomography (PET)). We will review the use of molecular reporters of biological processes (e.g. apoptosis and protein kinase activity) for high throughput drug screening and new cancer therapies, diffusion MRI as a biomarker for early treatment response and PET and SPECT radioligands in oncology. PMID:21075334</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19980236898&hterms=science+images&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D20%26Ntt%3Dscience%2Bimages','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19980236898&hterms=science+images&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D20%26Ntt%3Dscience%2Bimages"><span><span class="hlt">IMAGE</span> Mission Science</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Gallagher, D. L.; Fok, M.-C.; Fuselier, S.; Gladstone, G. R.; Green, J. L.; Fung, S. F.; Perez, J.; Reiff, P.; Roelof, E. C.; Wilson, G.</p>
         <p>1998-01-01</p>
         <p>Simultaneous, global measurement of major magnetospheric plasma systems will be performed for the first time with the <span class="hlt">Imager</span> for Magnetopause-to-Aurora Global Exploration (<span class="hlt">IMAGE</span>) Mission. The ring current, plasmasphere, and auroral systems will be <span class="hlt">imaged</span> using energetic neutral and ultraviolet cameras. Quantitative remote measurement of the magnetosheath, plasmaspheric, and magnetospheric densities will be obtained through radio sounding by the Radio Plasma <span class="hlt">Imager</span>. The <span class="hlt">IMAGE</span> Mission will open a new era in global magnetospheric physics, while bringing with it new challenges in data analysis. An overview of the <span class="hlt">IMAGE</span> Theory and Modeling team efforts will be presented, including the state of development of Internet tools that will be available to the science community for access and analysis of <span class="hlt">IMAGE</span> observations.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2006SPIE.6047E..0LW','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2006SPIE.6047E..0LW"><span>Uncooled thermal <span class="hlt">imaging</span> and <span class="hlt">image</span> analysis</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Wang, Shiyun; Chang, Benkang; Yu, Chunyu; Zhang, Junju; Sun, Lianjun</p>
         <p>2006-09-01</p>
         <p>Thermal <span class="hlt">imager</span> can transfer difference of temperature to difference of electric signal level, so can be application to medical treatment such as estimation of blood flow speed and vessel 1ocation [1], assess pain [2] and so on. With the technology of un-cooled focal plane array (UFPA) is grown up more and more, some simple medical function can be completed with un-cooled thermal <span class="hlt">imager</span>, for example, quick warning for fever heat with SARS. It is required that performance of <span class="hlt">imaging</span> is stabilization and spatial and temperature resolution is high enough. In all performance parameters, noise equivalent temperature difference (NETD) is often used as the criterion of universal performance. 320 x 240 α-Si micro-bolometer UFPA has been applied widely presently for its steady performance and sensitive responsibility. In this paper, NETD of UFPA and the relation between NETD and temperature are researched. several vital parameters that can affect NETD are listed and an universal formula is presented. Last, the <span class="hlt">images</span> from the kind of thermal <span class="hlt">imager</span> are analyzed based on the purpose of detection persons with fever heat. An applied thermal <span class="hlt">image</span> intensification method is introduced.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2017SPIE10127E..07A','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2017SPIE10127E..07A"><span>Sonorous <span class="hlt">images</span> through digital holographic <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Azevedo, Isabel; Sandford-Richardson, Elizabeth</p>
         <p>2017-03-01</p>
         <p>The art of the last fifty years has significantly surrounded the presence of the body, the relationship between human and interactive technologies. Today in interactive art, there are not only representations that speak of the body but actions and behaviours that involve the body. In holography, the <span class="hlt">image</span> appears and disappears from the observer's vision field; because the holographic <span class="hlt">image</span> is light, we can see multidimensional spaces, shapes and colours existing on the same time, presence and absence of the <span class="hlt">image</span> on the holographic plate. And the <span class="hlt">image</span> can be flowing in front of the plate that sometimes people try touching it with his hands. That means, to the viewer will be interactive events, with no beginning or end that can be perceived in any direction, forward or backward, depending on the relative position and the time the viewer spends in front of the hologram. To explore that feature we are proposing an installation with four holograms, and several sources of different kind of sounds connected with each hologram. When viewers will move in front of each hologram they will activate different sources of sound. The search is not only about the <span class="hlt">images</span> in the holograms, but also the looking for different types of sounds that this demand will require. The digital holograms were produced using the HoloCam Portable Light System with the 35 mm camera Canon 700D to capture <span class="hlt">image</span> information, it was then edited on computer using the Motion 5 and Final Cut Pro X programs.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/27987555','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/27987555"><span>Clinical Amyloid <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Mallik, Atul; Drzezga, Alex; Minoshima, Satoshi</p>
         <p>2017-01-01</p>
         <p>Amyloid plaques, along with neurofibrillary tangles, are a neuropathologic hallmark of Alzheimer disease (AD). Recently, amyloid PET radiotracers have been developed and approved for clinical use in the evaluation of suspected neurodegenerative disorders. In both research and clinical settings, amyloid PET <span class="hlt">imaging</span> has provided important diagnostic and prognostic information for the management of patients with possible AD, mild cognitive impairment (MCI), and other challenging diagnostic presentations. Although the overall impact of amyloid <span class="hlt">imaging</span> is still being evaluated, the Society of Nuclear Medicine and Molecular <span class="hlt">Imaging</span> and Alzheimer's Association Amyloid <span class="hlt">Imaging</span> Task Force have created appropriate use criteria for the standard clinical use of amyloid PET <span class="hlt">imaging</span>. By the appropriate use criteria, amyloid <span class="hlt">imaging</span> is appropriate for patients with (1) persistent or unexplained MCI, (2) AD as a possible but still uncertain diagnosis after expert evaluation and (3) atypically early-age-onset progressive dementia. To better understand the clinical and economic effect of amyloid <span class="hlt">imaging</span>, the <span class="hlt">Imaging</span> Dementia-Evidence for Amyloid Scanning (IDEAS) study is an ongoing large multicenter study in the United States, which is evaluating how amyloid <span class="hlt">imaging</span> affects diagnosis, management, and outcomes for cognitively impaired patients who cannot be completely evaluated by clinical assessment alone. Multiple other large-scale studies are evaluating the prognostic role of amyloid PET <span class="hlt">imaging</span> for predicting MCI progression to AD in general and high-risk populations. At the same time, amyloid <span class="hlt">imaging</span> is an important tool for evaluating potential disease-modifying therapies for AD. Overall, the increased use of amyloid PET <span class="hlt">imaging</span> has led to a better understanding of the strengths and limitations of this <span class="hlt">imaging</span> modality and how it may best be used with other clinical, molecular, and <span class="hlt">imaging</span> assessment techniques for the diagnosis and management of neurodegenerative disorders</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2017SPD....48.0601D','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2017SPD....48.0601D"><span>Noise Gating Solar <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>DeForest, Craig; Seaton, Daniel B.; Darnell, John A.</p>
         <p>2017-08-01</p>
         <p>I present and demonstrate a new, general purpose post-processing technique, "3D noise gating", that can reduce <span class="hlt">image</span> noise by an order of magnitude or more without effective loss of spatial or temporal resolution in typical solar applications.Nearly all scientific <span class="hlt">images</span> are, ultimately, limited by noise. Noise can be direct Poisson "shot noise" from photon counting effects, or introduced by other means such as detector read noise. Noise is typically represented as a random variable (perhaps with location- or <span class="hlt">image</span>-dependent characteristics) that is sampled once per pixel or once per resolution element of an <span class="hlt">image</span> sequence. Noise limits many aspects of <span class="hlt">image</span> analysis, including photometry, spatiotemporal resolution, feature identification, morphology extraction, and background modeling and separation.Identifying and separating noise from <span class="hlt">image</span> signal is difficult. The common practice of blurring in space and/or time works because most <span class="hlt">image</span> "signal" is concentrated in the low Fourier components of an <span class="hlt">image</span>, while noise is evenly distributed. Blurring in space and/or time attenuates the high spatial and temporal frequencies, reducing noise at the expense of also attenuating <span class="hlt">image</span> detail. Noise-gating exploits the same property -- "coherence" -- that we use to identify features in <span class="hlt">images</span>, to separate <span class="hlt">image</span> features from noise.Processing <span class="hlt">image</span> sequences through 3-D noise gating results in spectacular (more than 10x) improvements in signal-to-noise ratio, while not blurring bright, resolved features in either space or time. This improves most types of <span class="hlt">image</span> analysis, including feature identification, time sequence extraction, absolute and relative photometry (including differential emission measure analysis), feature tracking, computer vision, correlation tracking, background modeling, cross-scale analysis, visual display/presentation, and <span class="hlt">image</span> compression.I will introduce noise gating, describe the method, and show examples from several instruments (including SDO</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=PIA04312&hterms=images+mars&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D20%26Ntt%3Dimages%2Bmars','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=PIA04312&hterms=images+mars&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D20%26Ntt%3Dimages%2Bmars"><span>Sojourner's First <span class="hlt">Images</span> From Mars</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>2003-01-01</p>
         <p>These <span class="hlt">images</span> are views of the Mars Pathfinder Lander's forward ramp before (top <span class="hlt">image</span>) and after (bottom <span class="hlt">image</span>) deployment. Some data from the before <span class="hlt">image</span> was lost due to rover-lander communication problems.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.sil.si.edu/imagegalaxy/imageGalaxy_whatsnew.cfm','SCIGOVWS'); return false;" href="http://www.sil.si.edu/imagegalaxy/imageGalaxy_whatsnew.cfm"><span>What's New | Galaxy of <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.science.gov/aboutsearch.html">Science.gov Websites</a></p>
      <p></p>
         <p></p>
         <p>] <em>View</em> <span class="hlt">Images</span> Details ID: SIL32-035-02 Enlarge <em><span class="hlt">Image</span></em> <em>View</em> <span class="hlt">Images</span> Details ID: SIL32-038-02 Enlarge <em><span class="hlt">Image</span></em> <em>View</em> <span class="hlt">Images</span> Details ID: SIL-2004_CT_6_1 Enlarge <em><span class="hlt">Image</span></em> <em>View</em> <span class="hlt">Images</span> Details ID: SIL32-010-01 Enlarge <em><span class="hlt">Image</span></em> <em>View</em> <span class="hlt">Images</span> Details ID: SIL32-013-05 Enlarge <em><span class="hlt">Image</span></em> <em>View</em> <span class="hlt">Images</span> Details ID: SIL32-014-02 Enlarge</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/870885','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/870885"><span><span class="hlt">Image</span> compression technique</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Fu, Chi-Yung; Petrich, Loren I.</p>
         <p>1997-01-01</p>
         <p>An <span class="hlt">image</span> is compressed by identifying edge pixels of the <span class="hlt">image</span>; creating a filled edge array of pixels each of the pixels in the filled edge array which corresponds to an edge pixel having a value equal to the value of a pixel of the <span class="hlt">image</span> array selected in response to the edge pixel, and each of the pixels in the filled edge array which does not correspond to an edge pixel having a value which is a weighted average of the values of surrounding pixels in the filled edge array which do correspond to edge pixels; and subtracting the filled edge array from the <span class="hlt">image</span> array to create a difference array. The edge file and the difference array are then separately compressed and transmitted or stored. The original <span class="hlt">image</span> is later reconstructed by creating a preliminary array in response to the received edge file, and adding the preliminary array to the received difference array. Filling is accomplished by solving Laplace's equation using a multi-grid technique. Contour and difference file coding techniques also are described. The techniques can be used in a method for processing a plurality of <span class="hlt">images</span> by selecting a respective compression approach for each <span class="hlt">image</span>, compressing each of the <span class="hlt">images</span> according to the compression approach selected, and transmitting each of the <span class="hlt">images</span> as compressed, in correspondence with an indication of the approach selected for the <span class="hlt">image</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1989SPIE.1092..584D','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1989SPIE.1092..584D"><span>Enhancement of PET <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Davis, Paul B.; Abidi, Mongi A.</p>
         <p>1989-05-01</p>
         <p>PET is the only <span class="hlt">imaging</span> modality that provides doctors with early analytic and quantitative biochemical assessment and precise localization of pathology. In PET <span class="hlt">images</span>, boundary information as well as local pixel intensity are both crucial for manual and/or automated feature tracing, extraction, and identification. Unfortunately, the present PET technology does not provide the necessary <span class="hlt">image</span> quality from which such precise analytic and quantitative measurements can be made. PET <span class="hlt">images</span> suffer from significantly high levels of radial noise present in the form of streaks caused by the inexactness of the models used in <span class="hlt">image</span> reconstruction. In this paper, our objective is to model PET noise and remove it without altering dominant features in the <span class="hlt">image</span>. The ultimate goal here is to enhance these dominant features to allow for automatic computer interpretation and classification of PET <span class="hlt">images</span> by developing techniques that take into consideration PET signal characteristics, data collection, and data reconstruction. We have modeled the noise steaks in PET <span class="hlt">images</span> in both rectangular and polar representations and have shown both analytically and through computer simulation that it exhibits consistent mapping patterns. A class of filters was designed and applied successfully. Visual inspection of the filtered <span class="hlt">images</span> show clear enhancement over the original <span class="hlt">images</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/458583','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/biblio/458583"><span><span class="hlt">Image</span> compression technique</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Fu, C.Y.; Petrich, L.I.</p>
         <p>1997-03-25</p>
         <p>An <span class="hlt">image</span> is compressed by identifying edge pixels of the <span class="hlt">image</span>; creating a filled edge array of pixels each of the pixels in the filled edge array which corresponds to an edge pixel having a value equal to the value of a pixel of the <span class="hlt">image</span> array selected in response to the edge pixel, and each of the pixels in the filled edge array which does not correspond to an edge pixel having a value which is a weighted average of the values of surrounding pixels in the filled edge array which do correspond to edge pixels; and subtracting the filled edge array from the <span class="hlt">image</span> array to create a difference array. The edge file and the difference array are then separately compressed and transmitted or stored. The original <span class="hlt">image</span> is later reconstructed by creating a preliminary array in response to the received edge file, and adding the preliminary array to the received difference array. Filling is accomplished by solving Laplace`s equation using a multi-grid technique. Contour and difference file coding techniques also are described. The techniques can be used in a method for processing a plurality of <span class="hlt">images</span> by selecting a respective compression approach for each <span class="hlt">image</span>, compressing each of the <span class="hlt">images</span> according to the compression approach selected, and transmitting each of the <span class="hlt">images</span> as compressed, in correspondence with an indication of the approach selected for the <span class="hlt">image</span>. 16 figs.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_5");'>5</a></li>
      <li><a href="#" onclick='return showDiv("page_6");'>6</a></li>
      <li class="active"><span>7</span></li>
   <li><a href="#" onclick='return showDiv("page_8");'>8</a></li>
      <li><a href="#" onclick='return showDiv("page_9");'>9</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_7 -->
   <div id="page_8" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_6");'>6</a></li>
      <li><a href="#" onclick='return showDiv("page_7");'>7</a></li>
      <li class="active"><span>8</span></li>
   <li><a href="#" onclick='return showDiv("page_9");'>9</a></li>
      <li><a href="#" onclick='return showDiv("page_10");'>10</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="141">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2017SPIE10333E..0WP','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2017SPIE10333E..0WP"><span>Correlation plenoptic <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Pepe, Francesco V.; Di Lena, Francesco; Garuccio, Augusto; D'Angelo, Milena</p>
         <p>2017-06-01</p>
         <p>Plenoptic <span class="hlt">Imaging</span> (PI) is a novel optical technique for achieving tridimensional <span class="hlt">imaging</span> in a single shot. In conventional PI, a microlens array is inserted in the native <span class="hlt">image</span> plane and the sensor array is moved behind the microlenses. On the one hand, the microlenses act as <span class="hlt">imaging</span> pixels to reproduce the <span class="hlt">image</span> of the scene; on the other hand, each microlens reproduces on the sensor array an <span class="hlt">image</span> of the camera lens, thus providing the angular information associated with each <span class="hlt">imaging</span> pixel. The recorded propagation direction is exploited, in post- processing, to computationally retrace the geometrical light path, thus enabling the refocusing of different planes within the scene, the extension of the depth of field of the acquired <span class="hlt">image</span>, as well as the 3D reconstruction of the scene. However, a trade-off between spatial and angular resolution is built in the standard plenoptic <span class="hlt">imaging</span> process. We demonstrate that the second-order spatio-temporal correlation properties of light can be exploited to overcome this fundamental limitation. Using two correlated beams, from either a chaotic or an entangled photon source, we can perform <span class="hlt">imaging</span> in one arm and simultaneously obtain the angular information in the other arm. In fact, we show that the second order correlation function possesses plenoptic <span class="hlt">imaging</span> properties (i.e., it encodes both spatial and angular information), and is thus characterized by a key re-focusing and 3D <span class="hlt">imaging</span> capability. From a fundamental standpoint, the plenoptic application is the first situation where the counterintuitive properties of correlated systems are effectively used to beat intrinsic limits of standard <span class="hlt">imaging</span> systems. From a practical standpoint, our protocol can dramatically enhance the potentials of PI, paving the way towards its promising applications.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19740019552','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19740019552"><span>Introduction to computer <span class="hlt">image</span> processing</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Moik, J. G.</p>
         <p>1973-01-01</p>
         <p>Theoretical backgrounds and digital techniques for a class of <span class="hlt">image</span> processing problems are presented. <span class="hlt">Image</span> formation in the context of linear system theory, <span class="hlt">image</span> evaluation, noise characteristics, mathematical operations on <span class="hlt">image</span> and their implementation are discussed. Various techniques for <span class="hlt">image</span> restoration and <span class="hlt">image</span> enhancement are presented. Methods for object extraction and the problem of pictorial pattern recognition and classification are discussed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20090022352','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20090022352"><span>Synthetic Foveal <span class="hlt">Imaging</span> Technology</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Hoenk, Michael; Monacos, Steve; Nikzad, Shouleh</p>
         <p>2009-01-01</p>
         <p>Synthetic Foveal <span class="hlt">imaging</span> Technology (SyFT) is an emerging discipline of <span class="hlt">image</span> capture and <span class="hlt">image</span>-data processing that offers the prospect of greatly increased capabilities for real-time processing of large, high-resolution <span class="hlt">images</span> (including mosaic <span class="hlt">images</span>) for such purposes as automated recognition and tracking of moving objects of interest. SyFT offers a solution to the <span class="hlt">image</span>-data processing problem arising from the proposed development of gigapixel mosaic focal-plane <span class="hlt">image</span>-detector assemblies for very wide field-of-view <span class="hlt">imaging</span> with high resolution for detecting and tracking sparse objects or events within narrow subfields of view. In order to identify and track the objects or events without the means of dynamic adaptation to be afforded by SyFT, it would be necessary to post-process data from an <span class="hlt">image</span>-data space consisting of terabytes of data. Such post-processing would be time-consuming and, as a consequence, could result in missing significant events that could not be observed at all due to the time evolution of such events or could not be observed at required levels of fidelity without such real-time adaptations as adjusting focal-plane operating conditions or aiming of the focal plane in different directions to track such events. The basic concept of foveal <span class="hlt">imaging</span> is straightforward: In imitation of a natural eye, a foveal-vision <span class="hlt">image</span> sensor is designed to offer higher resolution in a small region of interest (ROI) within its field of view. Foveal vision reduces the amount of unwanted information that must be transferred from the <span class="hlt">image</span> sensor to external <span class="hlt">image</span>-data-processing circuitry. The aforementioned basic concept is not new in itself: indeed, <span class="hlt">image</span> sensors based on these concepts have been described in several previous NASA Tech Briefs articles. Active-pixel integrated-circuit <span class="hlt">image</span> sensors that can be programmed in real time to effect foveal artificial vision on demand are one such example. What is new in SyFT is a synergistic combination of recent</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/23315587','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/23315587"><span>[<span class="hlt">Imaging</span> center - optimization of the <span class="hlt">imaging</span> process].</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Busch, H-P</p>
         <p>2013-04-01</p>
         <p>Hospitals around the world are under increasing pressure to optimize the economic efficiency of treatment processes. <span class="hlt">Imaging</span> is responsible for a great part of the success but also of the costs of treatment. In routine work an excessive supply of <span class="hlt">imaging</span> methods leads to an "as well as" strategy up to the limit of the capacity without critical reflection. Exams that have no predictable influence on the clinical outcome are an unjustified burden for the patient. They are useless and threaten the financial situation and existence of the hospital. In recent years the focus of process optimization was exclusively on the quality and efficiency of performed single examinations. In the future critical discussion of the effectiveness of single exams in relation to the clinical outcome will be more important. Unnecessary exams can be avoided, only if in addition to the optimization of single exams (efficiency) there is an optimization strategy for the total <span class="hlt">imaging</span> process (efficiency and effectiveness). This requires a new definition of processes (<span class="hlt">Imaging</span> Pathway), new structures for organization (<span class="hlt">Imaging</span> Center) and a new kind of thinking on the part of the medical staff. Motivation has to be changed from gratification of performed exams to gratification of process quality (medical quality, service quality, economics), including the avoidance of additional (unnecessary) exams. © Georg Thieme Verlag KG Stuttgart · New York.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2001NIMPA.471..140H','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2001NIMPA.471..140H"><span>Dual-modality <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Hasegawa, Bruce; Tang, H. Roger; Da Silva, Angela J.; Wong, Kenneth H.; Iwata, Koji; Wu, Max C.</p>
         <p>2001-09-01</p>
         <p>In comparison to conventional medical <span class="hlt">imaging</span> techniques, dual-modality <span class="hlt">imaging</span> offers the advantage of correlating anatomical information from X-ray computed tomography (CT) with functional measurements from single-photon emission computed tomography (SPECT) or with positron emission tomography (PET). The combined X-ray/radionuclide <span class="hlt">images</span> from dual-modality <span class="hlt">imaging</span> can help the clinician to differentiate disease from normal uptake of radiopharmaceuticals, and to improve diagnosis and staging of disease. In addition, phantom and animal studies have demonstrated that a priori structural information from CT can be used to improve quantification of tissue uptake and organ function by correcting the radionuclide data for errors due to photon attenuation, partial volume effects, scatter radiation, and other physical effects. Dual-modality <span class="hlt">imaging</span> therefore is emerging as a method of improving the visual quality and the quantitative accuracy of radionuclide <span class="hlt">imaging</span> for diagnosis of patients with cancer and heart disease.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/26912443','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/26912443"><span>Interventional Molecular <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Solomon, Stephen B; Cornelis, Francois</p>
         <p>2016-04-01</p>
         <p>Although molecular <span class="hlt">imaging</span> has had a dramatic impact on diagnostic <span class="hlt">imaging</span>, it has only recently begun to be integrated into interventional procedures. Its significant impact is attributed to its ability to provide noninvasive, physiologic information that supplements conventional morphologic <span class="hlt">imaging</span>. The four major interventional opportunities for molecular <span class="hlt">imaging</span> are, first, to provide guidance to localize a target; second, to provide tissue analysis to confirm that the target has been reached; third, to provide in-room, posttherapy assessment; and fourth, to deliver targeted therapeutics. With improved understanding and application of(18)F-FDG, as well as the addition of new molecular probes beyond(18)F-FDG, the future holds significant promise for the expansion of molecular <span class="hlt">imaging</span> into the realm of interventional procedures. © 2016 by the Society of Nuclear Medicine and Molecular <span class="hlt">Imaging</span>, Inc.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/29944202','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/29944202"><span>Multiphoton Intravital Calcium <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Cheetham, Claire E J</p>
         <p>2018-06-26</p>
         <p>Multiphoton intravital calcium <span class="hlt">imaging</span> is a powerful technique that enables high-resolution longitudinal monitoring of cellular and subcellular activity hundreds of microns deep in the living organism. This unit addresses the application of 2-photon microscopy to <span class="hlt">imaging</span> of genetically encoded calcium indicators (GECIs) in the mouse brain. The protocols in this unit enable real-time intravital <span class="hlt">imaging</span> of intracellular calcium concentration simultaneously in hundreds of neurons, or at the resolution of single synapses, as mice respond to sensory stimuli or perform behavioral tasks. Protocols are presented for implantation of a cranial <span class="hlt">imaging</span> window to provide optical access to the brain and for 2-photon <span class="hlt">image</span> acquisition. Protocols for implantation of both open skull and thinned skull windows for single or multi-session <span class="hlt">imaging</span> are described. © 2018 by John Wiley & Sons, Inc. © 2018 John Wiley & Sons, Inc.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20150003153','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20150003153"><span>Synthetic Foveal <span class="hlt">Imaging</span> Technology</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Nikzad, Shouleh (Inventor); Monacos, Steve P. (Inventor); Hoenk, Michael E. (Inventor)</p>
         <p>2013-01-01</p>
         <p>Apparatuses and methods are disclosed that create a synthetic fovea in order to identify and highlight interesting portions of an <span class="hlt">image</span> for further processing and rapid response. Synthetic foveal <span class="hlt">imaging</span> implements a parallel processing architecture that uses reprogrammable logic to implement embedded, distributed, real-time foveal <span class="hlt">image</span> processing from different sensor types while simultaneously allowing for lossless storage and retrieval of raw <span class="hlt">image</span> data. Real-time, distributed, adaptive processing of multi-tap <span class="hlt">image</span> sensors with coordinated processing hardware used for each output tap is enabled. In mosaic focal planes, a parallel-processing network can be implemented that treats the mosaic focal plane as a single ensemble rather than a set of isolated sensors. Various applications are enabled for <span class="hlt">imaging</span> and robotic vision where processing and responding to enormous amounts of data quickly and efficiently is important.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/879966','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/879966"><span>Video Toroid Cavity <span class="hlt">Imager</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Gerald, II, Rex E.; Sanchez, Jairo; Rathke, Jerome W.</p>
         <p>2004-08-10</p>
         <p>A video toroid cavity <span class="hlt">imager</span> for in situ measurement of electrochemical properties of an electrolytic material sample includes a cylindrical toroid cavity resonator containing the sample and employs NMR and video <span class="hlt">imaging</span> for providing high-resolution spectral and visual information of molecular characteristics of the sample on a real-time basis. A large magnetic field is applied to the sample under controlled temperature and pressure conditions to simultaneously provide NMR spectroscopy and video <span class="hlt">imaging</span> capabilities for investigating electrochemical transformations of materials or the evolution of long-range molecular aggregation during cooling of hydrocarbon melts. The video toroid cavity <span class="hlt">imager</span> includes a miniature commercial video camera with an adjustable lens, a modified compression coin cell <span class="hlt">imager</span> with a fiat circular principal detector element, and a sample mounted on a transparent circular glass disk, and provides NMR information as well as a video <span class="hlt">image</span> of a sample, such as a polymer film, with micrometer resolution.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19990047045','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19990047045"><span>Integrated Dual <span class="hlt">Imaging</span> Detector</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Rust, David M.</p>
         <p>1999-01-01</p>
         <p>A new type of <span class="hlt">image</span> detector was designed to simultaneously analyze the polarization of light at all picture elements in a scene. The integrated Dual <span class="hlt">Imaging</span> detector (IDID) consists of a lenslet array and a polarizing beamsplitter bonded to a commercial charge coupled device (CCD). The IDID simplifies the design and operation of solar vector magnetographs and the <span class="hlt">imaging</span> polarimeters and spectroscopic <span class="hlt">imagers</span> used, for example, in atmosphere and solar research. When used in a solar telescope, the vector magnetic fields on the solar surface. Other applications include environmental monitoring, robot vision, and medical diagnoses (through the eye). Innovations in the IDID include (1) two interleaved <span class="hlt">imaging</span> arrays (one for each polarization plane); (2) large dynamic range (well depth of 10(exp 5) electrons per pixel); (3) simultaneous readout and display of both <span class="hlt">images</span>; and (4) laptop computer signal processing to produce polarization maps in field situations.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19820024965','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19820024965"><span>Photocapacitive <span class="hlt">image</span> converter</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Miller, W. E.; Sher, A.; Tsuo, Y. H. (Inventor)</p>
         <p>1982-01-01</p>
         <p>An apparatus for converting a radiant energy <span class="hlt">image</span> into corresponding electrical signals including an <span class="hlt">image</span> converter is described. The <span class="hlt">image</span> converter includes a substrate of semiconductor material, an insulating layer on the front surface of the substrate, and an electrical contact on the back surface of the substrate. A first series of parallel transparent conductive stripes is on the insulating layer with a processing circuit connected to each of the conductive stripes for detecting the modulated voltages generated thereon. In a first embodiment of the invention, a modulated light stripe perpendicular to the conductive stripes scans the <span class="hlt">image</span> converter. In a second embodiment a second insulating layer is deposited over the conductive stripes and a second series of parallel transparent conductive stripes perpendicular to the first series is on the second insulating layer. A different frequency current signal is applied to each of the second series of conductive stripes and a modulated <span class="hlt">image</span> is applied to the <span class="hlt">image</span> converter.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1229733','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1229733"><span><span class="hlt">Imaging</span> arrangement and microscope</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Pertsinidis, Alexandros; Chu, Steven</p>
         <p>2015-12-15</p>
         <p>An embodiment of the present invention is an <span class="hlt">imaging</span> arrangement that includes <span class="hlt">imaging</span> optics, a fiducial light source, and a control system. In operation, the <span class="hlt">imaging</span> optics separate light into first and second tight by wavelength and project the first and second light onto first and second areas within first and second detector regions, respectively. The <span class="hlt">imaging</span> optics separate fiducial light from the fiducial light source into first and second fiducial light and project the first and second fiducial light onto third and fourth areas within the first and second detector regions, respectively. The control system adjusts alignment of the <span class="hlt">imaging</span> optics so that the first and second fiducial light projected onto the first and second detector regions maintain relatively constant positions within the first and second detector regions, respectively. Another embodiment of the present invention is a microscope that includes the <span class="hlt">imaging</span> arrangement.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020083269','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020083269"><span><span class="hlt">Image</span> Processing Software</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1992-01-01</p>
         <p>To convert raw data into environmental products, the National Weather Service and other organizations use the Global 9000 <span class="hlt">image</span> processing system marketed by Global <span class="hlt">Imaging</span>, Inc. The company's GAE software package is an enhanced version of the TAE, developed by Goddard Space Flight Center to support remote sensing and <span class="hlt">image</span> processing applications. The system can be operated in three modes and is combined with HP Apollo workstation hardware.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/AD1042512','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/AD1042512"><span>Tinnitus Multimodal <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2016-12-01</p>
         <p><span class="hlt">images</span> were segmented into gray and white matter <span class="hlt">images</span> and spatially normalized to the MNI template (3 mm isotropic voxels) using the DARTEL toolbox in...AWARD NUMBER: W81XWH-13-1-0494 TITLE: Tinnitus Multimodal <span class="hlt">Imaging</span> PRINCIPAL INVESTIGATOR: Steven Wan Cheung CONTRACTING ORGANIZATION... Medical Research and Materiel Command Fort Detrick, Maryland 21702-5012 DISTRIBUTION STATEMENT: Approved for Public Release; Distribution Unlimited</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20834.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20834.html"><span>Dawn LAMO <span class="hlt">Image</span> 134</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-07-22</p>
         <p>Liber Crater is featured at lower left in this <span class="hlt">image</span> from Ceres. Named for the Roman god of agriculture, Liber is 14 miles 23 kilometers. NASA's Dawn spacecraft took this <span class="hlt">image</span> on June 16, 2016, from its low-altitude mapping orbit, at a distance of about 240 miles (385 kilometers) above the surface. The <span class="hlt">image</span> resolution is 120 feet (35 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20834</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20856.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20856.html"><span>Dawn LAMO <span class="hlt">Image</span> 136</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-07-26</p>
         <p>Liber Crater is featured at lower left in this <span class="hlt">image</span> from Ceres. Named for the Roman god of agriculture, Liber is 14 miles (23 kilometers) wide. NASA's Dawn spacecraft took this <span class="hlt">image</span> on June 16, 2016, from its low-altitude mapping orbit, at a distance of about 240 miles (385 kilometers) above the surface. The <span class="hlt">image</span> resolution is 120 feet (35 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20834</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19830009707&hterms=apple&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D10%26Ntt%3Dapple','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19830009707&hterms=apple&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D10%26Ntt%3Dapple"><span>Apple <span class="hlt">Image</span> Processing Educator</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Gunther, F. J.</p>
         <p>1981-01-01</p>
         <p>A software system design is proposed and demonstrated with pilot-project software. The system permits the Apple II microcomputer to be used for personalized computer-assisted instruction in the digital <span class="hlt">image</span> processing of LANDSAT <span class="hlt">images</span>. The programs provide data input, menu selection, graphic and hard-copy displays, and both general and detailed instructions. The pilot-project results are considered to be successful indicators of the capabilities and limits of microcomputers for digital <span class="hlt">image</span> processing education.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA04627.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA04627.html"><span>Deep <span class="hlt">Imaging</span> Survey</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2003-07-25</p>
         <p>This is the first Deep <span class="hlt">Imaging</span> Survey <span class="hlt">image</span> taken by NASA Galaxy Evolution Explorer. On June 22 and 23, 2003, the spacecraft obtained this near ultraviolet <span class="hlt">image</span> of the Groth region by adding multiple orbits for a total exposure time of 14,000 seconds. Tens of thousands of objects can be identified in this picture. http://photojournal.jpl.nasa.gov/catalog/PIA04627</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1260239','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1260239"><span>Beam <span class="hlt">imaging</span> sensor</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>McAninch, Michael D.; Root, Jeffrey J.</p>
         <p>2016-07-05</p>
         <p>The present invention relates generally to the field of sensors for beam <span class="hlt">imaging</span> and, in particular, to a new and useful beam <span class="hlt">imaging</span> sensor for use in determining, for example, the power density distribution of a beam including, but not limited to, an electron beam or an ion beam. In one embodiment, the beam <span class="hlt">imaging</span> sensor of the present invention comprises, among other items, a circumferential slit that is either circular, elliptical or polygonal in nature.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA189440','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA189440"><span>Computing Intrinsic <span class="hlt">Images</span>.</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1986-08-01</p>
         <p>most of the algorithms fail when applied to real <span class="hlt">images</span>. (2) Usually the constraints from the geometry and the physics of the problem are not enough...large subset of real <span class="hlt">images</span>), and so most of the algorithms fail when applied to real <span class="hlt">images</span>. (2) Usually the constraints from the geometry and the...constraints from the geometry and the physics of the problem are not enough to guarantee uniqueness of the computed parameters. In this case, strong</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_6");'>6</a></li>
      <li><a href="#" onclick='return showDiv("page_7");'>7</a></li>
      <li class="active"><span>8</span></li>
   <li><a href="#" onclick='return showDiv("page_9");'>9</a></li>
      <li><a href="#" onclick='return showDiv("page_10");'>10</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_8 -->
   <div id="page_9" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_7");'>7</a></li>
      <li><a href="#" onclick='return showDiv("page_8");'>8</a></li>
      <li class="active"><span>9</span></li>
   <li><a href="#" onclick='return showDiv("page_10");'>10</a></li>
      <li><a href="#" onclick='return showDiv("page_11");'>11</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="161">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA204490','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA204490"><span>Human <span class="hlt">Image</span> Understanding</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1989-01-01</p>
         <p>A Theory of Human <span class="hlt">Image</span> Understanding &#34 and the reprint of the chapter &#34Aspects and...Extensions of a Theory of Human <span class="hlt">Image</span> Understanding &#34 in Z. Pylyshyn (Ed). CONTENTS I. Introduction and Background ............................... 2 II. A...edges Fig;i 4 Some nonacmdentg differences between a brick and a cylinder. From Fig. 5, Recognition-by-Components: A theory of human <span class="hlt">image</span></p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2145802','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2145802"><span><span class="hlt">Images</span> of Illness</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Longhurst, Mark F.</p>
         <p>1992-01-01</p>
         <p>The <span class="hlt">images</span> we as physicians retain of our patients have a bearing on the evolution of our clinical behaviour and attributes. These <span class="hlt">images</span> can enhance our diagnostic and therapeutic skills, increase our capacity to care for people with incurable diseases, and offer insights into our own emotional response. A recollection of five people with Parkinson's disease offers a college of <span class="hlt">images</span> to give us further insights into the meaning of illness-for the patient and the physician. PMID:20469529</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1999PhDT........29Z','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1999PhDT........29Z"><span>Investigations of <span class="hlt">image</span> fusion</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Zhang, Zhong</p>
         <p>1999-12-01</p>
         <p>The objective of <span class="hlt">image</span> fusion is to combine information from multiple <span class="hlt">images</span> of the same scene. The result of <span class="hlt">image</span> fusion is a single <span class="hlt">image</span> which is more suitable for the purpose of human visual perception or further <span class="hlt">image</span> processing tasks. In this thesis, a region-based fusion algorithm using the wavelet transform is proposed. The identification of important features in each <span class="hlt">image</span>, such as edges and regions of interest, are used to guide the fusion process. The idea of multiscale grouping is also introduced and a generic <span class="hlt">image</span> fusion framework based on multiscale decomposition is studied. The framework includes all of the existing multiscale-decomposition- based fusion approaches we found in the literature which did not assume a statistical model for the source <span class="hlt">images</span>. Comparisons indicate that our framework includes some new approaches which outperform the existing approaches for the cases we consider. Registration must precede our fusion algorithms. So we proposed a hybrid scheme which uses both feature-based and intensity-based methods. The idea of robust estimation of optical flow from time- varying <span class="hlt">images</span> is employed with a coarse-to-fine multi- resolution approach and feature-based registration to overcome some of the limitations of the intensity-based schemes. Experiments show that this approach is robust and efficient. Assessing <span class="hlt">image</span> fusion performance in a real application is a complicated issue. In this dissertation, a mixture probability density function model is used in conjunction with the Expectation- Maximization algorithm to model histograms of edge intensity. Some new techniques are proposed for estimating the quality of a noisy <span class="hlt">image</span> of a natural scene. Such quality measures can be used to guide the fusion. Finally, we study fusion of <span class="hlt">images</span> obtained from several copies of a new type of camera developed for video surveillance. Our techniques increase the capability and reliability of the surveillance system and provide an easy way to obtain 3-D</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1227874','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1227874"><span>Microscopy <span class="hlt">imaging</span> device with advanced <span class="hlt">imaging</span> properties</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Ghosh, Kunal; Burns, Laurie; El Gamal, Abbas; Schnitzer, Mark J.; Cocker, Eric; Ho, Tatt Wei</p>
         <p>2015-11-24</p>
         <p>Systems, methods and devices are implemented for microscope <span class="hlt">imaging</span> solutions. One embodiment of the present disclosure is directed toward an epifluorescence microscope. The microscope includes an <span class="hlt">image</span> capture circuit including an array of optical sensor. An optical arrangement is configured to direct excitation light of less than about 1 mW to a target object in a field of view of that is at least 0.5 mm.sup.2 and to direct epi-fluorescence emission caused by the excitation light to the array of optical sensors. The optical arrangement and array of optical sensors are each sufficiently close to the target object to provide at least 2.5 .mu.m resolution for an <span class="hlt">image</span> of the field of view.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1330343','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1330343"><span>Microscopy <span class="hlt">imaging</span> device with advanced <span class="hlt">imaging</span> properties</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Ghosh, Kunal; Burns, Laurie; El Gamal, Abbas; Schnitzer, Mark J.; Cocker, Eric; Ho, Tatt Wei</p>
         <p>2016-10-25</p>
         <p>Systems, methods and devices are implemented for microscope <span class="hlt">imaging</span> solutions. One embodiment of the present disclosure is directed toward an epifluorescence microscope. The microscope includes an <span class="hlt">image</span> capture circuit including an array of optical sensor. An optical arrangement is configured to direct excitation light of less than about 1 mW to a target object in a field of view of that is at least 0.5 mm.sup.2 and to direct epi-fluorescence emission caused by the excitation light to the array of optical sensors. The optical arrangement and array of optical sensors are each sufficiently close to the target object to provide at least 2.5 .mu.m resolution for an <span class="hlt">image</span> of the field of view.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1333302','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1333302"><span>Microscopy <span class="hlt">imaging</span> device with advanced <span class="hlt">imaging</span> properties</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Ghosh, Kunal; Burns, Laurie; El Gamal, Abbas; Schnitzer, Mark J.; Cocker, Eric; Ho, Tatt Wei</p>
         <p>2016-11-22</p>
         <p>Systems, methods and devices are implemented for microscope <span class="hlt">imaging</span> solutions. One embodiment of the present disclosure is directed toward an epifluorescence microscope. The microscope includes an <span class="hlt">image</span> capture circuit including an array of optical sensor. An optical arrangement is configured to direct excitation light of less than about 1 mW to a target object in a field of view of that is at least 0.5 mm.sup.2 and to direct epi-fluorescence emission caused by the excitation light to the array of optical sensors. The optical arrangement and array of optical sensors are each sufficiently close to the target object to provide at least 2.5 .mu.m resolution for an <span class="hlt">image</span> of the field of view.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1353096','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1353096"><span>Microscopy <span class="hlt">imaging</span> device with advanced <span class="hlt">imaging</span> properties</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Ghosh, Kunal; Burns, Laurie; El Gamal, Abbas; Schnitzer, Mark J.; Cocker, Eric; Ho, Tatt Wei</p>
         <p>2017-04-25</p>
         <p>Systems, methods and devices are implemented for microscope <span class="hlt">imaging</span> solutions. One embodiment of the present disclosure is directed toward an epifluorescence microscope. The microscope includes an <span class="hlt">image</span> capture circuit including an array of optical sensor. An optical arrangement is configured to direct excitation light of less than about 1 mW to a target object in a field of view of that is at least 0.5 mm.sup.2 and to direct epi-fluorescence emission caused by the excitation light to the array of optical sensors. The optical arrangement and array of optical sensors are each sufficiently close to the target object to provide at least 2.5 .mu.m resolution for an <span class="hlt">image</span> of the field of view.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20000030658','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20000030658"><span>Ultrasonic <span class="hlt">Imaging</span> System</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Youngquist, Robert C. (Inventor); Moerk, Steven (Inventor)</p>
         <p>1999-01-01</p>
         <p>An <span class="hlt">imaging</span> system is described which can be used to either passively search for sources of ultrasonics or as an active phase <span class="hlt">imaging</span> system. which can <span class="hlt">image</span> fires. gas leaks, or air temperature gradients. This system uses an array of ultrasonic receivers coupled to an ultrasound collector or lens to provide an electronic <span class="hlt">image</span> of the ultrasound intensity in a selected angular region of space. A system is described which includes a video camera to provide a visual reference to a region being examined for ultrasonic signals.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/29751058','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/29751058"><span><span class="hlt">Imaging</span> brain tumour microstructure.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Nilsson, Markus; Englund, Elisabet; Szczepankiewicz, Filip; van Westen, Danielle; Sundgren, Pia C</p>
         <p>2018-05-08</p>
         <p><span class="hlt">Imaging</span> is an indispensable tool for brain tumour diagnosis, surgical planning, and follow-up. Definite diagnosis, however, often demands histopathological analysis of microscopic features of tissue samples, which have to be obtained by invasive means. A non-invasive alternative may be to probe corresponding microscopic tissue characteristics by MRI, or so called 'microstructure <span class="hlt">imaging</span>'. The promise of microstructure <span class="hlt">imaging</span> is one of 'virtual biopsy' with the goal to offset the need for invasive procedures in favour of <span class="hlt">imaging</span> that can guide pre-surgical planning and can be repeated longitudinally to monitor and predict treatment response. The exploration of such methods is motivated by the striking link between parameters from MRI and tumour histology, for example the correlation between the apparent diffusion coefficient and cellularity. Recent microstructure <span class="hlt">imaging</span> techniques probe even more subtle and specific features, providing parameters associated to cell shape, size, permeability, and volume distributions. However, the range of scenarios in which these techniques provide reliable <span class="hlt">imaging</span> biomarkers that can be used to test medical hypotheses or support clinical decisions is yet unknown. Accurate microstructure <span class="hlt">imaging</span> may moreover require acquisitions that go beyond conventional data acquisition strategies. This review covers a wide range of candidate microstructure <span class="hlt">imaging</span> methods based on diffusion MRI and relaxometry, and explores advantages, challenges, and potential pitfalls in brain tumour microstructure <span class="hlt">imaging</span>. Copyright © 2018. Published by Elsevier Inc.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/140718','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/biblio/140718"><span>Ferroelectric optical <span class="hlt">image</span> comparator</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Butler, M.A.; Land, C.E.; Martin, S.J.; Pfeifer, K.B.</p>
         <p>1993-11-30</p>
         <p>A ferroelectric optical <span class="hlt">image</span> comparator has a lead lanthanum zirconate titanate thin-film device which is constructed with a semi-transparent or transparent conductive first electrode on one side of the thin film, a conductive metal second electrode on the other side of the thin film, and the second electrode is in contact with a nonconducting substrate. A photoinduced current in the device represents the dot product between a stored <span class="hlt">image</span> and an <span class="hlt">image</span> projected onto the first electrode. One-dimensional autocorrelations are performed by measuring this current while displacing the projected <span class="hlt">image</span>. 7 figures.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/16402378','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/16402378"><span>Turboprop: improved PROPELLER <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Pipe, James G; Zwart, Nicholas</p>
         <p>2006-02-01</p>
         <p>A variant of periodically rotated overlapping parallel lines with enhanced reconstruction (PROPELLER) MRI, called turboprop, is introduced. This method employs an oscillating readout gradient during each spin echo of the echo train to collect more lines of data per echo train, which reduces the minimum scan time, motion-related artifact, and specific absorption rate (SAR) while increasing sampling efficiency. It can be applied to conventional fast spin-echo (FSE) <span class="hlt">imaging</span>; however, this article emphasizes its application in diffusion-weighted <span class="hlt">imaging</span> (DWI). The method is described and compared with conventional PROPELLER <span class="hlt">imaging</span>, and clinical <span class="hlt">images</span> collected with this PROPELLER variant are shown. Copyright 2006 Wiley-Liss, Inc.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA04626.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA04626.html"><span>Groth Deep Locations <span class="hlt">Image</span></span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2003-07-25</p>
         <p>NASA's Galaxy Evolution Explorer photographed this ultraviolet color blowup of the Groth Deep <span class="hlt">Image</span> on June 22 and June 23, 2003. Hundreds of galaxies are detected in this portion of the <span class="hlt">image</span>, and the faint red galaxies are believed to be 6 billion light years away. The white boxes show the location of these distant galaxies, of which more than a 100 can be detected in this <span class="hlt">image</span>. NASA astronomers expect to detect 10,000 such galaxies after extrapolating to the full <span class="hlt">image</span> at a deeper exposure level. http://photojournal.jpl.nasa.gov/catalog/PIA04626</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19980236569','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19980236569"><span><span class="hlt">Image</span> Registration Workshop Proceedings</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>LeMoigne, Jacqueline (Editor)</p>
         <p>1997-01-01</p>
         <p>Automatic <span class="hlt">image</span> registration has often been considered as a preliminary step for higher-level processing, such as object recognition or data fusion. But with the unprecedented amounts of data which are being and will continue to be generated by newly developed sensors, the very topic of automatic <span class="hlt">image</span> registration has become and important research topic. This workshop presents a collection of very high quality work which has been grouped in four main areas: (1) theoretical aspects of <span class="hlt">image</span> registration; (2) applications to satellite imagery; (3) applications to medical imagery; and (4) <span class="hlt">image</span> registration for computer vision research.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2009SPIE.7264E..08H','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2009SPIE.7264E..08H"><span>Clinical <span class="hlt">image</span> processing engine</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Han, Wei; Yao, Jianhua; Chen, Jeremy; Summers, Ronald</p>
         <p>2009-02-01</p>
         <p>Our group provides clinical <span class="hlt">image</span> processing services to various institutes at NIH. We develop or adapt <span class="hlt">image</span> processing programs for a variety of applications. However, each program requires a human operator to select a specific set of <span class="hlt">images</span> and execute the program, as well as store the results appropriately for later use. To improve efficiency, we design a parallelized clinical <span class="hlt">image</span> processing engine (CIPE) to streamline and parallelize our service. The engine takes DICOM <span class="hlt">images</span> from a PACS server, sorts and distributes the <span class="hlt">images</span> to different applications, multithreads the execution of applications, and collects results from the applications. The engine consists of four modules: a listener, a router, a job manager and a data manager. A template filter in XML format is defined to specify the <span class="hlt">image</span> specification for each application. A MySQL database is created to store and manage the incoming DICOM <span class="hlt">images</span> and application results. The engine achieves two important goals: reduce the amount of time and manpower required to process medical <span class="hlt">images</span>, and reduce the turnaround time for responding. We tested our engine on three different applications with 12 datasets and demonstrated that the engine improved the efficiency dramatically.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016OptEn..55h3105S','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016OptEn..55h3105S"><span>Fiber pixelated <span class="hlt">image</span> database</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Shinde, Anant; Perinchery, Sandeep Menon; Matham, Murukeshan Vadakke</p>
         <p>2016-08-01</p>
         <p><span class="hlt">Imaging</span> of physically inaccessible parts of the body such as the colon at micron-level resolution is highly important in diagnostic medical <span class="hlt">imaging</span>. Though flexible endoscopes based on the <span class="hlt">imaging</span> fiber bundle are used for such diagnostic procedures, their inherent honeycomb-like structure creates fiber pixelation effects. This impedes the observer from perceiving the information from an <span class="hlt">image</span> captured and hinders the direct use of <span class="hlt">image</span> processing and machine intelligence techniques on the recorded signal. Significant efforts have been made by researchers in the recent past in the development and implementation of pixelation removal techniques. However, researchers have often used their own set of <span class="hlt">images</span> without making source data available which subdued their usage and adaptability universally. A database of pixelated <span class="hlt">images</span> is the current requirement to meet the growing diagnostic needs in the healthcare arena. An innovative fiber pixelated <span class="hlt">image</span> database is presented, which consists of pixelated <span class="hlt">images</span> that are synthetically generated and experimentally acquired. Sample space encompasses test patterns of different scales, sizes, and shapes. It is envisaged that this proposed database will alleviate the current limitations associated with relevant research and development and would be of great help for researchers working on comb structure removal algorithms.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20291.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20291.html"><span>Color <span class="hlt">Image</span> of Pluto</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-12-31</p>
         <p>Pluto nearly fills the frame in this <span class="hlt">image</span> from the Long Range Reconnaissance <span class="hlt">Imager</span> (LORRI) aboard New Horizons, taken on July 13, 2015, when the spacecraft was 476,000 miles (768,000 kilometers) from the surface. This is the last and most detailed <span class="hlt">image</span> sent to Earth before the spacecraft's closest approach to Pluto on July 14. The color <span class="hlt">image</span> has been combined with lower-resolution color information from the Ralph instrument that was acquired earlier on July 13. http://photojournal.jpl.nasa.gov/catalog/PIA20291</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/869044','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/869044"><span>Ferroelectric optical <span class="hlt">image</span> comparator</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Butler, Michael A.; Land, Cecil E.; Martin, Stephen J.; Pfeifer, Kent B.</p>
         <p>1993-01-01</p>
         <p>A ferroelectric optical <span class="hlt">image</span> comparator has a lead lanthanum zirconate titanate thin-film device which is constructed with a semi-transparent or transparent conductive first electrode on one side of the thin film, a conductive metal second electrode on the other side of the thin film, and the second electrode is in contact with a nonconducting substrate. A photoinduced current in the device represents the dot product between a stored <span class="hlt">image</span> and an <span class="hlt">image</span> projected onto the first electrode. One-dimensional autocorrelations are performed by measuring this current while displacing the projected <span class="hlt">image</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020090911','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020090911"><span><span class="hlt">Image</span> Processing System</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1986-01-01</p>
         <p>Mallinckrodt Institute of Radiology (MIR) is using a digital <span class="hlt">image</span> processing system which employs NASA-developed technology. MIR's computer system is the largest radiology system in the world. It is used in diagnostic <span class="hlt">imaging</span>. Blood vessels are injected with x-ray dye, and the <span class="hlt">images</span> which are produced indicate whether arteries are hardened or blocked. A computer program developed by Jet Propulsion Laboratory known as Mini-VICAR/IBIS was supplied to MIR by COSMIC. The program provides the basis for developing the computer <span class="hlt">imaging</span> routines for data processing, contrast enhancement and picture display.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/ency/article/007451.htm','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/ency/article/007451.htm"><span><span class="hlt">Imaging</span> and radiology</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>Interventional radiology; Diagnostic radiology; X-ray <span class="hlt">imaging</span> ... DIAGNOSTIC RADIOLOGY Diagnostic radiology helps health care professionals see structures inside your body. Doctors that specialize in the interpretation ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/7016834','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/biblio/7016834"><span>Quantitative luminescence <span class="hlt">imaging</span> system</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Erwin, D.N.; Kiel, J.L.; Batishko, C.R.; Stahl, K.A.</p>
         <p>1990-08-14</p>
         <p>The QLIS <span class="hlt">images</span> and quantifies low-level chemiluminescent reactions in an electromagnetic field. It is capable of real time nonperturbing measurement and simultaneous recording of many biochemical and chemical reactions such as luminescent immunoassays or enzyme assays. The system comprises <span class="hlt">image</span> transfer optics, a low-light level digitizing camera with <span class="hlt">image</span> intensifying microchannel plates, an <span class="hlt">image</span> process or, and a control computer. The <span class="hlt">image</span> transfer optics may be a fiber <span class="hlt">image</span> guide with a bend, or a microscope, to take the light outside of the RF field. Output of the camera is transformed into a localized rate of cumulative digitalized data or enhanced video display or hard-copy <span class="hlt">images</span>. The system may be used as a luminescent microdosimetry device for radiofrequency or microwave radiation, as a thermal dosimeter, or in the dosimetry of ultra-sound (sonoluminescence) or ionizing radiation. It provides a near-real-time system capable of measuring the extremely low light levels from luminescent reactions in electromagnetic fields in the areas of chemiluminescence assays and thermal microdosimetry, and is capable of near-real-time <span class="hlt">imaging</span> of the sample to allow spatial distribution analysis of the reaction. It can be used to instrument three distinctly different irradiation configurations, comprising (1) RF waveguide irradiation of a small Petri-dish-shaped sample cell, (2) RF irradiation of samples in a microscope for the microscopic <span class="hlt">imaging</span> and measurement, and (3) RF irradiation of small to human body-sized samples in an anechoic chamber. 22 figs.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_7");'>7</a></li>
      <li><a href="#" onclick='return showDiv("page_8");'>8</a></li>
      <li class="active"><span>9</span></li>
   <li><a href="#" onclick='return showDiv("page_10");'>10</a></li>
      <li><a href="#" onclick='return showDiv("page_11");'>11</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_9 -->
   <div id="page_10" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_8");'>8</a></li>
      <li><a href="#" onclick='return showDiv("page_9");'>9</a></li>
      <li class="active"><span>10</span></li>
   <li><a href="#" onclick='return showDiv("page_11");'>11</a></li>
      <li><a href="#" onclick='return showDiv("page_12");'>12</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="181">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/867493','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/867493"><span>Quantitative luminescence <span class="hlt">imaging</span> system</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Erwin, David N.; Kiel, Johnathan L.; Batishko, Charles R.; Stahl, Kurt A.</p>
         <p>1990-01-01</p>
         <p>The QLIS <span class="hlt">images</span> and quantifies low-level chemiluminescent reactions in an electromagnetic field. It is capable of real time nonperturbing measurement and simultaneous recording of many biochemical and chemical reactions such as luminescent immunoassays or enzyme assays. The system comprises <span class="hlt">image</span> transfer optics, a low-light level digitizing camera with <span class="hlt">image</span> intensifying microchannel plates, an <span class="hlt">image</span> process or, and a control computer. The <span class="hlt">image</span> transfer optics may be a fiber <span class="hlt">image</span> guide with a bend, or a microscope, to take the light outside of the RF field. Output of the camera is transformed into a localized rate of cumulative digitalized data or enhanced video display or hard-copy <span class="hlt">images</span>. The system may be used as a luminescent microdosimetry device for radiofrequency or microwave radiation, as a thermal dosimeter, or in the dosimetry of ultra-sound (sonoluminescence) or ionizing radiation. It provides a near-real-time system capable of measuring the extremely low light levels from luminescent reactions in electromagnetic fields in the areas of chemiluminescence assays and thermal microdosimetry, and is capable of near-real-time <span class="hlt">imaging</span> of the sample to allow spatial distribution analysis of the reaction. It can be used to instrument three distinctly different irradiation configurations, comprising (1) RF waveguide irradiation of a small Petri-dish-shaped sample cell, (2) RF irradiation of samples in a microscope for the microscopie <span class="hlt">imaging</span> and measurement, and (3) RF irradiation of small to human body-sized samples in an anechoic chamber.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2017ApJ...850..172J','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2017ApJ...850..172J"><span>Dynamical <span class="hlt">Imaging</span> with Interferometry</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Johnson, Michael D.; Bouman, Katherine L.; Blackburn, Lindy; Chael, Andrew A.; Rosen, Julian; Shiokawa, Hotaka; Roelofs, Freek; Akiyama, Kazunori; Fish, Vincent L.; Doeleman, Sheperd S.</p>
         <p>2017-12-01</p>
         <p>By linking widely separated radio dishes, the technique of very long baseline interferometry (VLBI) can greatly enhance angular resolution in radio astronomy. However, at any given moment, a VLBI array only sparsely samples the information necessary to form an <span class="hlt">image</span>. Conventional <span class="hlt">imaging</span> techniques partially overcome this limitation by making the assumption that the observed cosmic source structure does not evolve over the duration of an observation, which enables VLBI networks to accumulate information as Earth rotates and changes the projected array geometry. Although this assumption is appropriate for nearly all VLBI, it is almost certainly violated for submillimeter observations of the Galactic center supermassive black hole, Sagittarius A* (Sgr A*), which has a gravitational timescale of only ∼ 20 s and exhibits intrahour variability. To address this challenge, we develop several techniques to reconstruct dynamical <span class="hlt">images</span> (“movies”) from interferometric data. Our techniques are applicable to both single-epoch and multiepoch variability studies, and they are suitable for exploring many different physical processes including flaring regions, stable <span class="hlt">images</span> with small time-dependent perturbations, steady accretion dynamics, or kinematics of relativistic jets. Moreover, dynamical <span class="hlt">imaging</span> can be used to estimate time-averaged <span class="hlt">images</span> from time-variable data, eliminating many spurious <span class="hlt">image</span> artifacts that arise when using standard <span class="hlt">imaging</span> methods. We demonstrate the effectiveness of our techniques using synthetic observations of simulated black hole systems and 7 mm Very Long Baseline Array observations of M87, and we show that dynamical <span class="hlt">imaging</span> is feasible for Event Horizon Telescope observations of Sgr A*.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/19380271','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/19380271"><span>Sparse <span class="hlt">image</span> reconstruction for molecular <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Ting, Michael; Raich, Raviv; Hero, Alfred O</p>
         <p>2009-06-01</p>
         <p>The application that motivates this paper is molecular <span class="hlt">imaging</span> at the atomic level. When discretized at subatomic distances, the volume is inherently sparse. Noiseless measurements from an <span class="hlt">imaging</span> technology can be modeled by convolution of the <span class="hlt">image</span> with the system point spread function (psf). Such is the case with magnetic resonance force microscopy (MRFM), an emerging technology where <span class="hlt">imaging</span> of an individual tobacco mosaic virus was recently demonstrated with nanometer resolution. We also consider additive white Gaussian noise (AWGN) in the measurements. Many prior works of sparse estimators have focused on the case when H has low coherence; however, the system matrix H in our application is the convolution matrix for the system psf. A typical convolution matrix has high coherence. This paper, therefore, does not assume a low coherence H. A discrete-continuous form of the Laplacian and atom at zero (LAZE) p.d.f. used by Johnstone and Silverman is formulated, and two sparse estimators derived by maximizing the joint p.d.f. of the observation and <span class="hlt">image</span> conditioned on the hyperparameters. A thresholding rule that generalizes the hard and soft thresholding rule appears in the course of the derivation. This so-called hybrid thresholding rule, when used in the iterative thresholding framework, gives rise to the hybrid estimator, a generalization of the lasso. Estimates of the hyperparameters for the lasso and hybrid estimator are obtained via Stein's unbiased risk estimate (SURE). A numerical study with a Gaussian psf and two sparse <span class="hlt">images</span> shows that the hybrid estimator outperforms the lasso.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2012ascl.soft06013R','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2012ascl.soft06013R"><span><span class="hlt">Image</span>J: <span class="hlt">Image</span> processing and analysis in Java</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Rasband, W. S.</p>
         <p>2012-06-01</p>
         <p><span class="hlt">Image</span>J is a public domain Java <span class="hlt">image</span> processing program inspired by NIH <span class="hlt">Image</span>. It can display, edit, analyze, process, save and print 8-bit, 16-bit and 32-bit <span class="hlt">images</span>. It can read many <span class="hlt">image</span> formats including TIFF, GIF, JPEG, BMP, DICOM, FITS and "raw". It supports "stacks", a series of <span class="hlt">images</span> that share a single window. It is multithreaded, so time-consuming operations such as <span class="hlt">image</span> file reading can be performed in parallel with other operations.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3606310','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3606310"><span>BMC Ecology <span class="hlt">image</span> competition: the winning <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p></p>
         <p>2013-01-01</p>
         <p>BMC Ecology announces the winning entries in its inaugural Ecology <span class="hlt">Image</span> Competition, open to anyone affiliated with a research institute. The competition, which received more than 200 entries from international researchers at all career levels and a wide variety of scientific disciplines, was looking for striking visual interpretations of ecological processes. In this Editorial, our academic Section Editors and guest judge Dr Yan Wong explain what they found most appealing about their chosen winning entries, and highlight a few of the outstanding <span class="hlt">images</span> that didn’t quite make it to the top prize. PMID:23517630</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/23517630','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/23517630"><span>BMC Ecology <span class="hlt">image</span> competition: the winning <span class="hlt">images</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Harold, Simon; Wong, Yan; Baguette, Michel; Bonsall, Michael B; Clobert, Jean; Royle, Nick J; Settele, Josef</p>
         <p>2013-03-22</p>
         <p>BMC Ecology announces the winning entries in its inaugural Ecology <span class="hlt">Image</span> Competition, open to anyone affiliated with a research institute. The competition, which received more than 200 entries from international researchers at all career levels and a wide variety of scientific disciplines, was looking for striking visual interpretations of ecological processes. In this Editorial, our academic Section Editors and guest judge Dr Yan Wong explain what they found most appealing about their chosen winning entries, and highlight a few of the outstanding <span class="hlt">images</span> that didn't quite make it to the top prize.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20110015087','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20110015087"><span>Multipurpose Hyperspectral <span class="hlt">Imaging</span> System</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Mao, Chengye; Smith, David; Lanoue, Mark A.; Poole, Gavin H.; Heitschmidt, Jerry; Martinez, Luis; Windham, William A.; Lawrence, Kurt C.; Park, Bosoon</p>
         <p>2005-01-01</p>
         <p>A hyperspectral <span class="hlt">imaging</span> system of high spectral and spatial resolution that incorporates several innovative features has been developed to incorporate a focal plane scanner (U.S. Patent 6,166,373). This feature enables the system to be used for both airborne/spaceborne and laboratory hyperspectral <span class="hlt">imaging</span> with or without relative movement of the <span class="hlt">imaging</span> system, and it can be used to scan a target of any size as long as the target can be <span class="hlt">imaged</span> at the focal plane; for example, automated inspection of food items and identification of single-celled organisms. The spectral resolution of this system is greater than that of prior terrestrial multispectral <span class="hlt">imaging</span> systems. Moreover, unlike prior high-spectral resolution airborne and spaceborne hyperspectral <span class="hlt">imaging</span> systems, this system does not rely on relative movement of the target and the <span class="hlt">imaging</span> system to sweep an <span class="hlt">imaging</span> line across a scene. This compact system (see figure) consists of a front objective mounted at a translation stage with a motorized actuator, and a line-slit <span class="hlt">imaging</span> spectrograph mounted within a rotary assembly with a rear adaptor to a charged-coupled-device (CCD) camera. Push-broom scanning is carried out by the motorized actuator which can be controlled either manually by an operator or automatically by a computer to drive the line-slit across an <span class="hlt">image</span> at a focal plane of the front objective. To reduce the cost, the system has been designed to integrate as many as possible off-the-shelf components including the CCD camera and spectrograph. The system has achieved high spectral and spatial resolutions by using a high-quality CCD camera, spectrograph, and front objective lens. Fixtures for attachment of the system to a microscope (U.S. Patent 6,495,818 B1) make it possible to acquire multispectral <span class="hlt">images</span> of single cells and other microscopic objects.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA521717','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA521717"><span>An <span class="hlt">Image</span> Secret Sharing Method</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2006-07-01</p>
         <p>the secret <span class="hlt">image</span> in lossless manner and (2) any or fewer <span class="hlt">image</span> shares cannot get sufficient information to reveal the ... secret <span class="hlt">image</span>. It is an effective, reliable and secure method to prevent the secret <span class="hlt">image</span> from being lost, stolen or corrupted. In comparison with...other <span class="hlt">image</span> secret sharing methods, this approach&#8217s advantages are its large compression rate on the size of the <span class="hlt">image</span> shares, its strong protection of the secret <span class="hlt">image</span> and its ability for real-time</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19730013662','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19730013662"><span><span class="hlt">Image</span> correlation and sampling study</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Popp, D. J.; Mccormack, D. S.; Sedwick, J. L.</p>
         <p>1972-01-01</p>
         <p>The development of analytical approaches for solving <span class="hlt">image</span> correlation and <span class="hlt">image</span> sampling of multispectral data is discussed. Relevant multispectral <span class="hlt">image</span> statistics which are applicable to <span class="hlt">image</span> correlation and sampling are identified. The general <span class="hlt">image</span> statistics include intensity mean, variance, amplitude histogram, power spectral density function, and autocorrelation function. The translation problem associated with digital <span class="hlt">image</span> registration and the analytical means for comparing commonly used correlation techniques are considered. General expressions for determining the reconstruction error for specific <span class="hlt">image</span> sampling strategies are developed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2007SPIE.6514E..2PL','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2007SPIE.6514E..2PL"><span>Automated <span class="hlt">image</span> analysis of uterine cervical <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Li, Wenjing; Gu, Jia; Ferris, Daron; Poirson, Allen</p>
         <p>2007-03-01</p>
         <p>Cervical Cancer is the second most common cancer among women worldwide and the leading cause of cancer mortality of women in developing countries. If detected early and treated adequately, cervical cancer can be virtually prevented. Cervical precursor lesions and invasive cancer exhibit certain morphologic features that can be identified during a visual inspection exam. Digital <span class="hlt">imaging</span> technologies allow us to assist the physician with a Computer-Aided Diagnosis (CAD) system. In colposcopy, epithelium that turns white after application of acetic acid is called acetowhite epithelium. Acetowhite epithelium is one of the major diagnostic features observed in detecting cancer and pre-cancerous regions. Automatic extraction of acetowhite regions from cervical <span class="hlt">images</span> has been a challenging task due to specular reflection, various illumination conditions, and most importantly, large intra-patient variation. This paper presents a multi-step acetowhite region detection system to analyze the acetowhite lesions in cervical <span class="hlt">images</span> automatically. First, the system calibrates the color of the cervical <span class="hlt">images</span> to be independent of screening devices. Second, the anatomy of the uterine cervix is analyzed in terms of cervix region, external os region, columnar region, and squamous region. Third, the squamous region is further analyzed and subregions based on three levels of acetowhite are identified. The extracted acetowhite regions are accompanied by color scores to indicate the different levels of acetowhite. The system has been evaluated by 40 human subjects' data and demonstrates high correlation with experts' annotations.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=media+AND+body+AND+image&pg=5&id=EJ587554','ERIC'); return false;" href="https://eric.ed.gov/?q=media+AND+body+AND+image&pg=5&id=EJ587554"><span>Television <span class="hlt">Images</span> and Adolescent Girls' Body <span class="hlt">Image</span> Disturbance.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Botta, Renee A.</p>
         <p>1999-01-01</p>
         <p>Contributes to scholarship on the effects of media <span class="hlt">images</span> on adolescents, using social-comparison theory and critical-viewing theory. Finds that media do have an impact on body-<span class="hlt">image</span> disturbance. Suggests that body-<span class="hlt">image</span> processing is the key to understanding how television <span class="hlt">images</span> affect adolescent girls' body-<span class="hlt">image</span> attitudes and behaviors. (SR)</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/971077','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/971077"><span>Edge-based correlation <span class="hlt">image</span> registration for multispectral <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Nandy, Prabal [Albuquerque, NM</p>
         <p>2009-11-17</p>
         <p>Registration information for <span class="hlt">images</span> of a common target obtained from a plurality of different spectral bands can be obtained by combining edge detection and phase correlation. The <span class="hlt">images</span> are edge-filtered, and pairs of the edge-filtered <span class="hlt">images</span> are then phase correlated to produce phase correlation <span class="hlt">images</span>. The registration information can be determined based on these phase correlation <span class="hlt">images</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA613544','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA613544"><span>Tinnitus Multimodal <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2014-10-01</p>
         <p>1 AWARD NUMBER: W81XWH-13-1-0494 TITLE: Tinnitus Multimodal <span class="hlt">Imaging</span> PRINCIPAL INVESTIGATOR...TYPE Annual 3. DATES COVERED 30 Sept 2013 – 29 Oct 2014 4. TITLE AND SUBTITLE 5a. CONTRACT NUMBER Tinnitus Multimodal <span class="hlt">Imaging</span>...AVAILABILITY STATEMENT Approved for Public Release; Distribution Unlimited 13. SUPPLEMENTARY NOTES 14. ABSTRACT Tinnitus is a common auditory</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=polyester&id=EJ369324','ERIC'); return false;" href="https://eric.ed.gov/?q=polyester&id=EJ369324"><span>Overcoming the Polyester <span class="hlt">Image</span>.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Regan, Dorothy</p>
         <p>1988-01-01</p>
         <p>Urges community colleges to overcome their <span class="hlt">image</span> problem by documenting the colleges' impact on their communities. Suggests ways to determine what data should be collected, how to collect the information, and how it can be used to empower faculty, staff, and alumni to change the institution's <span class="hlt">image</span>. (DMM)</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/27239941','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/27239941"><span>Nanophotonic <span class="hlt">Image</span> Sensors.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Chen, Qin; Hu, Xin; Wen, Long; Yu, Yan; Cumming, David R S</p>
         <p>2016-09-01</p>
         <p>The increasing miniaturization and resolution of <span class="hlt">image</span> sensors bring challenges to conventional optical elements such as spectral filters and polarizers, the properties of which are determined mainly by the materials used, including dye polymers. Recent developments in spectral filtering and optical manipulating techniques based on nanophotonics have opened up the possibility of an alternative method to control light spectrally and spatially. By integrating these technologies into <span class="hlt">image</span> sensors, it will become possible to achieve high compactness, improved process compatibility, robust stability and tunable functionality. In this Review, recent representative achievements on nanophotonic <span class="hlt">image</span> sensors are presented and analyzed including <span class="hlt">image</span> sensors with nanophotonic color filters and polarizers, metamaterial-based THz <span class="hlt">image</span> sensors, filter-free nanowire <span class="hlt">image</span> sensors and nanostructured-based multispectral <span class="hlt">image</span> sensors. This novel combination of cutting edge photonics research and well-developed commercial products may not only lead to an important application of nanophotonics but also offer great potential for next generation <span class="hlt">image</span> sensors beyond Moore's Law expectations. © 2016 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/23147361','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/23147361"><span><span class="hlt">Imaging</span> the pleura.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Mortensen, Chloe; Bhatnagar, Rahul; Edey, Anthony J</p>
         <p>2012-11-01</p>
         <p>Pleural disease is now recognized as an important subspecialty of pulmonary medicine, with increasing provision being made for specialist services and procedures. In response, the field of pleural <span class="hlt">imaging</span> has advanced in recent years, especially with regard to ultrasound. Salient multimodality <span class="hlt">imaging</span> techniques are discussed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/17679335','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/17679335"><span>Vibration mode <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Zhang, Xiaoming; Zeraati, Mohammad; Kinnick, Randall R; Greenleaf, James F; Fatemi, Mostafa</p>
         <p>2007-06-01</p>
         <p>A new method for <span class="hlt">imaging</span> the vibration mode of an object is investigated. The radiation force of ultrasound is used to scan the object at a resonant frequency of the object. The vibration of the object is measured by laser and the resulting acoustic emission from the object is measured by a hydrophone. It is shown that the measured signal is proportional to the value of the mode shape at the focal point of the ultrasound beam. Experimental studies are carried out on a mechanical heart valve and arterial phantoms. The mode <span class="hlt">images</span> on the valve are made by the hydrophone measurement and confirmed by finite-element method simulations. Compared with conventional B-scan <span class="hlt">imaging</span> on arterial phantoms, the mode <span class="hlt">imaging</span> can show not only the interface of the artery and the gelatin, but also the vibration modes of the artery. The <span class="hlt">images</span> taken on the phantom surface suggest that an <span class="hlt">image</span> of an interior artery can be made by vibration measurements on the surface of the body. However, the <span class="hlt">image</span> of the artery can be improved if the vibration of the artery is measured directly. <span class="hlt">Imaging</span> of the structure in the gelatin or tissue can be enhanced by small bubbles and contrast agents.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19920023961','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19920023961"><span><span class="hlt">Image</span> processing mini manual</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Matthews, Christine G.; Posenau, Mary-Anne; Leonard, Desiree M.; Avis, Elizabeth L.; Debure, Kelly R.; Stacy, Kathryn; Vonofenheim, Bill</p>
         <p>1992-01-01</p>
         <p>The intent is to provide an introduction to the <span class="hlt">image</span> processing capabilities available at the Langley Research Center (LaRC) Central Scientific Computing Complex (CSCC). Various <span class="hlt">image</span> processing software components are described. Information is given concerning the use of these components in the Data Visualization and Animation Laboratory at LaRC.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/18796564','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/18796564"><span><span class="hlt">Imaging</span> in podiatry.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Church, Elizabeth J</p>
         <p>2008-09-01</p>
         <p>This article examines the vulnerability of the foot to injury and disease and the role <span class="hlt">imaging</span> plays in ferreting out the causes of pain and dysfunction. The discussion includes a broad overview of foot disorders and describes the expanding role played by <span class="hlt">imaging</span> in the diagnosis and management of food disorders.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA04625.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA04625.html"><span>Groth Deep <span class="hlt">Image</span></span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2003-07-25</p>
         <p>This ultraviolet color blowup of the Groth Deep <span class="hlt">Image</span> was taken by NASA Galaxy Evolution Explorer on June 22 and June 23, 2003. Many hundreds of galaxies are detected in this portion of the <span class="hlt">image</span>. NASA astronomers believe the faint red galaxies are 6 billion light years away. http://photojournal.jpl.nasa.gov/catalog/PIA04625</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_8");'>8</a></li>
      <li><a href="#" onclick='return showDiv("page_9");'>9</a></li>
      <li class="active"><span>10</span></li>
   <li><a href="#" onclick='return showDiv("page_11");'>11</a></li>
      <li><a href="#" onclick='return showDiv("page_12");'>12</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_10 -->
   <div id="page_11" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_9");'>9</a></li>
      <li><a href="#" onclick='return showDiv("page_10");'>10</a></li>
      <li class="active"><span>11</span></li>
   <li><a href="#" onclick='return showDiv("page_12");'>12</a></li>
      <li><a href="#" onclick='return showDiv("page_13");'>13</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="201">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=optic+AND+lenses&pg=2&id=EJ943408','ERIC'); return false;" href="https://eric.ed.gov/?q=optic+AND+lenses&pg=2&id=EJ943408"><span><span class="hlt">Images</span> of Axial Objects</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Rabal, Hector; Cap, Nelly; Trivi, Marcelo</p>
         <p>2011-01-01</p>
         <p><span class="hlt">Imaging</span> of three-dimensional objects by lenses and mirrors is sometimes poorly indicated in textbooks and can be incorrectly drawn. We stress a need to clarify the concept of longitudinal magnification, with simulated <span class="hlt">images</span> illustrating distortions introduced along the optical axis. We consider all possible positions of the object for both a…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19790000442&hterms=special+library&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D10%26Ntt%3Dspecial%2Blibrary','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19790000442&hterms=special+library&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D10%26Ntt%3Dspecial%2Blibrary"><span><span class="hlt">Image</span>-analysis library</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1980-01-01</p>
         <p>MATHPAC <span class="hlt">image</span>-analysis library is collection of general-purpose mathematical and statistical routines and special-purpose data-analysis and pattern-recognition routines for <span class="hlt">image</span> analysis. MATHPAC library consists of Linear Algebra, Optimization, Statistical-Summary, Densities and Distribution, Regression, and Statistical-Test packages.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/25482523','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/25482523"><span>Live-cell <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Cole, Richard</p>
         <p>2014-01-01</p>
         <p>It would be hard to argue that live-cell <span class="hlt">imaging</span> has not changed our view of biology. The past 10 years have seen an explosion of interest in <span class="hlt">imaging</span> cellular processes, down to the molecular level. There are now many advanced techniques being applied to live cell <span class="hlt">imaging</span>. However, cellular health is often under appreciated. For many researchers, if the cell at the end of the experiment has not gone into apoptosis or is blebbed beyond recognition, than all is well. This is simply incorrect. There are many factors that need to be considered when performing live-cell <span class="hlt">imaging</span> in order to maintain cellular health such as: <span class="hlt">imaging</span> modality, media, temperature, humidity, PH, osmolality, and photon dose. The wavelength of illuminating light, and the total photon dose that the cells are exposed to, comprise two of the most important and controllable parameters of live-cell <span class="hlt">imaging</span>. The lowest photon dose that achieves a measureable metric for the experimental question should be used, not the dose that produces cover photo quality <span class="hlt">images</span>. This is paramount to ensure that the cellular processes being investigated are in their in vitro state and not shifted to an alternate pathway due to environmental stress. The timing of the mitosis is an ideal canary in the gold mine, in that any stress induced from the <span class="hlt">imaging</span> will result in the increased length of mitosis, thus providing a control model for the current imagining conditions.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4594455','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4594455"><span>Live-cell <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Cole, Richard</p>
         <p>2014-01-01</p>
         <p>It would be hard to argue that live-cell <span class="hlt">imaging</span> has not changed our view of biology. The past 10 years have seen an explosion of interest in <span class="hlt">imaging</span> cellular processes, down to the molecular level. There are now many advanced techniques being applied to live cell <span class="hlt">imaging</span>. However, cellular health is often under appreciated. For many researchers, if the cell at the end of the experiment has not gone into apoptosis or is blebbed beyond recognition, than all is well. This is simply incorrect. There are many factors that need to be considered when performing live-cell <span class="hlt">imaging</span> in order to maintain cellular health such as: <span class="hlt">imaging</span> modality, media, temperature, humidity, PH, osmolality, and photon dose. The wavelength of illuminating light, and the total photon dose that the cells are exposed to, comprise two of the most important and controllable parameters of live-cell <span class="hlt">imaging</span>. The lowest photon dose that achieves a measureable metric for the experimental question should be used, not the dose that produces cover photo quality <span class="hlt">images</span>. This is paramount to ensure that the cellular processes being investigated are in their in vitro state and not shifted to an alternate pathway due to environmental stress. The timing of the mitosis is an ideal canary in the gold mine, in that any stress induced from the <span class="hlt">imaging</span> will result in the increased length of mitosis, thus providing a control model for the current imagining conditions. PMID:25482523</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=quantum+AND+physics&pg=7&id=EJ675363','ERIC'); return false;" href="https://eric.ed.gov/?q=quantum+AND+physics&pg=7&id=EJ675363"><span><span class="hlt">Images</span> of Atoms.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Wright, Tony</p>
         <p>2003-01-01</p>
         <p>Recommends using a simple <span class="hlt">image</span>, such as the fuzzy atom ball to help students develop a useful understanding of the molecular world. Explains that the <span class="hlt">image</span> helps students easily grasp ideas about atoms and molecules and leads naturally to more advanced ideas of atomic structure, chemical bonding, and quantum physics. (Author/NB)</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19910.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19910.html"><span>Dawn HAMO <span class="hlt">Image</span> 32</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-07</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span> was taken on September 9, 2015, and has a resolution of 450 feet (140 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19910</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19894.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19894.html"><span>Dawn HAMO <span class="hlt">Image</span> 16</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-15</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span>, with a resolution of 450 feet (140 meters) per pixel, was taken on August 24, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19894</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19986.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19986.html"><span>Dawn HAMO <span class="hlt">Image</span> 44</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-23</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the northern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on Sept. 21, 2015, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19986</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19897.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19897.html"><span>Dawn HAMO <span class="hlt">Image</span> 19</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-18</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span>, with a resolution of 450 feet (140 meters) per pixel, was taken on August 26, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19897</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19886.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19886.html"><span>Dawn HAMO <span class="hlt">Image</span> 10</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-04</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the southern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span>, with a resolution of 450 feet 140 meters per pixel, was taken on August 21, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19886</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19909.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19909.html"><span>Dawn HAMO <span class="hlt">Image</span> 31</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-06</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span> was taken on September 9, 2015, and has a resolution of 450 feet (140 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19909</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19994.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19994.html"><span>Dawn HAMO <span class="hlt">Image</span> 52</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-11-04</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the surface of dwarf planet Ceres at mid-latitudes from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on Sept. 29, 2015, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19994</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19971.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19971.html"><span>Dawn HAMO <span class="hlt">Image</span> 33</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-08</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on September 14, 2015, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19971</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19884.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19884.html"><span>Dawn HAMO <span class="hlt">Image</span> 8</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-02</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the northern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span>, with a resolution of 450 feet 140 meters per pixel, was taken on August 21, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19884</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19888.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19888.html"><span>Dawn HAMO <span class="hlt">Image</span> 12</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-10</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the southern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span>, with a resolution of 450 feet 140 meters per pixel, was taken on August 21, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19888</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19908.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19908.html"><span>Dawn HAMO <span class="hlt">Image</span> 30</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-05</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span> was taken on September 9, 2015, and has a resolution of 450 feet (140 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19908</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19990.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19990.html"><span>Dawn HAMO <span class="hlt">Image</span> 48</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-29</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the northern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on Sept. 22, 2015, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19990</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19895.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19895.html"><span>Dawn HAMO <span class="hlt">Image</span> 17</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-16</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span>, with a resolution of 450 feet (140 meters) per pixel, was taken on August 25, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19895</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19882.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19882.html"><span>Dawn HAMO <span class="hlt">Image</span> 6</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-08-31</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the southern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span>, with a resolution of 450 feet 140 meters per pixel, was taken on August 21, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19882</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19972.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19972.html"><span>Dawn HAMO <span class="hlt">Image</span> 34</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-09</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on September 15, 2015, and has a resolution of 450 feet 140 meters per pixel.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_9");'>9</a></li>
      <li><a href="#" onclick='return showDiv("page_10");'>10</a></li>
      <li class="active"><span>11</span></li>
   <li><a href="#" onclick='return showDiv("page_12");'>12</a></li>
      <li><a href="#" onclick='return showDiv("page_13");'>13</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_11 -->
   <div id="page_12" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_10");'>10</a></li>
      <li><a href="#" onclick='return showDiv("page_11");'>11</a></li>
      <li class="active"><span>12</span></li>
   <li><a href="#" onclick='return showDiv("page_13");'>13</a></li>
      <li><a href="#" onclick='return showDiv("page_14");'>14</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="221">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19978.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19978.html"><span>Dawn HAMO <span class="hlt">Image</span> 36</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-13</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on September 20, 2015, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19978</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19893.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19893.html"><span>Dawn HAMO <span class="hlt">Image</span> 15</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-14</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span>, with a resolution of 450 feet (140 meters) per pixel, was taken on August 24, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19893</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19883.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19883.html"><span>Dawn HAMO <span class="hlt">Image</span> 7</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-01</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the southern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span>, with a resolution of 450 feet 140 meters per pixel, was taken on August 21, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19883</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19973.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19973.html"><span>Dawn HAMO <span class="hlt">Image</span> 35</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-12</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on August 23, 2015, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19972</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19987.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19987.html"><span>Dawn HAMO <span class="hlt">Image</span> 45</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-26</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the surface of dwarf planet Ceres at mid-latitudes from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on Sept. 21, 2015, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19987</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19905.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19905.html"><span>Dawn HAMO <span class="hlt">Image</span> 27</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-30</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on August 22, 2015, and has a resolution of 450 feet 140 meters per pixel.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19907.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19907.html"><span>Dawn HAMO <span class="hlt">Image</span> 29</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-02</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span> was taken on September 9, 2015, and has a resolution of 450 feet (140 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19907</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19885.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19885.html"><span>Dawn HAMO <span class="hlt">Image</span> 9</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-03</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of Ceres at mid-latitudes from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span>, with a resolution of 450 feet 140 meters per pixel, was taken on August 21, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19885</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19899.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19899.html"><span>Dawn HAMO <span class="hlt">Image</span> 21</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-22</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span>, with a resolution of 450 feet (140 meters) per pixel, was taken on August 27, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19899</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19892.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19892.html"><span>Dawn HAMO <span class="hlt">Image</span> 14</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-11</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span>, with a resolution of 450 feet (140 meters) per pixel, was taken on August 24, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19892</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19982.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19982.html"><span>Dawn HAMO <span class="hlt">Image</span> 40</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-19</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the southern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on Sept. 20, 2015, and has a resolution of 450 feet 140 meters per pixel.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19906.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19906.html"><span>Dawn HAMO <span class="hlt">Image</span> 28</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-01</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span> was taken on August 24, 2015, and has a resolution of 450 feet (140 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19906</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19896.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19896.html"><span>Dawn HAMO <span class="hlt">Image</span> 18</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-17</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span>, with a resolution of 450 feet (140 meters) per pixel, was taken on August 26, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19896</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19984.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19984.html"><span>Dawn HAMO <span class="hlt">Image</span> 42</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-21</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the surface of dwarf planet Ceres at mid-latitudes, from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on Sept. 21, 2015, and has a resolution of 450 feet 140 meters per pixel.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19881.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19881.html"><span>Dawn HAMO <span class="hlt">Image</span> 5</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-08-28</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the northern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span>, with a resolution of 450 feet 140 meters per pixel, was taken on August 21, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19881</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19635.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19635.html"><span>Dawn HAMO <span class="hlt">Image</span> 4</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-08-27</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows a portion of the northern hemisphere of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span>, with a resolution of 450 feet (140 meters) per pixel, was taken on August 21, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19635</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19985.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19985.html"><span>Dawn HAMO <span class="hlt">Image</span> 43</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-22</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the southern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on Sept. 21, 2015, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19985</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19980.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19980.html"><span>Dawn HAMO <span class="hlt">Image</span> 38</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-15</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the southern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on Sept. 20, 2015, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19980</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19903.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19903.html"><span>Dawn HAMO <span class="hlt">Image</span> 25</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-28</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span> was taken on August 21, 2015, and has a resolution of 450 feet (140 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19903</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19904.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19904.html"><span>Dawn HAMO <span class="hlt">Image</span> 26</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-29</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span> was taken on August 21, 2015, and has a resolution of 450 feet (140 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19904</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_10");'>10</a></li>
      <li><a href="#" onclick='return showDiv("page_11");'>11</a></li>
      <li class="active"><span>12</span></li>
   <li><a href="#" onclick='return showDiv("page_13");'>13</a></li>
      <li><a href="#" onclick='return showDiv("page_14");'>14</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_12 -->
   <div id="page_13" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_11");'>11</a></li>
      <li><a href="#" onclick='return showDiv("page_12");'>12</a></li>
      <li class="active"><span>13</span></li>
   <li><a href="#" onclick='return showDiv("page_14");'>14</a></li>
      <li><a href="#" onclick='return showDiv("page_15");'>15</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="241">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19983.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19983.html"><span>Dawn HAMO <span class="hlt">Image</span> 41</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-20</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the northern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on Sept. 21, 2015, and has a resolution of 450 feet 140 meters per pixel.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19901.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19901.html"><span>Dawn HAMO <span class="hlt">Image</span> 23</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-24</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span>, with a resolution of 450 feet 140 meters per pixel, was taken on August 27, 2015.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19902.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19902.html"><span>Dawn HAMO <span class="hlt">Image</span> 24</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-25</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span> was taken on August 21, 2015, and has a resolution of 450 feet (140 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19902</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19887.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19887.html"><span>Dawn HAMO <span class="hlt">Image</span> 11</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-08</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the southern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span>, with a resolution of 450 feet 140 meters per pixel, was taken on August 21, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19887</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19900.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19900.html"><span>Dawn HAMO <span class="hlt">Image</span> 22</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-23</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span>, with a resolution of 450 feet (140 meters) per pixel, was taken on August 27, 2015. http://photojournal.jpl.nasa.gov/catalog/PIA19900</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19991.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19991.html"><span>Dawn HAMO <span class="hlt">Image</span> 49</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-30</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a portion of the northern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span> was taken on Sept. 22, 2015, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19991</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19898.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19898.html"><span>Dawn HAMO <span class="hlt">Image</span> 20</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-09-21</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers. The <span class="hlt">image</span>, with a resolution of 450 feet 140 meters per pixel, was taken on August 26, 2015.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1044039','SCIGOV-STC'); return false;" href="https://www.osti.gov/servlets/purl/1044039"><span>LWIR Snapshot <span class="hlt">Imaging</span> Polarimeter</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Dr. Robert E Sampson</p>
         <p></p>
         <p>This report describes the results of a phase 1 STTR to design a longwave infrared <span class="hlt">imaging</span> polarimeter. The system design, expected performance and components needed to construct the <span class="hlt">imaging</span> polarimeter are described. Expected performance is modeled and sytem specifications are presented.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=images+AND+mars&pg=2&id=ED409007','ERIC'); return false;" href="https://eric.ed.gov/?q=images+AND+mars&pg=2&id=ED409007"><span>Digital <span class="hlt">Image</span> Access & Retrieval.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Heidorn, P. Bryan, Ed.; Sandore, Beth, Ed.</p>
         <p></p>
         <p>Recent technological advances in computing and digital <span class="hlt">imaging</span> technology have had immediate and permanent consequences for visual resource collections. Libraries are involved in organizing and managing large visual resource collections. The central challenges in working with digital <span class="hlt">image</span> collections mirror those that libraries have sought to…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20100024506','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20100024506"><span>Polarization <span class="hlt">imaging</span> apparatus</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Zou, Yingyin Kevin (Inventor); Chen, Qiushui (Inventor); Zhao, Hongzhi (Inventor)</p>
         <p>2010-01-01</p>
         <p>A polarization <span class="hlt">imaging</span> apparatus measures the Stokes <span class="hlt">image</span> of a sample. The apparatus consists of an optical lens set 11, a linear polarizer 14 with its optical axis 18, a first variable phase retarder 12 with its optical axis 16 aligned 22.5.degree. to axis 18, a second variable phase retarder 13 with its optical axis 17 aligned 45.degree. to axis 18, a <span class="hlt">imaging</span> sensor 15 for sensing the intensity <span class="hlt">images</span> of the sample, a controller 101 and a computer 102. Two variable phase retarders 12 and 13 were controlled independently by a computer 102 through a controller unit 101 which generates a sequential of voltages to control the phase retardations of VPRs 12 and 13. A set of four intensity <span class="hlt">images</span>, I.sub.0, I.sub.1, I.sub.2 and I.sub.3 of the sample were captured by <span class="hlt">imaging</span> sensor 15 when the phase retardations of VPRs 12 and 13 were set at (0,0), (.pi.,0), (.pi.,.pi.) and (.pi./2,.pi.), respectively Then four Stokes components of a Stokes <span class="hlt">image</span>, S.sub.0, S.sub.1, S.sub.2 and S.sub.3 were calculated using the four intensity <span class="hlt">images</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/10283693','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/10283693"><span>Marketing mobile <span class="hlt">imaging</span> services.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>McCue, P</p>
         <p>1987-09-01</p>
         <p>Competition in the mobile <span class="hlt">imaging</span> arena has put radiologists, radiology directors, and other health care professionals in the unfamiliar position of being marketing agents for their services. Mobile <span class="hlt">imaging</span> is being promoted through consumer advertising as well as through the traditional route of physician referral. This article offers some of the marketing lessons being learned in the mobile arena.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=positioning+AND+marketing&pg=6&id=EJ437954','ERIC'); return false;" href="https://eric.ed.gov/?q=positioning+AND+marketing&pg=6&id=EJ437954"><span>Managing Institutional <span class="hlt">Image</span>.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Melchiori, Gerlinda S.</p>
         <p>1990-01-01</p>
         <p>A managerial process for enhancing the <span class="hlt">image</span> and public reputation of a higher education institution is outlined. It consists of five stages: market research; data analysis and market positioning; communication of results and recommendations to the administration; development of a global <span class="hlt">image</span> program; and impact evaluation. (MSE)</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA00229.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA00229.html"><span>Gaspra Optical Navigation <span class="hlt">Image</span></span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>1996-02-08</p>
         <p>This time-exposure picture of the asteroid Gaspra and background stars is one of four optical navigation <span class="hlt">images</span> made by NASA Galileo <span class="hlt">imaging</span> system to improve knowledge of Gaspra location for the spacecraft flyby. http://photojournal.jpl.nasa.gov/catalog/PIA00229</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20950.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20950.html"><span>Dawn LAMO <span class="hlt">Image</span> 188</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-10-07</p>
         <p>NASA's Dawn spacecraft views Oxo Crater (6 miles, 10 kilometers wide) in this view from Ceres. Dawn took this <span class="hlt">image</span> on June 4, 2016, from its low-altitude mapping orbit, at a distance of about 240 miles (385 kilometers) above the surface. The <span class="hlt">image</span> resolution is 120 feet (35 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20950</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/973426-nanoparticles-biomedical-imaging','SCIGOV-STC'); return false;" href="https://www.osti.gov/biblio/973426-nanoparticles-biomedical-imaging"><span>Nanoparticles for Biomedical <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Nune, Satish K.; Gunda, Padmaja; Thallapally, Praveen K.</p>
         <p>2009-11-01</p>
         <p>Background: Synthetic nanoparticles are emerging as versatile tools in biomedical applications, particularly in the area of biomedical <span class="hlt">imaging</span>. Nanoparticles 1 to 100 nm in diameter possess dimensions comparable to biological functional units. Diverse surface chemistries, unique magnetic properties, tunable absorption and emission properties, and recent advances in the synthesis and engineering of various nanoparticles suggest their potential as probes for early detection of diseases such as cancer. Surface functionalization has further expanded the potential of nanoparticles as probes for molecular <span class="hlt">imaging</span>. Objective: To summarize emerging research of nanoparticles for biomedical <span class="hlt">imaging</span> with increased selectivity and reduced non-specific uptake with increasedmore » spatial resolution containing stabilizers conjugated with targeting ligands. Methods: This review summarizes recent technological advances in the synthesis of various nanoparticle probes, and surveys methods to improve the targeting of nanoparticles for their applications in biomedical <span class="hlt">imaging</span>. Conclusion: Structural design of nanomaterials for biomedical <span class="hlt">imaging</span> continues to expand and diversify. Synthetic methods have aimed to control the size and surface characteristics of nanoparticles to control distribution, half-life and elimination. Although molecular <span class="hlt">imaging</span> applications using nanoparticles are advancing into clinical applications, challenges such as storage stability and long-term toxicology should continue to be addressed. Keywords: nanoparticle synthesis, surface modification, targeting, molecular <span class="hlt">imaging</span>, and biomedical <span class="hlt">imaging</span>.« less</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/29346091','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/29346091"><span>Plenoptic <span class="hlt">Image</span> Motion Deblurring.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Chandramouli, Paramanand; Jin, Meiguang; Perrone, Daniele; Favaro, Paolo</p>
         <p>2018-04-01</p>
         <p>We propose a method to remove motion blur in a single light field captured with a moving plenoptic camera. Since motion is unknown, we resort to a blind deconvolution formulation, where one aims to identify both the blur point spread function and the latent sharp <span class="hlt">image</span>. Even in the absence of motion, light field <span class="hlt">images</span> captured by a plenoptic camera are affected by a non-trivial combination of both aliasing and defocus, which depends on the 3D geometry of the scene. Therefore, motion deblurring algorithms designed for standard cameras are not directly applicable. Moreover, many state of the art blind deconvolution algorithms are based on iterative schemes, where blurry <span class="hlt">images</span> are synthesized through the <span class="hlt">imaging</span> model. However, current <span class="hlt">imaging</span> models for plenoptic <span class="hlt">images</span> are impractical due to their high dimensionality. We observe that plenoptic cameras introduce periodic patterns that can be exploited to obtain highly parallelizable numerical schemes to synthesize <span class="hlt">images</span>. These schemes allow extremely efficient GPU implementations that enable the use of iterative methods. We can then cast blind deconvolution of a blurry light field <span class="hlt">image</span> as a regularized energy minimization to recover a sharp high-resolution scene texture and the camera motion. Furthermore, the proposed formulation can handle non-uniform motion blur due to camera shake as demonstrated on both synthetic and real light field data.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1067329','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1067329"><span>Medical <span class="hlt">imaging</span> systems</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Frangioni, John V [Wayland, MA</p>
         <p>2012-07-24</p>
         <p>A medical <span class="hlt">imaging</span> system provides simultaneous rendering of visible light and fluorescent <span class="hlt">images</span>. The system may employ dyes in a small-molecule form that remains in a subject's blood stream for several minutes, allowing real-time <span class="hlt">imaging</span> of the subject's circulatory system superimposed upon a conventional, visible light <span class="hlt">image</span> of the subject. The system may also employ dyes or other fluorescent substances associated with antibodies, antibody fragments, or ligands that accumulate within a region of diagnostic significance. In one embodiment, the system provides an excitation light source to excite the fluorescent substance and a visible light source for general illumination within the same optical guide that is used to capture <span class="hlt">images</span>. In another embodiment, the system is configured for use in open surgical procedures by providing an operating area that is closed to ambient light. More broadly, the systems described herein may be used in <span class="hlt">imaging</span> applications where a visible light <span class="hlt">image</span> may be usefully supplemented by an <span class="hlt">image</span> formed from fluorescent emissions from a fluorescent substance that marks areas of functional interest.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5818880','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5818880"><span>Nanophotonic <span class="hlt">Image</span> Sensors</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Hu, Xin; Wen, Long; Yu, Yan; Cumming, David R. S.</p>
         <p>2016-01-01</p>
         <p>The increasing miniaturization and resolution of <span class="hlt">image</span> sensors bring challenges to conventional optical elements such as spectral filters and polarizers, the properties of which are determined mainly by the materials used, including dye polymers. Recent developments in spectral filtering and optical manipulating techniques based on nanophotonics have opened up the possibility of an alternative method to control light spectrally and spatially. By integrating these technologies into <span class="hlt">image</span> sensors, it will become possible to achieve high compactness, improved process compatibility, robust stability and tunable functionality. In this Review, recent representative achievements on nanophotonic <span class="hlt">image</span> sensors are presented and analyzed including <span class="hlt">image</span> sensors with nanophotonic color filters and polarizers, metamaterial‐based THz <span class="hlt">image</span> sensors, filter‐free nanowire <span class="hlt">image</span> sensors and nanostructured‐based multispectral <span class="hlt">image</span> sensors. This novel combination of cutting edge photonics research and well‐developed commercial products may not only lead to an important application of nanophotonics but also offer great potential for next generation <span class="hlt">image</span> sensors beyond Moore's Law expectations. PMID:27239941</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2013SPIE.8589E..10K','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2013SPIE.8589E..10K"><span>Photothermal <span class="hlt">imaging</span> of melanin</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Kerimo, Josef; DiMarzio, Charles A.</p>
         <p>2013-02-01</p>
         <p>We present photothermal <span class="hlt">images</span> of melanin using modulation with two laser beams. Strong melanin absorption followed by efficient nonradiative relaxation caused heating and an increase in temperature. This temperature effect was used as an <span class="hlt">imaging</span> contrast to detect melanin. Melanin from several samples including Sepia officinalis, black human hair, and live zebra fish, were <span class="hlt">imaged</span> with a high signal-to-noise ratio. For the <span class="hlt">imaging</span>, we focused two near infrared laser beams (pump and probe) collinearly with different wavelengths and the pump was modulated in amplitude. The thermally induced variations in the refractive index, at the modulation frequency, were detected by the scattering of the probe beam. The Photothermal method brings several <span class="hlt">imaging</span> benefits including the lack of background interference and the possibility of <span class="hlt">imaging</span> for an extended period of time without photodamage to the melanin. The dependence of the photothermal signal on the laser power, modulation frequency, and spatial offset of the probe is discussed. The new photothermal <span class="hlt">imaging</span> method is promising and provides background-free and label-free <span class="hlt">imaging</span> of melanin and can be implemented with low-cost CW lasers.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/869413','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/869413"><span>Fluorescent <span class="hlt">image</span> tracking velocimeter</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Shaffer, Franklin D.</p>
         <p>1994-01-01</p>
         <p>A multiple-exposure fluorescent <span class="hlt">image</span> tracking velocimeter (FITV) detects and measures the motion (trajectory, direction and velocity) of small particles close to light scattering surfaces. The small particles may follow the motion of a carrier medium such as a liquid, gas or multi-phase mixture, allowing the motion of the carrier medium to be observed, measured and recorded. The main components of the FITV include: (1) fluorescent particles; (2) a pulsed fluorescent excitation laser source; (3) an <span class="hlt">imaging</span> camera; and (4) an <span class="hlt">image</span> analyzer. FITV uses fluorescing particles excited by visible laser light to enhance particle <span class="hlt">image</span> detectability near light scattering surfaces. The excitation laser light is filtered out before reaching the <span class="hlt">imaging</span> camera allowing the fluoresced wavelengths emitted by the particles to be detected and recorded by the camera. FITV employs multiple exposures of a single camera <span class="hlt">image</span> by pulsing the excitation laser light for producing a series of <span class="hlt">images</span> of each particle along its trajectory. The time-lapsed <span class="hlt">image</span> may be used to determine trajectory and velocity and the exposures may be coded to derive directional information.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_11");'>11</a></li>
      <li><a href="#" onclick='return showDiv("page_12");'>12</a></li>
      <li class="active"><span>13</span></li>
   <li><a href="#" onclick='return showDiv("page_14");'>14</a></li>
      <li><a href="#" onclick='return showDiv("page_15");'>15</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_13 -->
   <div id="page_14" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_12");'>12</a></li>
      <li><a href="#" onclick='return showDiv("page_13");'>13</a></li>
      <li class="active"><span>14</span></li>
   <li><a href="#" onclick='return showDiv("page_15");'>15</a></li>
      <li><a href="#" onclick='return showDiv("page_16");'>16</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="261">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=fdm&id=EJ423360','ERIC'); return false;" href="https://eric.ed.gov/?q=fdm&id=EJ423360"><span>The Power of <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Haapaniemi, Peter</p>
         <p>1990-01-01</p>
         <p>Describes <span class="hlt">imaging</span> technology, which allows huge numbers of words and illustrations to be reduced to tiny fraction of space required by originals and discusses current applications. Highlights include <span class="hlt">image</span> processing system at National Archives; use by banks for high-speed check processing; engineering document management systems (EDMS); folder…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/867933','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/867933"><span>Heart <span class="hlt">imaging</span> method</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Collins, H. Dale; Gribble, R. Parks; Busse, Lawrence J.</p>
         <p>1991-01-01</p>
         <p>A method for providing an <span class="hlt">image</span> of the human heart's electrical system derives time-of-flight data from an array of EKG electrodes and this data is transformed into phase information. The phase information, treated as a hologram, is reconstructed to provide an <span class="hlt">image</span> in one or two dimensions of the electrical system of the functioning heart.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20145.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20145.html"><span>Dawn HAMO <span class="hlt">Image</span> 82</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-12-22</p>
         <p>Part of the southern hemisphere on dwarf planet Ceres is seen in this <span class="hlt">image</span> taken by NASA Dawn spacecraft. Hamori crater, named after a Japanese god and protector of tree leaves, is the large crater near the center of the <span class="hlt">image</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA00128.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA00128.html"><span>Moon - 18 <span class="hlt">Image</span> Mosaic</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>1996-02-05</p>
         <p>This mosaic picture of the Moon was compiled from 18 <span class="hlt">images</span> taken with a green filter NASA's Galileo <span class="hlt">imaging</span> system during the spacecraft flyby on December 7, 1992, some 11 hours before its Earth flyby. http://photojournal.jpl.nasa.gov/catalog/PIA00128</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=molecular+AND+genetics&pg=5&id=EJ1008965','ERIC'); return false;" href="https://eric.ed.gov/?q=molecular+AND+genetics&pg=5&id=EJ1008965"><span>Functional Magnetic Resonance <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Voos, Avery; Pelphrey, Kevin</p>
         <p>2013-01-01</p>
         <p>Functional magnetic resonance <span class="hlt">imaging</span> (fMRI), with its excellent spatial resolution and ability to visualize networks of neuroanatomical structures involved in complex information processing, has become the dominant technique for the study of brain function and its development. The accessibility of in-vivo pediatric brain-<span class="hlt">imaging</span> techniques…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=emotional+AND+marketing&pg=2&id=EJ822864','ERIC'); return false;" href="https://eric.ed.gov/?q=emotional+AND+marketing&pg=2&id=EJ822864"><span><span class="hlt">Image</span> and Prestige Planning</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Ager, Dennis</p>
         <p>2005-01-01</p>
         <p>The aim of this paper is to clarify some notions about <span class="hlt">image</span> and prestige planning. Starting from the Welsh example of language policy aiming to revitalise a language in danger of further decreasing in number of speakers and in centrality to Welsh life, definitions of four related terms are explored: <span class="hlt">image</span>, status, prestige and identity. Paired…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20670.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20670.html"><span>Dawn LAMO <span class="hlt">Image</span> 90</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-05-17</p>
         <p>This <span class="hlt">image</span> from NASA Dawn spacecraft shows the western rim of Azacca Crater on Ceres. A smaller impact feature sits on its flank. Of particular interest in this scene is the great number of small, bright spots, in the southern part of the <span class="hlt">image</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19940032303','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19940032303"><span>Studies on <span class="hlt">image</span> compression and <span class="hlt">image</span> reconstruction</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Sayood, Khalid; Nori, Sekhar; Araj, A.</p>
         <p>1994-01-01</p>
         <p>During this six month period our works concentrated on three, somewhat different areas. We looked at and developed a number of error concealment schemes for use in a variety of video coding environments. This work is described in an accompanying (draft) Masters thesis. In the thesis we describe application of this techniques to the MPEG video coding scheme. We felt that the unique frame ordering approach used in the MPEG scheme would be a challenge to any error concealment/error recovery technique. We continued with our work in the vector quantization area. We have also developed a new type of vector quantizer, which we call a scan predictive vector quantization. The scan predictive VQ was tested on data processed at Goddard to approximate Landsat 7 HRMSI resolution and compared favorably with existing VQ techniques. A paper describing this work is included. The third area is concerned more with reconstruction than compression. While there is a variety of efficient lossless <span class="hlt">image</span> compression schemes, they all have a common property that they use past data to encode future data. This is done either via taking differences, context modeling, or by building dictionaries. When encoding large <span class="hlt">images</span>, this common property becomes a common flaw. When the user wishes to decode just a portion of the <span class="hlt">image</span>, the requirement that the past history be available forces the decoding of a significantly larger portion of the <span class="hlt">image</span> than desired by the user. Even with intelligent partitioning of the <span class="hlt">image</span> dataset, the number of pixels decoded may be four times the number of pixels requested. We have developed an adaptive scanning strategy which can be used with any lossless compression scheme and which lowers the additional number of pixels to be decoded to about 7 percent of the number of pixels requested! A paper describing these results is included.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3387979','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3387979"><span><span class="hlt">Imaging</span> voltage in neurons</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Peterka, Darcy S.; Takahashi, Hiroto; Yuste, Rafael</p>
         <p>2011-01-01</p>
         <p>In the last decades, <span class="hlt">imaging</span> membrane potential has become a fruitful approach to study neural circuits, especially in invertebrate preparations with large, resilient neurons. At the same time, particularly in mammalian preparations, voltage <span class="hlt">imaging</span> methods suffer from poor signal to noise and secondary side effects, and they fall short of providing single-cell resolution when <span class="hlt">imaging</span> of the activity of neuronal populations. As an introduction to these techniques, we briefly review different voltage <span class="hlt">imaging</span> methods (including organic fluorophores, SHG chromophores, genetic indicators, hybrid, nanoparticles and intrinsic approaches), and illustrate some of their applications to neuronal biophysics and mammalian circuit analysis. We discuss their mechanisms of voltage sensitivity, from reorientation, electrochromic or electro-optical phenomena, to interaction among chromophores or membrane scattering, and highlight their advantages and shortcomings, commenting on the outlook for development of novel voltage <span class="hlt">imaging</span> methods. PMID:21220095</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/2299700','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/2299700"><span>Pediatric digital chest <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Tarver, R D; Cohen, M; Broderick, N J; Conces, D J</p>
         <p>1990-01-01</p>
         <p>The Philips Computed Radiography system performs well with pediatric portable chest radiographs, handling the throughout of a busy intensive care service 24 hours a day. <span class="hlt">Images</span> are excellent and routinely provide a conventional (unenhanced) <span class="hlt">image</span> and an edge-enhanced <span class="hlt">image</span>. Radiation dose is decreased by the lowered frequency of repeat examinations and the ability of the plates to respond to a much lower dose and still provide an adequate <span class="hlt">image</span>. The high quality and uniform density of serial PCR portable radiographs greatly enhances diagnostic content of the films. Decreased resolution has not been a problem clinically. <span class="hlt">Image</span> manipulation and electronic transfer to remote viewing stations appear to be helpful and are currently being evaluated further. The PCR system provides a marked improvement in pediatric portable chest radiology.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4133524','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4133524"><span>Pancreatitis-<span class="hlt">imaging</span> approach</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Busireddy, Kiran K; AlObaidy, Mamdoh; Ramalho, Miguel; Kalubowila, Janaka; Baodong, Liu; Santagostino, Ilaria; Semelka, Richard C</p>
         <p>2014-01-01</p>
         <p>Pancreatitis is defined as the inflammation of the pancreas and considered the most common pancreatic disease in children and adults. <span class="hlt">Imaging</span> plays a signiﬁcant role in the diagnosis, severity assessment, recognition of complications and guiding therapeutic interventions. In the setting of pancreatitis, wider availability and good <span class="hlt">image</span> quality make multi-detector contrast-enhanced computed tomography (MD-CECT) the most used <span class="hlt">imaging</span> technique. However, magnetic resonance <span class="hlt">imaging</span> (MRI) offers diagnostic capabilities similar to those of CT, with additional intrinsic advantages including lack of ionizing radiation and exquisite soft tissue characterization. This article reviews the proposed definitions of revised Atlanta classification for acute pancreatitis, illustrates a wide range of morphologic pancreatic parenchymal and associated peripancreatic changes for different types of acute pancreatitis. It also describes the spectrum of early and late chronic pancreatitis <span class="hlt">imaging</span> findings and illustrates some of the less common types of chronic pancreatitis, with special emphasis on the role of CT and MRI. PMID:25133027</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1992SPIE.1654..130W','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1992SPIE.1654..130W"><span>Compression for radiological <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Wilson, Dennis L.</p>
         <p>1992-07-01</p>
         <p>The viewing of radiological <span class="hlt">images</span> has peculiarities that must be taken into account in the design of a compression technique. The <span class="hlt">images</span> may be manipulated on a workstation to change the contrast, to change the center of the brightness levels that are viewed, and even to invert the <span class="hlt">images</span>. Because of the possible consequences of losing information in a medical application, bit preserving compression is used for the <span class="hlt">images</span> used for diagnosis. However, for archiving the <span class="hlt">images</span> may be compressed to 10 of their original size. A compression technique based on the Discrete Cosine Transform (DCT) takes the viewing factors into account by compressing the changes in the local brightness levels. The compression technique is a variation of the CCITT JPEG compression that suppresses the blocking of the DCT except in areas of very high contrast.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/870306','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/870306"><span><span class="hlt">Image</span> forming apparatus</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Satoh, Hisao; Haneda, Satoshi; Ikeda, Tadayoshi; Morita, Shizuo; Fukuchi, Masakazu</p>
         <p>1996-01-01</p>
         <p>In an <span class="hlt">image</span> forming apparatus having a detachable process cartridge in which an <span class="hlt">image</span> carrier on which an electrostatic latent <span class="hlt">image</span> is formed, and a developing unit which develops the electrostatic latent <span class="hlt">image</span> so that a toner <span class="hlt">image</span> can be formed, both integrally formed into one unit. There is provided a developer container including a discharge section which can be inserted into a supply opening of the developing unit, and a container in which a predetermined amount of developer is contained, wherein the developer container is provided to the toner supply opening of the developing unit and the developer is supplied into the developing unit housing when a toner stirring screw of the developing unit is rotated.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5480791','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5480791"><span><span class="hlt">Imaging</span> of sialadenitis</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Mukherji, Suresh</p>
         <p>2017-01-01</p>
         <p>Sialadenitis is an inflammation or infection of the salivary glands that may affect the parotid, submandibular and small salivary glands. <span class="hlt">Imaging</span> findings vary among unilateral or bilateral salivary gland enlargement, atrophy, abscess, ductal dilation, cysts, stones and calcification. <span class="hlt">Imaging</span> can detect abscess in acute bacterial suppurative sialadenitis, ductal changes with cysts in chronic adult and juvenile recurrent parotitis. <span class="hlt">Imaging</span> is sensitive for detection of salivary stones and stricture in obstructive sialadenitis. Immunoglobulin G4-sialadenitis appears as bilateral submandibular gland enlargement. <span class="hlt">Imaging</span> is helpful in staging and surveillance of patients with Sjögren’s syndrome. Correlation of <span class="hlt">imaging</span> findings with clinical presentation can aid diagnosis of granulomatous sialadenitis. Post-treatment sialadenitis can occur after radiotherapy, radioactive iodine or surgery. PMID:28059621</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016ASSL..439....1B','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016ASSL..439....1B"><span>Lucky <span class="hlt">Imaging</span> in Astronomy</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Brandner, Wolfgang; Hormuth, Felix</p>
         <p></p>
         <p>Lucky <span class="hlt">Imaging</span> improves the angular resolution of astronomical observations hampered by atmospheric turbulence ("seeing"). Unlike adaptive optics, Lucky <span class="hlt">Imaging</span> is a passive observing technique with individual integration times comparable to the atmospheric coherence time. Thanks to the advent of essentially noise free "Electron multiplying CCD" detectors, Lucky <span class="hlt">Imaging</span> saw a renewed interest in the past decade. It is now routinely used at a number of 2-5-m class telescopes, such as ESO's NTT. We review the history of Lucky <span class="hlt">Imaging</span>, present the technical implementation, describe the data analysis philosophy, and show some recent results obtained with this technique. We also discuss the advantages and limitations of Lucky <span class="hlt">Imaging</span> compared to other passive and active high angular resolution observing techniques.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2017PhyEd..52b3003Z','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2017PhyEd..52b3003Z"><span>What is an <span class="hlt">image</span>?</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Zetie, K. P.</p>
         <p>2017-03-01</p>
         <p>In basic physics, often in their first year of study of the subject, students meet the concept of an <span class="hlt">image</span>, for example when using pinhole cameras and finding the position of an <span class="hlt">image</span> in a mirror. They are also familiar with the term in photography and design, through software which allows <span class="hlt">image</span> manipulation, even ‘in-camera’ on most Smartphones. But what is meant by the term <span class="hlt">image</span>? A good, clear definition is not readily available in a range of textbooks I examined, nor on various physics sites, beyond something like ‘a representation of an object’ or ‘a reproduction of an object formed using a mirror or lens’ (or words to those effects). None of this explains why a mirror forms an <span class="hlt">image</span> and a piece of paper does not, or why a pinhole does, but a large hole does not. In this short paper, these ideas are explored in an investigative way.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/18267522','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/18267522"><span>Spread spectrum <span class="hlt">image</span> steganography.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Marvel, L M; Boncelet, C R; Retter, C T</p>
         <p>1999-01-01</p>
         <p>In this paper, we present a new method of digital steganography, entitled spread spectrum <span class="hlt">image</span> steganography (SSIS). Steganography, which means "covered writing" in Greek, is the science of communicating in a hidden manner. Following a discussion of steganographic communication theory and review of existing techniques, the new method, SSIS, is introduced. This system hides and recovers a message of substantial length within digital imagery while maintaining the original <span class="hlt">image</span> size and dynamic range. The hidden message can be recovered using appropriate keys without any knowledge of the original <span class="hlt">image</span>. <span class="hlt">Image</span> restoration, error-control coding, and techniques similar to spread spectrum are described, and the performance of the system is illustrated. A message embedded by this method can be in the form of text, imagery, or any other digital signal. Applications for such a data-hiding scheme include in-band captioning, covert communication, <span class="hlt">image</span> tamperproofing, authentication, embedded control, and revision tracking.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1160243','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1160243"><span>Time encoded radiation <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Marleau, Peter; Brubaker, Erik; Kiff, Scott</p>
         <p>2014-10-21</p>
         <p>The various technologies presented herein relate to detecting nuclear material at a large stand-off distance. An <span class="hlt">imaging</span> system is presented which can detect nuclear material by utilizing time encoded <span class="hlt">imaging</span> relating to maximum and minimum radiation particle counts rates. The <span class="hlt">imaging</span> system is integrated with a data acquisition system that can utilize variations in photon pulse shape to discriminate between neutron and gamma-ray interactions. Modulation in the detected neutron count rates as a function of the angular orientation of the detector due to attenuation of neighboring detectors is utilized to reconstruct the neutron source distribution over 360 degrees around the <span class="hlt">imaging</span> system. Neutrons (e.g., fast neutrons) and/or gamma-rays are incident upon scintillation material in the <span class="hlt">imager</span>, the photons generated by the scintillation material are converted to electrical energy from which the respective neutrons/gamma rays can be determined and, accordingly, a direction to, and the location of, a radiation source identified.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19920012030','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19920012030"><span>Photographic <span class="hlt">Image</span> Restoration</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Hite, Gerald E.</p>
         <p>1991-01-01</p>
         <p>Deblurring capabilities would significantly improve the Flight Science Support Office's ability to monitor the effects of lift-off on the shuttle and landing on the orbiter. A deblurring program was written and implemented to extract information from blurred <span class="hlt">images</span> containing a straight line or edge and to use that information to deblur the <span class="hlt">image</span>. The program was successfully applied to an <span class="hlt">image</span> blurred by improper focussing and two blurred by different amounts of blurring. In all cases, the reconstructed modulation transfer function not only had the same zero contours as the Fourier transform of the blurred <span class="hlt">image</span> but the associated point spread function also had structure not easily described by simple parameterizations. The difficulties posed by the presence of noise in the blurred <span class="hlt">image</span> necessitated special consideration. An amplitude modification technique was developed for the zero contours of the modulation transfer function at low to moderate frequencies and a smooth filter was used to suppress high frequency noise.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-GSFC_20171208_Archive_e002107.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-GSFC_20171208_Archive_e002107.html"><span>Aurora Composite <span class="hlt">Image</span></span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2017-12-08</p>
         <p>This composite <span class="hlt">image</span> presents the three most visible elements of space weather: a storm from the Sun, aurora as seen from space, and aurora as seen from the Earth. The solar storm is a corona mass ejection (CME) composite from EIT 304Å superimposed on a LASCO C2 <span class="hlt">image</span>, both from SOHO. The middle <span class="hlt">image</span> from Polar’s VIS <span class="hlt">imager</span> shows charged particles as they spread down across the U.S. during a large solar storm event on July 14, 2000. Lastly, Jan Curtis took this <span class="hlt">image</span> of an aurora display in Alaska, the visible evidence of space weather that we see here on Earth. Credit: NASA/GSFC/SOHO/ESA To learn more go to the SOHO website: sohowww.nascom.nasa.gov/home.html To learn more about NASA's Sun Earth Day go here: sunearthday.nasa.gov/2010/index.php</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_12");'>12</a></li>
      <li><a href="#" onclick='return showDiv("page_13");'>13</a></li>
      <li class="active"><span>14</span></li>
   <li><a href="#" onclick='return showDiv("page_15");'>15</a></li>
      <li><a href="#" onclick='return showDiv("page_16");'>16</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_14 -->
   <div id="page_15" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_13");'>13</a></li>
      <li><a href="#" onclick='return showDiv("page_14");'>14</a></li>
      <li class="active"><span>15</span></li>
   <li><a href="#" onclick='return showDiv("page_16");'>16</a></li>
      <li><a href="#" onclick='return showDiv("page_17");'>17</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="281">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-GSFC_20171208_Archive_e000343.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-GSFC_20171208_Archive_e000343.html"><span>Mercury Transit (Composite <span class="hlt">Image</span>)</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2017-12-08</p>
         <p>On May 9, 2016, Mercury passed directly between the sun and Earth. This event – which happens about 13 times each century – is called a transit. NASA’s Solar Dynamics Observatory, or SDO, studies the sun 24/7 and captured the entire seven-and-a-half-hour event. This composite <span class="hlt">image</span> of Mercury’s journey across the sun was created with visible-light <span class="hlt">images</span> from the Helioseismic and Magnetic <span class="hlt">Imager</span> on SDO. <span class="hlt">Image</span> Credit: NASA's Goddard Space Flight Center/SDO/Genna Duberstein NASA <span class="hlt">image</span> use policy. NASA Goddard Space Flight Center enables NASA’s mission through four scientific endeavors: Earth Science, Heliophysics, Solar System Exploration, and Astrophysics. Goddard plays a leading role in NASA’s accomplishments by contributing compelling scientific knowledge to advance the Agency’s mission. Follow us on Twitter Like us on Facebook Find us on Instagram</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2011SPIE.7867E..0SQ','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2011SPIE.7867E..0SQ"><span>Social <span class="hlt">image</span> quality</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Qiu, Guoping; Kheiri, Ahmed</p>
         <p>2011-01-01</p>
         <p>Current subjective <span class="hlt">image</span> quality assessments have been developed in the laboratory environments, under controlledconditions, and are dependent on the participation of limited numbers of observers. In this research, with the help of Web 2.0 and social media technology, a new method for building a subjective <span class="hlt">image</span> quality metric has been developed where the observers are the Internet users. A website with a simple user interface that enables Internet users from anywhere at any time to vote for a better quality version of a pair of the same <span class="hlt">image</span> has been constructed. Users' votes are recorded and used to rank the <span class="hlt">images</span> according to their perceived visual qualities. We have developed three rank aggregation algorithms to process the recorded pair comparison data, the first uses a naive approach, the second employs a Condorcet method, and the third uses the Dykstra's extension of Bradley-Terry method. The website has been collecting data for about three months and has accumulated over 10,000 votes at the time of writing this paper. Results show that the Internet and its allied technologies such as crowdsourcing offer a promising new paradigm for <span class="hlt">image</span> and video quality assessment where hundreds of thousands of Internet users can contribute to building more robust <span class="hlt">image</span> quality metrics. We have made Internet user generated social <span class="hlt">image</span> quality (SIQ) data of a public <span class="hlt">image</span> database available online (http://www.hdri.cs.nott.ac.uk/siq/) to provide the <span class="hlt">image</span> quality research community with a new source of ground truth data. The website continues to collect votes and will include more public <span class="hlt">image</span> databases and will also be extended to include videos to collect social video quality (SVQ) data. All data will be public available on the website in due course.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/26585712','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/26585712"><span>Groupwise <span class="hlt">Image</span> Registration Guided by a Dynamic Digraph of <span class="hlt">Images</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Tang, Zhenyu; Fan, Yong</p>
         <p>2016-04-01</p>
         <p>For groupwise <span class="hlt">image</span> registration, graph theoretic methods have been adopted for discovering the manifold of <span class="hlt">images</span> to be registered so that accurate registration of <span class="hlt">images</span> to a group center <span class="hlt">image</span> can be achieved by aligning similar <span class="hlt">images</span> that are linked by the shortest graph paths. However, the <span class="hlt">image</span> similarity measures adopted to build a graph of <span class="hlt">images</span> in the extant methods are essentially pairwise measures, not effective for capturing the groupwise similarity among multiple <span class="hlt">images</span>. To overcome this problem, we present a groupwise <span class="hlt">image</span> similarity measure that is built on sparse coding for characterizing <span class="hlt">image</span> similarity among all input <span class="hlt">images</span> and build a directed graph (digraph) of <span class="hlt">images</span> so that similar <span class="hlt">images</span> are connected by the shortest paths of the digraph. Following the shortest paths determined according to the digraph, <span class="hlt">images</span> are registered to a group center <span class="hlt">image</span> in an iterative manner by decomposing a large anatomical deformation field required to register an <span class="hlt">image</span> to the group center <span class="hlt">image</span> into a series of small ones between similar <span class="hlt">images</span>. During the iterative <span class="hlt">image</span> registration, the digraph of <span class="hlt">images</span> evolves dynamically at each iteration step to pursue an accurate estimation of the <span class="hlt">image</span> manifold. Moreover, an adaptive dictionary strategy is adopted in the groupwise <span class="hlt">image</span> similarity measure to ensure fast convergence of the iterative registration procedure. The proposed method has been validated based on both simulated and real brain <span class="hlt">images</span>, and experiment results have demonstrated that our method was more effective for learning the manifold of input <span class="hlt">images</span> and achieved higher registration accuracy than state-of-the-art groupwise <span class="hlt">image</span> registration methods.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/24311236','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/24311236"><span>Medical <span class="hlt">imaging</span>, PACS, and <span class="hlt">imaging</span> informatics: retrospective.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Huang, H K</p>
         <p>2014-01-01</p>
         <p>Historical reviews of PACS (picture archiving and communication system) and <span class="hlt">imaging</span> informatics development from different points of view have been published in the past (Huang in Euro J Radiol 78:163-176, 2011; Lemke in Euro J Radiol 78:177-183, 2011; Inamura and Jong in Euro J Radiol 78:184-189, 2011). This retrospective attempts to look at the topic from a different angle by identifying certain basic medical <span class="hlt">imaging</span> inventions in the 1960s and 1970s which had conceptually defined basic components of PACS guiding its course of development in the 1980s and 1990s, as well as subsequent <span class="hlt">imaging</span> informatics research in the 2000s. In medical <span class="hlt">imaging</span>, the emphasis was on the innovations at Georgetown University in Washington, DC, in the 1960s and 1970s. During the 1980s and 1990s, research and training support from US government agencies and public and private medical <span class="hlt">imaging</span> manufacturers became available for training of young talents in biomedical physics and for developing the key components required for PACS development. In the 2000s, computer hardware and software as well as communication networks advanced by leaps and bounds, opening the door for medical <span class="hlt">imaging</span> informatics to flourish. Because many key components required for the PACS operation were developed by the UCLA PACS Team and its collaborative partners in the 1980s, this presentation is centered on that aspect. During this period, substantial collaborative research efforts by many individual teams in the US and in Japan were highlighted. Credits are due particularly to the Pattern Recognition Laboratory at Georgetown University, and the computed radiography (CR) development at the Fuji Electric Corp. in collaboration with Stanford University in the 1970s; the <span class="hlt">Image</span> Processing Laboratory at UCLA in the 1980s-1990s; as well as the early PACS development at the Hokkaido University, Sapporo, Japan, in the late 1970s, and film scanner and digital radiography developed by Konishiroku Photo Ind. Co. Ltd</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=authentic+AND+leadership&pg=7&id=ED500099','ERIC'); return false;" href="https://eric.ed.gov/?q=authentic+AND+leadership&pg=7&id=ED500099"><span>Building an Authentic Leadership <span class="hlt">Image</span></span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Criswell, Corey; Campbell, David</p>
         <p>2008-01-01</p>
         <p>Your <span class="hlt">image</span> can be either an asset or a liability for you as a leader. <span class="hlt">Image</span> building is neither superficial nor unimportant. It's not about creating a false <span class="hlt">image</span>, but recognizing genuine aspects of yourself that should be coming across to other people--but aren't. Crafting your <span class="hlt">image</span> requires you to gain a clear picture of the <span class="hlt">image</span> people are…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/909581','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/909581"><span>Adaptive wiener <span class="hlt">image</span> restoration kernel</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Yuan, Ding [Henderson, NV</p>
         <p>2007-06-05</p>
         <p>A method and device for restoration of electro-optical <span class="hlt">image</span> data using an adaptive Wiener filter begins with constructing <span class="hlt">imaging</span> system Optical Transfer Function, and the Fourier Transformations of the noise and the <span class="hlt">image</span>. A spatial representation of the <span class="hlt">imaged</span> object is restored by spatial convolution of the <span class="hlt">image</span> using a Wiener restoration kernel.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/25336773','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/25336773"><span>Mirror <span class="hlt">image</span> agnosia.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Chandra, Sadanandavalli Retnaswami; Issac, Thomas Gregor</p>
         <p>2014-10-01</p>
         <p>Gnosis is a modality-specific ability to access semantic knowledge of an object or stimulus in the presence of normal perception. Failure of this is agnosia or disorder of recognition. It can be highly selective within a mode. self-<span class="hlt">images</span> are different from others as none has seen one's own <span class="hlt">image</span> except in reflection. Failure to recognize this <span class="hlt">image</span> can be labeled as mirror <span class="hlt">image</span> agnosia or Prosopagnosia for reflected self-<span class="hlt">image</span>. Whereas mirror agnosia is a well-recognized situation where the person while looking at reflected <span class="hlt">images</span> of other objects in the mirror he imagines that the objects are in fact inside the mirror and not outside. Five patients, four females, and one male presented with failure to recognize reflected self-<span class="hlt">image</span>, resulting in patients conversing with the <span class="hlt">image</span> as a friend, fighting because the person in mirror is wearing her nose stud, suspecting the reflected self-<span class="hlt">image</span> to be an intruder; but did not have prosopagnosia for others faces, non living objects on self and also apraxias except dressing apraxia in one patient. This phenomena is new to our knowledge. Mirror <span class="hlt">image</span> agnosia is an unique phenomena which is seen in patients with parietal lobe atrophy without specificity to a category of dementing illness and seems to disappear as disease advances. Reflected self-<span class="hlt">images</span> probably have a specific neural substrate that gets affected very early in posterior dementias specially the ones which predominantly affect the right side. At that phase most patients are mistaken as suffering from psychiatric disorder as cognition is moderately preserved. As disease becomes more widespread this symptom becomes masked. A high degree of suspicion and proper assessment might help physicians to recognize the organic cause of the symptom so that early therapeutic interventions can be initiated. Further assessment of the symptom with FMRI and PET scan is likely to solve the mystery of how brain handles reflected self-<span class="hlt">images</span>. A new observation involving failure</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4201793','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4201793"><span>Mirror <span class="hlt">Image</span> Agnosia</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Chandra, Sadanandavalli Retnaswami; Issac, Thomas Gregor</p>
         <p>2014-01-01</p>
         <p>Background: Gnosis is a modality-specific ability to access semantic knowledge of an object or stimulus in the presence of normal perception. Failure of this is agnosia or disorder of recognition. It can be highly selective within a mode. self-<span class="hlt">images</span> are different from others as none has seen one's own <span class="hlt">image</span> except in reflection. Failure to recognize this <span class="hlt">image</span> can be labeled as mirror <span class="hlt">image</span> agnosia or Prosopagnosia for reflected self-<span class="hlt">image</span>. Whereas mirror agnosia is a well-recognized situation where the person while looking at reflected <span class="hlt">images</span> of other objects in the mirror he imagines that the objects are in fact inside the mirror and not outside. Material and Methods:: Five patients, four females, and one male presented with failure to recognize reflected self-<span class="hlt">image</span>, resulting in patients conversing with the <span class="hlt">image</span> as a friend, fighting because the person in mirror is wearing her nose stud, suspecting the reflected self-<span class="hlt">image</span> to be an intruder; but did not have prosopagnosia for others faces, non living objects on self and also apraxias except dressing apraxia in one patient. This phenomena is new to our knowledge. Results: Mirror <span class="hlt">image</span> agnosia is an unique phenomena which is seen in patients with parietal lobe atrophy without specificity to a category of dementing illness and seems to disappear as disease advances. Discussion: Reflected self-<span class="hlt">images</span> probably have a specific neural substrate that gets affected very early in posterior dementias specially the ones which predominantly affect the right side. At that phase most patients are mistaken as suffering from psychiatric disorder as cognition is moderately preserved. As disease becomes more widespread this symptom becomes masked. A high degree of suspicion and proper assessment might help physicians to recognize the organic cause of the symptom so that early therapeutic interventions can be initiated. Further assessment of the symptom with FMRI and PET scan is likely to solve the mystery of how brain handles</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2010SPIE.7690E..0QA','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2010SPIE.7690E..0QA"><span>Compressive light field <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Ashok, Amit; Neifeld, Mark A.</p>
         <p>2010-04-01</p>
         <p>Light field <span class="hlt">imagers</span> such as the plenoptic and the integral <span class="hlt">imagers</span> inherently measure projections of the four dimensional (4D) light field scalar function onto a two dimensional sensor and therefore, suffer from a spatial vs. angular resolution trade-off. Programmable light field <span class="hlt">imagers</span>, proposed recently, overcome this spatioangular resolution trade-off and allow high-resolution capture of the (4D) light field function with multiple measurements at the cost of a longer exposure time. However, these light field <span class="hlt">imagers</span> do not exploit the spatio-angular correlations inherent in the light fields of natural scenes and thus result in photon-inefficient measurements. Here, we describe two architectures for compressive light field <span class="hlt">imaging</span> that require relatively few photon-efficient measurements to obtain a high-resolution estimate of the light field while reducing the overall exposure time. Our simulation study shows that, compressive light field <span class="hlt">imagers</span> using the principal component (PC) measurement basis require four times fewer measurements and three times shorter exposure time compared to a conventional light field <span class="hlt">imager</span> in order to achieve an equivalent light field reconstruction quality.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20100009689','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20100009689"><span>Polarization <span class="hlt">Imaging</span> Apparatus</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Zou, Yingyin K.; Chen, Qiushui</p>
         <p>2010-01-01</p>
         <p>A polarization <span class="hlt">imaging</span> apparatus has shown promise as a prototype of instruments for medical <span class="hlt">imaging</span> with contrast greater than that achievable by use of non-polarized light. The underlying principles of design and operation are derived from observations that light interacts with tissue ultrastructures that affect reflectance, scattering, absorption, and polarization of light. The apparatus utilizes high-speed electro-optical components for generating light properties and acquiring polarization <span class="hlt">images</span> through aligned polarizers. These components include phase retarders made of OptoCeramic (registered TradeMark) material - a ceramic that has a high electro-optical coefficient. The apparatus includes a computer running a program that implements a novel algorithm for controlling the phase retarders, capturing <span class="hlt">image</span> data, and computing the Stokes polarization <span class="hlt">images</span>. Potential applications include <span class="hlt">imaging</span> of superficial cancers and other skin lesions, early detection of diseased cells, and microscopic analysis of tissues. The high <span class="hlt">imaging</span> speed of this apparatus could be beneficial for observing live cells or tissues, and could enable rapid identification of moving targets in astronomy and national defense. The apparatus could also be used as an analysis tool in material research and industrial processing.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19720006520','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19720006520"><span>Spaceborne electronic <span class="hlt">imaging</span> systems</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1971-01-01</p>
         <p>Criteria and recommended practices for the design of the spaceborne elements of electronic <span class="hlt">imaging</span> systems are presented. A spaceborne electronic <span class="hlt">imaging</span> system is defined as a device that collects energy in some portion of the electromagnetic spectrum with detector(s) whose direct output is an electrical signal that can be processed (using direct transmission or delayed transmission after recording) to form a pictorial <span class="hlt">image</span>. This definition encompasses both <span class="hlt">image</span> tube systems and scanning point-detector systems. The intent was to collect the design experience and recommended practice of the several systems possessing the common denominator of acquiring <span class="hlt">images</span> from space electronically and to maintain the system viewpoint rather than pursuing specialization in devices. The devices may be markedly different physically, but each was designed to provide a particular type of <span class="hlt">image</span> within particular limitations. Performance parameters which determine the type of system selected for a given mission and which influence the design include: Sensitivity, Resolution, Dynamic range, Spectral response, Frame rate/bandwidth, Optics compatibility, <span class="hlt">Image</span> motion, Radiation resistance, Size, Weight, Power, and Reliability.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5648558','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5648558"><span>Perspective: Advanced particle <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Chandler, David W.</p>
         <p>2017-01-01</p>
         <p>Since the first ion <span class="hlt">imaging</span> experiment [D. W. Chandler and P. L. Houston, J. Chem. Phys. 87, 1445–1447 (1987)], demonstrating the capability of collecting an <span class="hlt">image</span> of the photofragments from a unimolecular dissociation event and analyzing that <span class="hlt">image</span> to obtain the three-dimensional velocity distribution of the fragments, the efficacy and breadth of application of the ion <span class="hlt">imaging</span> technique have continued to improve and grow. With the addition of velocity mapping, ion/electron centroiding, and slice <span class="hlt">imaging</span> techniques, the versatility and velocity resolution have been unmatched. Recent improvements in molecular beam, laser, sensor, and computer technology are allowing even more advanced particle <span class="hlt">imaging</span> experiments, and eventually we can expect multi-mass <span class="hlt">imaging</span> with co-variance and full coincidence capability on a single shot basis with repetition rates in the kilohertz range. This progress should further enable “complete” experiments—the holy grail of molecular dynamics—where all quantum numbers of reactants and products of a bimolecular scattering event are fully determined and even under our control. PMID:28688442</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1982OptEn..21..101C','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1982OptEn..21..101C"><span>Guest Editorial <span class="hlt">Image</span> Quality</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Cheatham, Patrick S.</p>
         <p>1982-02-01</p>
         <p>The term <span class="hlt">image</span> quality can, unfortunately, apply to anything from a public relations firm's discussion to a comparison between corner drugstores' film processing. If we narrow the discussion to optical systems, we clarify the problem somewhat, but only slightly. We are still faced with a multitude of <span class="hlt">image</span> quality measures all different, and all couched in different terminology. Optical designers speak of MTF values, digital processors talk about summations of before and after <span class="hlt">image</span> differences, pattern recognition engineers allude to correlation values, and radar <span class="hlt">imagers</span> use side-lobe response values measured in decibels. Further complexity is introduced by terms such as information content, bandwidth, Strehl ratios, and, of course, limiting resolution. The problem is to compare these different yardsticks and try to establish some concrete ideas about evaluation of a final <span class="hlt">image</span>. We need to establish the <span class="hlt">image</span> attributes which are the most important to perception of the <span class="hlt">image</span> in question and then begin to apply the different system parameters to those attributes.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19448.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19448.html"><span>MESSENGER Final <span class="hlt">Image</span></span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-04-30</p>
         <p>Today, the MESSENGER spacecraft sent its final <span class="hlt">image</span>. Originally planned to orbit Mercury for one year, the mission exceeded all expectations, lasting for over four years and acquiring extensive datasets with its seven scientific instruments and radio science investigation. This afternoon, the spacecraft succumbed to the pull of solar gravity and impacted Mercury's surface. The <span class="hlt">image</span> shown here is the last one acquired and transmitted back to Earth by the mission. The <span class="hlt">image</span> is located within the floor of the 93-kilometer-diameter crater Jokai. The spacecraft struck the planet just north of Shakespeare basin. Date acquired: April 30, 2015 <span class="hlt">Image</span> Mission Elapsed Time (MET): 72716050 <span class="hlt">Image</span> ID: 8422953 Instrument: Narrow Angle Camera (NAC) of the Mercury Dual <span class="hlt">Imaging</span> System (MDIS) Center Latitude: 72.0° Center Longitude: 223.8° E Resolution: 2.1 meters/pixel Scale: This <span class="hlt">image</span> is about 1 kilometers (0.6 miles) across Incidence Angle: 57.9° Emission Angle: 56.5° Phase Angle: 40.7° http://photojournal.jpl.nasa.gov/catalog/PIA19448</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/867365','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/867365"><span>Polarization transfer NMR <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Sillerud, Laurel O.; van Hulsteyn, David B.</p>
         <p>1990-01-01</p>
         <p>A nuclear magnetic resonance (NMR) <span class="hlt">image</span> is obtained with spatial information modulated by chemical information. The modulation is obtained through polarization transfer from a first element representing the desired chemical, or functional, information, which is covalently bonded and spin-spin coupled with a second element effective to provide the <span class="hlt">imaging</span> data. First and second rf pulses are provided at first and second frequencies for exciting the <span class="hlt">imaging</span> and functional elements, with <span class="hlt">imaging</span> gradients applied therebetween to spatially separate the nuclei response for <span class="hlt">imaging</span>. The second rf pulse is applied at a time after the first pulse which is the inverse of the spin coupling constant to select the transfer element nuclei which are spin coupled to the functional element nuclei for <span class="hlt">imaging</span>. In a particular application, compounds such as glucose, lactate, or lactose, can be labeled with .sup.13 C and metabolic processes involving the compounds can be <span class="hlt">imaged</span> with the sensitivity of .sup.1 H and the selectivity of .sup.13 C.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/28673870','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/28673870"><span>Fluorescence lifetime <span class="hlt">imaging</span> ophthalmoscopy.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Dysli, Chantal; Wolf, Sebastian; Berezin, Mikhail Y; Sauer, Lydia; Hammer, Martin; Zinkernagel, Martin S</p>
         <p>2017-09-01</p>
         <p><span class="hlt">Imaging</span> techniques based on retinal autofluorescence have found broad applications in ophthalmology because they are extremely sensitive and noninvasive. Conventional fundus autofluorescence <span class="hlt">imaging</span> measures fluorescence intensity of endogenous retinal fluorophores. It mainly derives its signal from lipofuscin at the level of the retinal pigment epithelium. Fundus autofluorescence, however, can not only be characterized by the spatial distribution of the fluorescence intensity or emission spectrum, but also by a characteristic fluorescence lifetime function. The fluorescence lifetime is the average amount of time a fluorophore remains in the excited state following excitation. Fluorescence lifetime <span class="hlt">imaging</span> ophthalmoscopy (FLIO) is an emerging <span class="hlt">imaging</span> modality for in vivo measurement of lifetimes of endogenous retinal fluorophores. Recent reports in this field have contributed to our understanding of the pathophysiology of various macular and retinal diseases. Within this review, the basic concept of fluorescence lifetime <span class="hlt">imaging</span> is provided. It includes technical background information and correlation with in vitro measurements of individual retinal metabolites. In a second part, clinical applications of fluorescence lifetime <span class="hlt">imaging</span> and fluorescence lifetime features of selected retinal diseases such as Stargardt disease, age-related macular degeneration, choroideremia, central serous chorioretinopathy, macular holes, diabetic retinopathy, and retinal artery occlusion are discussed. Potential areas of use for fluorescence lifetime <span class="hlt">imaging</span> ophthalmoscopy will be outlined at the end of this review. Copyright © 2017 The Authors. Published by Elsevier Ltd.. All rights reserved.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/pages/biblio/1371480-perspective-advanced-particle-imaging','SCIGOV-DOEP'); return false;" href="https://www.osti.gov/pages/biblio/1371480-perspective-advanced-particle-imaging"><span>Perspective: Advanced particle <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/pages">DOE PAGES</a></p>
      <p>Chandler, David W.; Houston, Paul L.; Parker, David H.</p>
         <p>2017-05-26</p>
         <p>This study discuss, the first ion <span class="hlt">imaging</span> experiment demonstrating the capability of collecting an <span class="hlt">image</span> of the photofragments from a unimolecular dissociation event and analyzing that <span class="hlt">image</span> to obtain the three-dimensional velocity distribution of the fragments, the efficacy and breadth of application of the ion <span class="hlt">imaging</span> technique have continued to improve and grow. With the addition of velocity mapping, ion/electron centroiding, and slice <span class="hlt">imaging</span> techniques, the versatility and velocity resolution have been unmatched. Recent improvements in molecular beam, laser, sensor, and computer technology are allowing even more advanced particle <span class="hlt">imaging</span> experiments, and eventually we can expect multi-mass <span class="hlt">imaging</span> with co-variancemore » and full coincidence capability on a single shot basis with repetition rates in the kilohertz range. This progress should further enable “complete” experiments—the holy grail of molecular dynamics—where all quantum numbers of reactants and products of a bimolecular scattering event are fully determined and even under our control.« less</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19990052890&hterms=rust&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D30%26Ntt%3Drust','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19990052890&hterms=rust&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D30%26Ntt%3Drust"><span>A Compact Polarization <span class="hlt">Imager</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Thompson, Karl E.; Rust, David M.; Chen, Hua</p>
         <p>1995-01-01</p>
         <p>A new type of <span class="hlt">image</span> detector has been designed to analyze the polarization of light simultaneously at all picture elements (pixels) in a scene. The Integrated Dual <span class="hlt">Imaging</span> Detector (IDID) consists of a polarizing beamsplitter bonded to a custom-designed charge-coupled device with signal-analysis circuitry, all integrated on a silicon chip. The IDID should simplify the design and operation of <span class="hlt">imaging</span> polarimeters and spectroscopic <span class="hlt">imagers</span> used, for example, in atmospheric and solar research. Other applications include environmental monitoring and robot vision. Innovations in the IDID include two interleaved 512 x 1024 pixel <span class="hlt">imaging</span> arrays (one for each polarization plane), large dynamic range (well depth of 10(exp 6) electrons per pixel), simultaneous readout and display of both <span class="hlt">images</span> at 10(exp 6) pixels per second, and on-chip analog signal processing to produce polarization maps in real time. When used with a lithium niobate Fabry-Perot etalon or other color filter that can encode spectral information as polarization, the IDID can reveal tiny differences between simultaneous <span class="hlt">images</span> at two wavelengths.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1981tsc..rept.....S','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1981tsc..rept.....S"><span>Sensor <span class="hlt">image</span> prediction techniques</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Stenger, A. J.; Stone, W. R.; Berry, L.; Murray, T. J.</p>
         <p>1981-02-01</p>
         <p>The preparation of prediction imagery is a complex, costly, and time consuming process. <span class="hlt">Image</span> prediction systems which produce a detailed replica of the <span class="hlt">image</span> area require the extensive Defense Mapping Agency data base. The purpose of this study was to analyze the use of <span class="hlt">image</span> predictions in order to determine whether a reduced set of more compact <span class="hlt">image</span> features contains enough information to produce acceptable navigator performance. A job analysis of the navigator's mission tasks was performed. It showed that the cognitive and perceptual tasks he performs during navigation are identical to those performed for the targeting mission function. In addition, the results of the analysis of his performance when using a particular sensor can be extended to the analysis of this mission tasks using any sensor. An experimental approach was used to determine the relationship between navigator performance and the type of amount of information in the prediction <span class="hlt">image</span>. A number of subjects were given <span class="hlt">image</span> predictions containing varying levels of scene detail and different <span class="hlt">image</span> features, and then asked to identify the predicted targets in corresponding dynamic flight sequences over scenes of cultural, terrain, and mixed (both cultural and terrain) content.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19995.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19995.html"><span>Dawn HAMO <span class="hlt">Image</span> 53</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-11-05</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows the surface of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers) around mid-latitudes. The <span class="hlt">image</span> was taken on Sept. 28, 2015, and has a resolution of 450 feet (140 meters) per pixel. The unusual mountain Ahuna Mons is featured in this <span class="hlt">image</span>, named for the traditional post-harvest festival of the Sumi tribe of Nagaland, India. It is 4 miles (6 kilometers) tall and 12 miles (20 kilometers) in diameter. http://photojournal.jpl.nasa.gov/catalog/PIA19995</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_13");'>13</a></li>
      <li><a href="#" onclick='return showDiv("page_14");'>14</a></li>
      <li class="active"><span>15</span></li>
   <li><a href="#" onclick='return showDiv("page_16");'>16</a></li>
      <li><a href="#" onclick='return showDiv("page_17");'>17</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_15 -->
   <div id="page_16" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_14");'>14</a></li>
      <li><a href="#" onclick='return showDiv("page_15");'>15</a></li>
      <li class="active"><span>16</span></li>
   <li><a href="#" onclick='return showDiv("page_17");'>17</a></li>
      <li><a href="#" onclick='return showDiv("page_18");'>18</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="301">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=1665238','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=1665238"><span><span class="hlt">Imaging</span> of thymic disorders</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Bogot, Naama R; Quint, Leslie E</p>
         <p>2005-01-01</p>
         <p>Evaluation of the thymus poses a challenge to the radiologist. In addition to age-related changes in thymic size, shape, and tissue composition, there is considerable variability in the normal adult thymic appearance within any age group. Many different types of disorders may affect the thymus, including hyperplasia, cysts, and benign and malignant neoplasms, both primary and secondary; clinical and <span class="hlt">imaging</span> findings typical for each disease process are described in this article. Whereas computed tomography is the mainstay for <span class="hlt">imaging</span> the thymus, other <span class="hlt">imaging</span> modalities may occasionally provide additional structural or functional information. PMID:16361143</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/AD1034764','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/AD1034764"><span>Dynamic Optically Multiplexed <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2015-07-29</p>
         <p>Direction LENS.ZMX Configuration 1 of 1 3D Layout 10/21/2014 Scale: 1.000020.00 Millimeters X Y Z Parent Lens (a) (b) 0 20 40 60 80 100 0 20 40 60 80 100...V. Shah, and T. Shih “Design Architectures for Optically Multiplexed <span class="hlt">Imaging</span>,” in submission 9 R. Gupta, P . Indyk, E. Price, and Y . Rachlin...the number of multiplexed <span class="hlt">images</span>. As a result, measurements from a sufficiently fast sampling sensor can be processed to yield a low distortion <span class="hlt">image</span></p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3495576','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3495576"><span>Tendon and ligament <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Hodgson, R J; O'Connor, P J; Grainger, A J</p>
         <p>2012-01-01</p>
         <p>MRI and ultrasound are now widely used for the assessment of tendon and ligament abnormalities. Healthy tendons and ligaments contain high levels of collagen with a structured orientation, which gives rise to their characteristic normal <span class="hlt">imaging</span> appearances as well as causing particular <span class="hlt">imaging</span> artefacts. Changes to ligaments and tendons as a result of disease and injury can be demonstrated using both ultrasound and MRI. These have been validated against surgical and histological findings. Novel <span class="hlt">imaging</span> techniques are being developed that may improve the ability of MRI and ultrasound to assess tendon and ligament disease. PMID:22553301</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20880.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20880.html"><span>Dawn LAMO <span class="hlt">Image</span> 158</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-08-25</p>
         <p>An area along the rim of the crater at the center of this view from NASA Dawn spacecraft, has collapsed, producing a lobe-shaped feature where the material settled. The <span class="hlt">image</span> is centered at approximately 52 degrees north latitude, 316 degrees east longitude. NASA's Dawn spacecraft took this <span class="hlt">image</span> on May 28, 2016, from its low-altitude mapping orbit, at a distance of about 240 miles (385 kilometers) above the surface of Ceres. The <span class="hlt">image</span> resolution is 120 feet (35 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20880</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/25949070','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/25949070"><span><span class="hlt">Imaging</span> in diabetic retinopathy.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Salz, David A; Witkin, Andre J</p>
         <p>2015-01-01</p>
         <p>While the primary method for evaluating diabetic retinopathy involves direct and indirect ophthalmoscopy, various <span class="hlt">imaging</span> modalities are of significant utility in the screening, evaluation, diagnosis, and treatment of different presentations and manifestations of this disease. This manuscript is a review of the important <span class="hlt">imaging</span> modalities that are used in diabetic retinopathy, including color fundus photography, fluorescein angiography, B-scan ultrasonography, and optical coherence tomography. The article will provide an overview of these different <span class="hlt">imaging</span> techniques and how they can be most effectively used in current practice.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2327915','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2327915"><span><span class="hlt">Imaging</span> By Ultrasound</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Kidney, Maria R.</p>
         <p>1986-01-01</p>
         <p><span class="hlt">Imaging</span> by ultrasound has dramatically changed the investigation and management of many clinical problems. It is useful in many different parts of the body. In this brief discussion, the following topics are considered: hepatic lesions, bleeding in early pregnancy, gynecological pathology (adnexal lesions), aortic aneurysms, thyroid nodules and scrotal masses. The usefulness of duplex carotid sonography, which combines ultrasonic <span class="hlt">imaging</span> and Doppler studies, is also discussed. Other topics (gallstones, biliary obstruction, renal calculi, hydronephrosis) are discussed in the appropriate sections. <span class="hlt">Images</span>Figure 1Figure 2Figure 3Figure 4 PMID:21267202</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA263679','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA263679"><span>Multisensor <span class="hlt">Image</span> Analysis System</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1993-04-15</p>
         <p>AD-A263 679 II Uli! 91 Multisensor <span class="hlt">Image</span> Analysis System Final Report Authors. Dr. G. M. Flachs Dr. Michael Giles Dr. Jay Jordan Dr. Eric...or decision, unless so designated by other documentation. 93-09739 *>ft s n~. now illlllM3lMVf Multisensor <span class="hlt">Image</span> Analysis System Final...Multisensor <span class="hlt">Image</span> Analysis System 3. REPORT TYPE AND DATES COVERED FINAL: LQj&tt-Z JZOfVL 5. FUNDING NUMBERS 93 > 6. AUTHOR(S) Drs. Gerald</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/28964545','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/28964545"><span>Diagnostic <span class="hlt">Imaging</span> of Discospondylitis.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Ruoff, Catherine M; Kerwin, Sharon C; Taylor, Amanda R</p>
         <p>2018-01-01</p>
         <p>Discospondylitis can affect dogs of any age and breed and may be seen in cats. Although radiography remains the gold standard, advanced <span class="hlt">imaging</span>, such as CT and MRI, has benefits and likely allows earlier diagnosis and identification of concurrent disease. Because discospondylitis may affect multiple disk spaces, <span class="hlt">imaging</span> of the entire spine should be considered. There is a lengthening list of causative etiologic agents, and successful treatment hinges on correct identification. <span class="hlt">Image</span>-guided biopsy should be considered in addition to blood and urine cultures and Brucella canis screening and as an alternative to surgical biopsy in some cases. Copyright © 2017 Elsevier Inc. All rights reserved.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/872917','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/872917"><span>Scanning computed confocal <span class="hlt">imager</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>George, John S.</p>
         <p>2000-03-14</p>
         <p>There is provided a confocal <span class="hlt">imager</span> comprising a light source emitting a light, with a light modulator in optical communication with the light source for varying the spatial and temporal pattern of the light. A beam splitter receives the scanned light and direct the scanned light onto a target and pass light reflected from the target to a video capturing device for receiving the reflected light and transferring a digital <span class="hlt">image</span> of the reflected light to a computer for creating a virtual aperture and outputting the digital <span class="hlt">image</span>. In a transmissive mode of operation the invention omits the beam splitter means and captures light passed through the target.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/6006759','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/biblio/6006759"><span>Nuclear medicine <span class="hlt">imaging</span> system</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Bennett, G.W.; Brill, A.B.; Bizais, Y.J.C.; Rowe, R.W.; Zubal, I.G.</p>
         <p>1983-03-11</p>
         <p>It is an object of this invention to provide a nuclear <span class="hlt">imaging</span> system having the versatility to do positron annihilation studies, rotating single or opposed camera gamma emission studies, and orthogonal gamma emission studies. It is a further object of this invention to provide an <span class="hlt">imaging</span> system having the capability for orthogonal dual multipinhole tomography. It is another object of this invention to provide a nuclear <span class="hlt">imaging</span> system in which all available energy data, as well as patient physiological data, are acquired simultaneously in list mode.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2788813','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2788813"><span>Prenatal <span class="hlt">Imaging</span>: Ultrasonography and Magnetic Resonance <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Reddy, Uma M.; Filly, Roy A.; Copel, Joshua A.</p>
         <p>2009-01-01</p>
         <p>The Eunice Kennedy Shriver National Institute of Child Health and Human Development held a workshop on September 18–19, 2006, to summarize the available evidence on the role and performance of current fetal <span class="hlt">imaging</span> technology and to establish a research agenda. Ultrasonography is the <span class="hlt">imaging</span> modality of choice for pregnancy evaluation due to its relatively low cost, real-time capability, safety, and operator comfort and experience. First-trimester ultrasonography extends the available window for fetal observation and raises the possibility of performing an early anatomic survey. Three-dimensional ultrasonography has the potential to expand the clinical application of ultrasonography by permitting local acquisition of volumes and remote review and interpretation at specialized centers. New advances allow performance of fetal magnetic resonance <span class="hlt">imaging</span> (MRI) without maternal or fetal sedation, with improved characterization and prediction of prognosis of certain fetal central nervous system anomalies such as ventriculomegaly when compared with ultrasonography. Fewer data exist on the usefulness of fetal MRI for non–central nervous system anomalies. PMID:18591320</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2014ascl.soft08009P','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2014ascl.soft08009P"><span>IIPImage: Large-<span class="hlt">image</span> visualization</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Pillay, Ruven</p>
         <p>2014-08-01</p>
         <p>IIPImage is an advanced high-performance feature-rich <span class="hlt">image</span> server system that enables online access to full resolution floating point (as well as other bit depth) <span class="hlt">images</span> at terabyte scales. Paired with the VisiOmatic (ascl:1408.010) celestial <span class="hlt">image</span> viewer, the system can comfortably handle gigapixel size <span class="hlt">images</span> as well as advanced <span class="hlt">image</span> features such as both 8, 16 and 32 bit depths, CIELAB colorimetric <span class="hlt">images</span> and scientific imagery such as multispectral <span class="hlt">images</span>. Streaming is tile-based, which enables viewing, navigating and zooming in real-time around gigapixel size <span class="hlt">images</span>. Source <span class="hlt">images</span> can be in either TIFF or JPEG2000 format. Whole <span class="hlt">images</span> or regions within <span class="hlt">images</span> can also be rapidly and dynamically resized and exported by the server from a single source <span class="hlt">image</span> without the need to store multiple files in various sizes.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5126695','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5126695"><span>Clinically Approved Nanoparticle <span class="hlt">Imaging</span> Agents</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Thakor, Avnesh S.; Jokerst, Jesse V.; Ghanouni, Pejman; Campbell, Jos L.; Mittra, Erik</p>
         <p>2016-01-01</p>
         <p>Nanoparticles are a new class of <span class="hlt">imaging</span> agent used for both anatomic and molecular <span class="hlt">imaging</span>. Nanoparticle-based <span class="hlt">imaging</span> exploits the signal intensity, stability, and biodistribution behavior of submicron-diameter molecular <span class="hlt">imaging</span> agents. This review focuses on nanoparticles used in human medical <span class="hlt">imaging</span>, with an emphasis on radionuclide <span class="hlt">imaging</span> and MRI. Newer nanoparticle platforms are also discussed in relation to theranostic and multimodal uses. PMID:27738007</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/21546690-quantum-ghost-imaging-through-turbulence','SCIGOV-STC'); return false;" href="https://www.osti.gov/biblio/21546690-quantum-ghost-imaging-through-turbulence"><span>Quantum ghost <span class="hlt">imaging</span> through turbulence</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Dixon, P. Ben; Howland, Gregory A.; Howell, John C.</p>
         <p>2011-05-15</p>
         <p>We investigate the effect of turbulence on quantum ghost <span class="hlt">imaging</span>. We use entangled photons and demonstrate that for a specific experimental configuration the effect of turbulence can be greatly diminished. By decoupling the entangled photon source from the ghost-<span class="hlt">imaging</span> central <span class="hlt">image</span> plane, we are able to dramatically increase the ghost-<span class="hlt">image</span> quality. When <span class="hlt">imaging</span> a test pattern through turbulence, this method increases the <span class="hlt">imaged</span> pattern visibility from V=0.15{+-}0.04 to 0.42{+-}0.04.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2018SPIE10615E..4LH','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2018SPIE10615E..4LH"><span>Color <span class="hlt">image</span> guided depth <span class="hlt">image</span> super resolution using fusion filter</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>He, Jin; Liang, Bin; He, Ying; Yang, Jun</p>
         <p>2018-04-01</p>
         <p>Depth cameras are currently playing an important role in many areas. However, most of them can only obtain lowresolution (LR) depth <span class="hlt">images</span>. Color cameras can easily provide high-resolution (HR) color <span class="hlt">images</span>. Using color <span class="hlt">image</span> as a guide <span class="hlt">image</span> is an efficient way to get a HR depth <span class="hlt">image</span>. In this paper, we propose a depth <span class="hlt">image</span> super resolution (SR) algorithm, which uses a HR color <span class="hlt">image</span> as a guide <span class="hlt">image</span> and a LR depth <span class="hlt">image</span> as input. We use the fusion filter of guided filter and edge based joint bilateral filter to get HR depth <span class="hlt">image</span>. Our experimental results on Middlebury 2005 datasets show that our method can provide better quality in HR depth <span class="hlt">images</span> both numerically and visually.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19730050106&hterms=photo+image&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D70%26Ntt%3Dphoto%2Bimage','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19730050106&hterms=photo+image&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D70%26Ntt%3Dphoto%2Bimage"><span><span class="hlt">Image</span> enhancement by holography.</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Stroke, G. W.</p>
         <p>1973-01-01</p>
         <p>The speed of the holographic <span class="hlt">image</span> deblurring method has recently been further enhanced by a new speed in the realization of the powerful holographic <span class="hlt">image</span>-deblurring filter. The filter makes it possible to carry out the deblurring, in the optical computer used, in times of the order of one second. The experimental achievements using the holographic <span class="hlt">image</span>-enhancement method are illustrated with examples ranging from out-of-focus or motion-blurred photographs, including 'amateur' photos recorded on Polaroid film, to the sharpening of the best available electron micrographs of viruses. <span class="hlt">Images</span> recorded with X-rays, notably from rocket-borne photos of the sun, and out-of-focus photographs from cameras in NASA satellites have been similarly deblurred.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/1956419','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/1956419"><span>International <span class="hlt">images</span>: business cards.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Gaston, S; Pucci, J</p>
         <p>1991-01-01</p>
         <p>Nursing specialists engage in a variety of international professional activities. Business cards are an important aspect of establishing a professional <span class="hlt">image</span>. This article presents recommended business card contents, international etiquette, card design and production, and cared innovations.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/ency/imagepages/1053.htm','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/ency/imagepages/1053.htm"><span>Candida, fluorescent stain (<span class="hlt">image</span>)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>... a fluorescent stain of Candida. Candida is a yeast (fungus) that causes mild disease, but in immunocompromised individuals it may cause life-threatening illness. (<span class="hlt">Image</span> courtesy of the Centers for ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2015SPIE.9496E..0HS','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2015SPIE.9496E..0HS"><span>Selective-<span class="hlt">imaging</span> camera</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Szu, Harold; Hsu, Charles; Landa, Joseph; Cha, Jae H.; Krapels, Keith A.</p>
         <p>2015-05-01</p>
         <p>How can we design cameras that <span class="hlt">image</span> selectively in Full Electro-Magnetic (FEM) spectra? Without selective <span class="hlt">imaging</span>, we cannot use, for example, ordinary tourist cameras to see through fire, smoke, or other obscurants contributing to creating a Visually Degraded Environment (VDE). This paper addresses a possible new design of selective-<span class="hlt">imaging</span> cameras at firmware level. The design is consistent with physics of the irreversible thermodynamics of Boltzmann's molecular entropy. It enables <span class="hlt">imaging</span> in appropriate FEM spectra for sensing through the VDE, and displaying in color spectra for Human Visual System (HVS). We sense within the spectra the largest entropy value of obscurants such as fire, smoke, etc. Then we apply a smart firmware implementation of Blind Sources Separation (BSS) to separate all entropy sources associated with specific Kelvin temperatures. Finally, we recompose the scene using specific RGB colors constrained by the HVS, by up/down shifting Planck spectra at each pixel and time.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=20040071092&hterms=SEWAGE&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D30%26Ntt%3DSEWAGE','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=20040071092&hterms=SEWAGE&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D30%26Ntt%3DSEWAGE"><span>Coastal Research <span class="hlt">Imaging</span> Spectrometer</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Lucey, Paul G.; Williams, Timothy; Horton, Keith A.</p>
         <p>2002-01-01</p>
         <p>The Coastal Research <span class="hlt">Imaging</span> Spectrometer (CRIS) is an airborne remote-sensing system designed specifically for research on the physical, chemical, and biological characteristics of coastal waters. The CRIS includes a visible-light hyperspectral <span class="hlt">imaging</span> subsystem for measuring the color of water, which contains information on the biota, sediment, and nutrient contents of the water. The CRIS also includes an infrared <span class="hlt">imaging</span> subsystem, which provides information on the temperature of the water. The combination of measurements enables investigation of biological effects of both natural and artificial flows of water from land into the ocean, including diffuse and point-source flows that may contain biological and/or chemical pollutants. Temperature is an important element of such measurements because temperature contrasts can often be used to distinguish among flows from different sources: for example, a sewage outflow could manifest itself in spectral <span class="hlt">images</span> as a local high-temperature anomaly.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_14");'>14</a></li>
      <li><a href="#" onclick='return showDiv("page_15");'>15</a></li>
      <li class="active"><span>16</span></li>
   <li><a href="#" onclick='return showDiv("page_17");'>17</a></li>
      <li><a href="#" onclick='return showDiv("page_18");'>18</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_16 -->
   <div id="page_17" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_15");'>15</a></li>
      <li><a href="#" onclick='return showDiv("page_16");'>16</a></li>
      <li class="active"><span>17</span></li>
   <li><a href="#" onclick='return showDiv("page_18");'>18</a></li>
      <li><a href="#" onclick='return showDiv("page_19");'>19</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="321">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20578.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20578.html"><span>Dawn LAMO <span class="hlt">Image</span> 83</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-05-06</p>
         <p>Ceres densely cratered landscape is revealed in this <span class="hlt">image</span> taken by the framing camera aboard NASA Dawn spacecraft. The craters show various degrees of degradation. The youngest craters have sharp rims.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20579.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20579.html"><span>Dawn LAMO <span class="hlt">Image</span> 84</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-05-09</p>
         <p>Ceres densely cratered landscape is revealed in this <span class="hlt">image</span> taken by the framing camera aboard NASA Dawn spacecraft. The craters show various degrees of degradation. The youngest craters have sharp rims.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2012PhyEd..47..342R','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2012PhyEd..47..342R"><span><span class="hlt">Images</span> in the air</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Riveros, H. G.; Rosenberger, Franz</p>
         <p>2012-05-01</p>
         <p>This article discusses two 'magic tricks' in terms of underlying optical principles. The first trick is new and produces a 'ghost' in the air, and the second is the classical real <span class="hlt">image</span> produced with two parabolic mirrors.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/ency/imagepages/9341.htm','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/ency/imagepages/9341.htm"><span>Gallbladder radionuclide scan (<span class="hlt">image</span>)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>... gallbladder radionuclide scan is performed by injecting a tracer (radioactive chemical) into the bloodstream. A gamma camera ... detect the gamma rays being emitted from the tracer, and the <span class="hlt">image</span> of where the tracer is ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2996670','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2996670"><span>Stimulated Raman photoacoustic <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Yakovlev, Vladislav V.; Zhang, Hao F.; Noojin, Gary D.; Denton, Michael L.; Thomas, Robert J.; Scully, Marlan O.</p>
         <p>2010-01-01</p>
         <p>Achieving label-free, molecular-specific <span class="hlt">imaging</span> with high spatial resolution in deep tissue is often considered the grand challenge of optical <span class="hlt">imaging</span>. To accomplish this goal, significant optical scattering in tissues has to be overcome while achieving molecular specificity without resorting to extrinsic labeling. We demonstrate the feasibility of developing such an optical <span class="hlt">imaging</span> modality by combining the molecularly specific stimulated Raman excitation with the photoacoustic detection. By employing two ultrashort excitation laser pulses, separated in frequency by the vibrational frequency of a targeted molecule, only the specific vibrational level of the target molecules in the illuminated tissue volume is excited. This targeted optical absorption generates ultrasonic waves (referred to as stimulated Raman photoacoustic waves) which are detected using a traditional ultrasonic transducer to form an <span class="hlt">image</span> following the design of the established photoacoustic microscopy. PMID:21059930</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/12095212','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/12095212"><span>Terahertz multistatic reflection <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Dorney, Timothy D; Symes, William W; Baraniuk, Richard G; Mittleman, Daniel M</p>
         <p>2002-07-01</p>
         <p>We describe a new <span class="hlt">imaging</span> method using single-cycle pulses of terahertz (THz) radiation. This technique emulates the data collection and <span class="hlt">image</span> processing procedures developed for geophysical prospecting and is made possible by the availability of fiber-coupled THz receiver antennas. We use a migration procedure to solve the inverse problem; this permits us to reconstruct the location, the shape, and the refractive index of targets. We show examples for both metallic and dielectric model targets, and we perform velocity analysis on dielectric targets to estimate the refractive indices of <span class="hlt">imaged</span> components. These results broaden the capabilities of THz <span class="hlt">imaging</span> systems and also demonstrate the viability of the THz system as a test bed for the exploration of new seismic processing methods.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/18089140','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/18089140"><span>Obesity and body <span class="hlt">image</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Schwartz, Marlene B; Brownell, Kelly D</p>
         <p>2004-01-01</p>
         <p>Modern western culture emphasizes thinness, denigrates excess weight, and stigmatizes obese individuals, making it likely that obese people internalize these messages and feel badly about the physical presence that brands them. There is clear evidence that obesity is linked with poor body <span class="hlt">image</span>, but not all obese persons suffer from this problem or are equally vulnerable. Risk factors identified thus far are degree of overweight, being female, and binge eating, with some evidence of risk increasing with early age of onset of obesity, race, and several additional factors. Treatments do exist for improving body <span class="hlt">image</span> in overweight individuals. Key questions are how to identify those in need of body <span class="hlt">image</span> intervention, how such programs can be integrated with weight loss treatments, and ultimately, how body <span class="hlt">image</span> distress can be prevented.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016SPIE.9854E..0ND','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016SPIE.9854E..0ND"><span>Handheld THz security <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Duling, Irl N.</p>
         <p>2016-05-01</p>
         <p>Terahertz energy, with its ability to penetrate clothing and non-conductive materials, has held much promise in the area of security scanning. Millimeter wave systems (300 GHz and below) have been widely deployed. These systems have used full two-dimensional surface <span class="hlt">imaging</span>, and have resulted in privacy concerns. Pulsed terahertz <span class="hlt">imaging</span>, can detect the presence of unwanted objects without the need for two-dimensional photographic <span class="hlt">imaging</span>. With high-speed waveform acquisition it is possible to create handheld tools that can be used to locate anomalies under clothing or headgear looking exclusively at either single point waveforms or cross-sectional <span class="hlt">images</span> which do not pose a privacy concern. Identification of the anomaly to classify it as a potential threat or a benign object is also possible.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19760000506&hterms=elec&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D60%26Ntt%3Delec','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19760000506&hterms=elec&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D60%26Ntt%3Delec"><span>Magnifying <span class="hlt">image</span> intensifier</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Vine, J.</p>
         <p>1977-01-01</p>
         <p>Coil assembly for zoom operation produces axial magnetic flux density that decreases in strength from photocathode to target. This results in magnification factor greater than unity. To extend magnification range, field is reversed in direction between object and <span class="hlt">image</span> planes.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20646.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20646.html"><span>Dawn LAMO <span class="hlt">Image</span> 106</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-06-09</p>
         <p>This <span class="hlt">image</span> from NASA Dawn spacecraft shows a portion of Ceres known as Erntedank Planum, a broad plateau 345 miles 555 kilometers wide. The terrain seen here lies just to the southeast of Occator Crater, home of Ceres brightest region.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/27072005','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/27072005"><span>Instrumentation in molecular <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Wells, R Glenn</p>
         <p>2016-12-01</p>
         <p>In vivo molecular <span class="hlt">imaging</span> is a challenging task and no single type of <span class="hlt">imaging</span> system provides an ideal solution. Nuclear medicine techniques like SPECT and PET provide excellent sensitivity but have poor spatial resolution. Optical <span class="hlt">imaging</span> has excellent sensitivity and spatial resolution, but light photons interact strongly with tissues and so only small animals and targets near the surface can be accurately visualized. CT and MRI have exquisite spatial resolution, but greatly reduced sensitivity. To overcome the limitations of individual modalities, molecular <span class="hlt">imaging</span> systems often combine individual cameras together, for example, merging nuclear medicine cameras with CT or MRI to allow the visualization of molecular processes with both high sensitivity and high spatial resolution.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20568.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20568.html"><span>Dawn LAMO <span class="hlt">Image</span> 73</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-04-22</p>
         <p>This <span class="hlt">image</span> from NASA Dawn spacecraft shows terrain within Chaminuka Crater on Ceres. Chaminuka was named for the spirit who provides rains during times of drought, according to the legends of the Shona people of Zimbabwe.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20315.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20315.html"><span>Dawn LAMO <span class="hlt">Image</span> 25</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-02-11</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows a densely cratered region within Meanderi Crater on Ceres. Elongated craters in the wall of the largest impact feature are likely the result of material slumping down the crater walls.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1998JHATD..19..107H','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1998JHATD..19..107H"><span>The NEAR Multispectral <span class="hlt">Imager</span>.</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Hawkins, S. E., III</p>
         <p>1998-06-01</p>
         <p>Multispectral <span class="hlt">Imager</span>, one of the primary instruments on the Near Earth Asteroid Rendezvous (NEAR) spacecraft, uses a five-element refractive optics telescope, an eight-position filter wheel, and a charge-coupled device detector to acquire <span class="hlt">images</span> over its sensitive wavelength range of ≍400 - 1100 nm. The primary science objectives of the Multispectral <span class="hlt">Imager</span> are to determine the morphology and composition of the surface of asteroid 433 Eros. The camera will have a critical role in navigating to the asteroid. Seven narrowband spectral filters have been selected to provide multicolor <span class="hlt">imaging</span> for comparative studies with previous observations of asteroids in the same class as Eros. The eighth filter is broadband and will be used for optical navigation. An overview of the instrument is presented, and design parameters and tradeoffs are discussed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20386.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20386.html"><span>Dawn LAMO <span class="hlt">Image</span> 32</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-02-23</p>
         <p>This <span class="hlt">image</span> of Ceres from NASA Dawn spacecraft was taken at an oblique viewing angle relative to the surface. The crater to the upper right is named Juling which displays prominent spurs of compacted material along its walls.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/ency/imagepages/1088.htm','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/ency/imagepages/1088.htm"><span>CT scan (<span class="hlt">image</span>)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>CT stands for computerized tomography. In this procedure, a thin X-ray beam is rotated around the ... D <span class="hlt">image</span> of a section through the body. CT scans are very detailed and provide excellent information ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/ency/imagepages/19900.htm','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/ency/imagepages/19900.htm"><span>Head CT (<span class="hlt">image</span>)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>CT stands for computerized tomography. In this procedure, a thin X-ray beam is rotated around the ... D <span class="hlt">image</span> of a section through the body. CT scans are very detailed and provide excellent information ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.radiologyinfo.org/en/info.cfm?pg=us-carotid&mobilebypass=1','NIH-MEDLINEPLUS'); return false;" href="https://www.radiologyinfo.org/en/info.cfm?pg=us-carotid&mobilebypass=1"><span>Carotid Ultrasound <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>... the patient. Because ultrasound <span class="hlt">images</span> are captured in real-time, they can show the structure and movement of ... by a computer, which in turn creates a real-time picture on the monitor. One or more frames ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/20523607','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/20523607"><span>Restoration of longitudinal <span class="hlt">images</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Hu, Y; Frieden, B R</p>
         <p>1988-01-15</p>
         <p>In this paper, a method of restoring longitudinal <span class="hlt">images</span> is developed. By using the transfer function for longitudinal objects, and inverse filtering, a longitudinal <span class="hlt">image</span> may be restored. The Fourier theory and sampling theorems for transverse <span class="hlt">images</span> cannot be used directly in the longitudinal case. A modification and reasonable approximation are introduced. We have numerically established a necessary relationship between just-resolved longitudinal separation (after inverse filtering), noise level, and the taking conditions of object distance and lens diameter. An empirical formula is also found to well-fit the computed results. This formula may be of use for designing optical systems which are to <span class="hlt">image</span> longitudinal details, such as in robotics or microscopy.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/5680838-thyroid-parathyroid-imaging','SCIGOV-STC'); return false;" href="https://www.osti.gov/biblio/5680838-thyroid-parathyroid-imaging"><span>Thyroid and parathyroid <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Sandler, M.P.; Patton, J.A.; Partain, C.L.</p>
         <p>1986-01-01</p>
         <p>This book describes the numerous modalities currently used in the diagnosis and treatment of both thyroid and parathyroid disorders. Each modality is fully explained and then evaluated in terms of benefits and limitations in the clinical context. Contents: Production and Quality Control of Radiopharmaceutics Used for Diagnosis and Therapy in Thyroid and Parathyroid Disorders. Basic Physics. Nuclear Instrumentation. Radioimmunoassay: Thyroid Function Tests. Quality Control. Embryology, Anatomy, Physiology, and Thyroid Function Studies. Scintigraphic Thyroid <span class="hlt">Imaging</span>. Neonatal and Pediatric Thyroid <span class="hlt">Imaging</span>. Radioiodine Thyroid Uptake Measurement. Radioiodine Treatment of Thyroid Disorders. Radiation Dosimetry of Diagnostic Procedures. Radiation Safety Procedures for High-Level I-131 Therapies.more » X-Ray Fluorescent Scanning. Thyroid Sonography. Computed Tomography in Thyroid Disease. Magnetic Resonance <span class="hlt">Imaging</span> in Thyroid Disease. Parathyroid <span class="hlt">Imaging</span>.« less</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_15");'>15</a></li>
      <li><a href="#" onclick='return showDiv("page_16");'>16</a></li>
      <li class="active"><span>17</span></li>
   <li><a href="#" onclick='return showDiv("page_18");'>18</a></li>
      <li><a href="#" onclick='return showDiv("page_19");'>19</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_17 -->
   <div id="page_18" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_16");'>16</a></li>
      <li><a href="#" onclick='return showDiv("page_17");'>17</a></li>
      <li class="active"><span>18</span></li>
   <li><a href="#" onclick='return showDiv("page_19");'>19</a></li>
      <li><a href="#" onclick='return showDiv("page_20");'>20</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="341">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20667.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20667.html"><span>Dawn LAMO <span class="hlt">Image</span> 87</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-05-12</p>
         <p>This <span class="hlt">image</span> from NASA Dawn spacecraft shows the rim of Occator crater, just east of the area containing the brightest spots on Ceres. The crater rim has collapsed, leaving structures geologists refer to as terraces.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/ency/imagepages/9987.htm','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/ency/imagepages/9987.htm"><span>Transvaginal ultrasound (<span class="hlt">image</span>)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>Transvaginal ultrasound is a method of <span class="hlt">imaging</span> the genital tract in females. A hand held probe is inserted directly ... vaginal cavity to scan the pelvic structures, while ultrasound pictures are viewed on a monitor. The test ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/ency/imagepages/1058.htm','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/ency/imagepages/1058.htm"><span>Abdominal ultrasound (<span class="hlt">image</span>)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>Abdominal ultrasound is a scanning technique used to <span class="hlt">image</span> the interior of the abdomen. Like the X-ray, MRI, ... it has its place as a diagnostic tool. Ultrasound scans use high frequency sound waves to produce ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/ency/imagepages/1005.htm','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/ency/imagepages/1005.htm"><span>Roundworm eggs - ascariasis (<span class="hlt">image</span>)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>Roundworms are the most common type of worm infection. It is estimated that there are 4,000, ... soil. Ingestion of contaminated soil then leads to roundworm infection. (<span class="hlt">Image</span> courtesy of the Centers for Disease ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5470636','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5470636"><span>Mirror <span class="hlt">Image</span> Proteins</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Zhao, Le; Lu, Wuyuan</p>
         <p>2017-01-01</p>
         <p>Proteins composed entirely of unnatural D-amino acids and the achiral amino acid glycine are mirror <span class="hlt">image</span> forms of their native L-protein counterparts. Recent advances in chemical protein synthesis afford unique and facile synthetic access to domain-sized mirror <span class="hlt">image</span> D-proteins, enabling protein research to be conducted through “the looking glass” and in a way previously unattainable. D-proteins can facilitate structure determination of their native L-forms that are difficult to crystallize (racemic X-ray crystallography); D-proteins can serve as the bait for library screening to ultimately yield pharmacologically superior D-peptide/D-protein therapeutics (mirror <span class="hlt">image</span> phage display); D-proteins can also be used as a powerful mechanistic tool for probing molecular events in biology. This review examines recent progress in the application of mirror <span class="hlt">image</span> proteins to structural biology, drug discovery, and immunology. PMID:25282524</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20314.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20314.html"><span>Dawn LAMO <span class="hlt">Image</span> 24</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-02-10</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft, shows the heavily cratered rim of an older, unnamed impact feature on Ceres. The crater density is almost the same inside and outside, and its wall is also quite battered by impacts.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/908346','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/908346"><span>Photothermal <span class="hlt">imaging</span> scanning microscopy</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Chinn, Diane [Pleasanton, CA; Stolz, Christopher J [Lathrop, CA; Wu, Zhouling [Pleasanton, CA; Huber, Robert [Discovery Bay, CA; Weinzapfel, Carolyn [Tracy, CA</p>
         <p>2006-07-11</p>
         <p>Photothermal <span class="hlt">Imaging</span> Scanning Microscopy produces a rapid, thermal-based, non-destructive characterization apparatus. Also, a photothermal characterization method of surface and subsurface features includes micron and nanoscale spatial resolution of meter-sized optical materials.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.sil.si.edu/imagegalaxy/imageGalaxy_SearchBooks.cfm','SCIGOVWS'); return false;" href="http://www.sil.si.edu/imagegalaxy/imageGalaxy_SearchBooks.cfm"><span>Search | Galaxy of <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.science.gov/aboutsearch.html">Science.gov Websites</a></p>
      <p></p>
         <p></p>
         <p><em>Books</em> dot header Search Tips Search Keywords in Author Last Name or in the Title of the <em>Books</em>: Enter a <em>books</em> <span class="hlt">Images</span> FAQ | Privacy | SI Terms of Use | Smithsonian Home DCSIMG</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016APS..MARS12004F','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016APS..MARS12004F"><span><span class="hlt">Imaging</span> and Analytics: The changing face of Medical <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Foo, Thomas</p>
         <p></p>
         <p>There have been significant technological advances in <span class="hlt">imaging</span> capability over the past 40 years. Medical <span class="hlt">imaging</span> capabilities have developed rapidly, along with technology development in computational processing speed and miniaturization. Moving to all-digital, the number of <span class="hlt">images</span> that are acquired in a routine clinical examination has increased dramatically from under 50 <span class="hlt">images</span> in the early days of CT and MRI to more than 500-1000 <span class="hlt">images</span> today. The staggering number of <span class="hlt">images</span> that are routinely acquired poses significant challenges for clinicians to interpret the data and to correctly identify the clinical problem. Although the time provided to render a clinical finding has not substantially changed, the amount of data available for interpretation has grown exponentially. In addition, the <span class="hlt">image</span> quality (spatial resolution) and information content (physiologically-dependent <span class="hlt">image</span> contrast) has also increased significantly with advances in medical <span class="hlt">imaging</span> technology. On its current trajectory, medical <span class="hlt">imaging</span> in the traditional sense is unsustainable. To assist in filtering and extracting the most relevant data elements from medical <span class="hlt">imaging</span>, <span class="hlt">image</span> analytics will have a much larger role. Automated <span class="hlt">image</span> segmentation, generation of parametric <span class="hlt">image</span> maps, and clinical decision support tools will be needed and developed apace to allow the clinician to manage, extract and utilize only the information that will help improve diagnostic accuracy and sensitivity. As medical <span class="hlt">imaging</span> devices continue to improve in spatial resolution, functional and anatomical information content, <span class="hlt">image</span>/data analytics will be more ubiquitous and integral to medical <span class="hlt">imaging</span> capability.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/AD1005118','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/AD1005118"><span>Tinnitus Multimodal <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2015-10-01</p>
         <p>AWARD NUMBER: W81XWH-13-1-0494 TITLE: Tinnitus Multimodal <span class="hlt">Imaging</span> PRINCIPAL INVESTIGATOR: Steven Wan Cheung CONTRACTING ORGANIZATION...NUMBER W81XWH-13-1-0494 Tinnitus Multimodal <span class="hlt">Imaging</span> 5b. GRANT NUMBER 5c. PROGRAM ELEMENT NUMBER 6. AUTHOR(S) 5d. PROJECT NUMBER Steven W. Cheung...13. SUPPLEMENTARY NOTES 14. ABSTRACT Tinnitus is a common auditory perceptual disorder whose neural substrates are under intense debate. This project</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/27364431','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/27364431"><span>Oncological <span class="hlt">image</span> analysis.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Brady, Sir Michael; Highnam, Ralph; Irving, Benjamin; Schnabel, Julia A</p>
         <p>2016-10-01</p>
         <p>Cancer is one of the world's major healthcare challenges and, as such, an important application of medical <span class="hlt">image</span> analysis. After a brief introduction to cancer, we summarise some of the major developments in oncological <span class="hlt">image</span> analysis over the past 20 years, but concentrating those in the authors' laboratories, and then outline opportunities and challenges for the next decade. Copyright © 2016 Elsevier B.V. All rights reserved.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3086609','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3086609"><span>Integrin Targeted MR <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Tan, Mingqian; Lu, Zheng-Rong</p>
         <p>2011-01-01</p>
         <p>Magnetic resonance <span class="hlt">imaging</span> (MRI) is a powerful medical diagnostic <span class="hlt">imaging</span> modality for integrin targeted <span class="hlt">imaging</span>, which uses the magnetic resonance of tissue water protons to display tissue anatomic structures with high spatial resolution. Contrast agents are often used in MRI to highlight specific regions of the body and make them easier to visualize. There are four main classes of MRI contrast agents based on their different contrast mechanisms, including T1, T2, chemical exchange saturation transfer (CEST) agents, and heteronuclear contrast agents. Integrins are an important family of heterodimeric transmembrane glycoproteins that function as mediators of cell-cell and cell-extracellular matrix interactions. The overexpressed integrins can be used as the molecular targets for designing suitable integrin targeted contrast agents for MR molecular <span class="hlt">imaging</span>. Integrin targeted contrast agent includes a targeting agent specific to a target integrin, a paramagnetic agent and a linker connecting the targeting agent with the paramagnetic agent. Proper selection of targeting agents is critical for targeted MRI contrast agents to effectively bind to integrins for in vivo <span class="hlt">imaging</span>. An ideal integrin targeted MR contrast agent should be non-toxic, provide strong contrast enhancement at the target sites and can be completely excreted from the body after MR <span class="hlt">imaging</span>. An overview of integrin targeted MR contrast agents based on small molecular and macromolecular Gd(III) complexes, lipid nanoparticles and superparamagnetic nanoparticles is provided for MR molecular <span class="hlt">imaging</span>. By using proper delivery systems for loading sufficient Gd(III) chelates or superparamagnetic nanoparticles, effective molecular <span class="hlt">imaging</span> of integrins with MRI has been demonstrated in animal models. PMID:21547154</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2012SPIE.8374E..0EH','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2012SPIE.8374E..0EH"><span>Thermal hyperspectral chemical <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Holma, Hannu; Hyvärinen, Timo; Mattila, Antti-Jussi; Kormano, Ilkka</p>
         <p>2012-06-01</p>
         <p>Several chemical compounds have their strongest spectral signatures in the thermal region. This paper presents three push-broom thermal hyperspectral <span class="hlt">imagers</span>. The first operates in MWIR (2.8-5 μm) with 35 nm spectral resolution. It consists of uncooled <span class="hlt">imaging</span> spectrograph and cryogenically cooled InSb camera, with spatial resolution of 320/640 pixels and <span class="hlt">image</span> rate to 400 Hz. The second <span class="hlt">imager</span> covers LWIR in 7.6-12 μm with 32 spectral bands. It employs an uncooled microbolometer array and spectrograph. These <span class="hlt">imagers</span> have been designed for chemical mapping in reflection mode in industry and laboratory. An efficient line-illumination source has been developed, and it makes possible thermal hyperspectral <span class="hlt">imaging</span> in reflection with much higher signal and SNR than is obtained from room temperature emission. Application demonstrations including sorting of dark plastics and mineralogical mapping of drill cores are presented. The third <span class="hlt">imager</span> utilizes a cryo-cooled MCT array with precisely temperature stabilized optics. The optics is not cooled, but instrument radiation is suppressed by special filtering and corrected by BMC (Background-Monitoring-on-Chip) method. The approach provides excellent sensitivity in an instrument which is portable and compact enough for installation in UAVs. The <span class="hlt">imager</span> has been verified in 7.6 to 12.3 μm to provide NESR of 18 mW/(m2 sr μm) at 10 μm for 300 K target with 100 spectral bands and 384 spatial samples. It results in SNR of higher than 500. The performance makes possible various applications from gas detection to mineral exploration and vegetation surveys. Results from outdoor and airborne experiments are shown.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA453941','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA453941"><span>Terahertz (THZ) <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2006-03-01</p>
         <p>work in <span class="hlt">image</span> processing for CWD and other security-related <span class="hlt">imaging</span> with visual, x - ray , infrared and millimeter wave imagery was seen as a jumping-off...advantage of the fact that, unlike x - rays which offer only magnitude information, THz offers phase information. as well. While the magnitude contains...perspective are analyzed, specially compared with X - ray process tomography system. 5. Gregory, I.S.; Tribe, W.R.; Cole, B.E.; Baker, C.; Evans, M.J</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20937.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20937.html"><span>Dawn LAMO <span class="hlt">Image</span> 175</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-09-20</p>
         <p>NASA's Dawn spacecraft obtained this view of Laukumate Crater (19 miles, 30 kilometers wide) on Ceres on June 2, 2016. Laukumate is named for a Latvian goddess of agriculture. Dawn took this <span class="hlt">image</span> from its low-altitude mapping orbit, at a distance of about 240 miles (385 kilometers) above the surface. The <span class="hlt">image</span> resolution is 120 feet (35 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20937</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20947.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20947.html"><span>Dawn LAMO <span class="hlt">Image</span> 185</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-10-04</p>
         <p>NASA's Dawn spacecraft spies Achita Crater on Ceres in this view. Achita is named for a Nigerian god of agriculture and is 25 miles (40 kilometers) wide. Dawn took this <span class="hlt">image</span> on June 3, 2016, from its low-altitude mapping orbit, at a distance of about 240 miles (385 kilometers) above the surface. The <span class="hlt">image</span> resolution is 120 feet (35 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20947</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/872814','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/872814"><span>Reflective optical <span class="hlt">imaging</span> system</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Shafer, David R.</p>
         <p>2000-01-01</p>
         <p>An optical system compatible with short wavelength (extreme ultraviolet) radiation comprising four reflective elements for projecting a mask <span class="hlt">image</span> onto a substrate. The four optical elements are characterized in order from object to <span class="hlt">image</span> as convex, concave, convex and concave mirrors. The optical system is particularly suited for step and scan lithography methods. The invention increases the slit dimensions associated with ringfield scanning optics, improves wafer throughput and allows higher semiconductor device density.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4023107','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4023107"><span>Retinal <span class="hlt">imaging</span> in uveitis</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Gupta, Vishali; Al-Dhibi, Hassan A.; Arevalo, J. Fernando</p>
         <p>2014-01-01</p>
         <p>Ancillary investigations are the backbone of uveitis workup for posterior segment inflammations. They help in establishing the differential diagnosis and making certain diagnosis by ruling out certain pathologies and are a useful aid in monitoring response to therapy during follow-up. These investigations include fundus photography including ultra wide field angiography, fundus autofluorescence <span class="hlt">imaging</span>, fluorescein angiography, optical coherence tomography and multimodal <span class="hlt">imaging</span>. This review aims to be an overview describing the role of these retinal investigations for posterior uveitis. PMID:24843301</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19992.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19992.html"><span>Dawn HAMO <span class="hlt">Image</span> 50</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-11-02</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows a portion of the southern hemisphere of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span> was taken on Sept. 28, 2015, and has a resolution of 450 feet (140 meters) per pixel. Urvara crater, named for the Indian and Iranian deity of plants and fields, is featured. Its diameter is 101 miles (163 kilometers). http://photojournal.jpl.nasa.gov/catalog/PIA19992</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20195.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20195.html"><span>Dawn LAMO <span class="hlt">Image</span> 5</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-01-13</p>
         <p>This view of the Cerean crater Victa was captured by NASA Dawn spacecraft on Dec. 19, 2015. The steep-walled crater is approximately 19 miles 30 kilometers in diameter, and was named for the Roman goddess of food and nourishment. Dawn took this <span class="hlt">image</span> from its low-altitude mapping orbit (LAMO), at an approximate altitude of 240 miles (385 kilometers) above Ceres. The <span class="hlt">image</span> resolution is 120 feet (35 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20195</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_16");'>16</a></li>
      <li><a href="#" onclick='return showDiv("page_17");'>17</a></li>
      <li class="active"><span>18</span></li>
   <li><a href="#" onclick='return showDiv("page_19");'>19</a></li>
      <li><a href="#" onclick='return showDiv("page_20");'>20</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_18 -->
   <div id="page_19" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_17");'>17</a></li>
      <li><a href="#" onclick='return showDiv("page_18");'>18</a></li>
      <li class="active"><span>19</span></li>
   <li><a href="#" onclick='return showDiv("page_20");'>20</a></li>
      <li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="361">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA096863','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA096863"><span>Techniques for Microwave <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1981-01-18</p>
         <p>reduce cross-range sidelobes in tht subsequent -&#8217 FT and the array was padd ,,d with 64 additional r,wis containing zeros . The configuration of the array is...of microwave imagery obtained by synthetic aperture processing described in reference 1-2. This type of <span class="hlt">image</span>. generated by processing radar data...1,000 wavelengths. Althouigh these are the intended applications, the <span class="hlt">imaging</span> methods con- sidered have general applicability to environments outside</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20944.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20944.html"><span>Dawn LAMO <span class="hlt">Image</span> 182</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2016-09-29</p>
         <p>NASA's Dawn spacecraft views Kupalo Crater in this view of Ceres. Kupalo, which measures 16 miles (26 kilometers) across and is located at southern mid-latitudes, is named for the Slavic god of vegetation and harvest. Dawn took this <span class="hlt">image</span> on June 2, 2016, from its low-altitude mapping orbit, at a distance of about 240 miles (385 kilometers) above the surface. The <span class="hlt">image</span> resolution is 120 feet (35 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20944</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19989.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19989.html"><span>Dawn HAMO <span class="hlt">Image</span> 47</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-28</p>
         <p>This <span class="hlt">image</span>, taken by NASA's Dawn spacecraft, shows a portion of the northern hemisphere of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The <span class="hlt">image</span> was taken on Sept. 22, 2015, and has a resolution of 450 feet (140 meters) per pixel. Jarovit crater, named for the Slavic god of fertility and harvest, is seen at lower left. Its diameter is 41 miles (66 kilometers). http://photojournal.jpl.nasa.gov/catalog/PIA19989</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA607157','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA607157"><span>Photoacoustic <span class="hlt">Imaging</span> of Epilepsy</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2014-04-01</p>
         <p>with the skin and skull intact. MCA, middle cerebral artery; RH, right hemispheres; LH, left hemispheres; LOB, left olfactory bulbs; ROB, Right...moving rat brain with skin and skull intact. (D) Open-skull photograph of the rat cortex surface after the PAT experiments The PAT detecting...22D shows a typical non-invasive PAT <span class="hlt">image</span> obtained with the miniature PAT <span class="hlt">imaging</span> system of a freely moving rat brain with skin and skull intact. Fig</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA017765','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA017765"><span><span class="hlt">Image</span> Analysis and Modeling</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1975-08-01</p>
         <p><span class="hlt">image</span> analysis and processing tasks such as information extraction, <span class="hlt">image</span> enhancement and restoration, coding, etc. The ultimate objective of this research is to form a basis for the development of technology relevant to military applications of machine extraction of information from aircraft and satellite imagery of the earth&#8217s surface. This report discusses research activities during the three month period February 1 - April 30,</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/27748707','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/27748707"><span>Cellular <span class="hlt">Imaging</span> With MRI.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Makela, Ashley V; Murrell, Donna H; Parkins, Katie M; Kara, Jenna; Gaudet, Jeffrey M; Foster, Paula J</p>
         <p>2016-10-01</p>
         <p>Cellular magnetic resonance <span class="hlt">imaging</span> (MRI) is an evolving field of <span class="hlt">imaging</span> with strong translational and research potential. The ability to detect, track, and quantify cells in vivo and over time allows for studying cellular events related to disease processes and may be used as a biomarker for decisions about treatments and for monitoring responses to treatments. In this review, we discuss methods for labeling cells, various applications for cellular MRI, the existing limitations, strategies to address these shortcomings, and clinical cellular MRI.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19910000634&hterms=wake+forest&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D20%26Ntt%3Dwake%2Bforest','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19910000634&hterms=wake+forest&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D20%26Ntt%3Dwake%2Bforest"><span>Spaceborne Microwave <span class="hlt">Imagers</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Stacey, J. M.</p>
         <p>1991-01-01</p>
         <p>Monograph presents comprehensive overview of science and technology of spaceborne microwave-<span class="hlt">imaging</span> systems. Microwave <span class="hlt">images</span> used as versatile orbiting, remote-sensing systems to investigate atmospheres and surfaces of planets. Detect surface objects through canopies of clouds, measure distributions of raindrops in clouds that their views penetrate, find meandering rivers in rain forests and underground water in arid regions, and provide information on ocean currents, wakes, ice/water boundaries, aircraft, ships, buoys, and bridges.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/24217100','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/24217100"><span>MRI brain <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Skinner, Sarah</p>
         <p>2013-11-01</p>
         <p>General practitioners (GPs) are expected to be allowed to request MRI scans for adults for selected clinically appropriate indications from November 2013 as part of the expansion of Medicare-funded MRI services announced by the Federal Government in 2011. This article aims to give a brief overview of MRI brain <span class="hlt">imaging</span> relevant to GPs, which will facilitate explanation of scan findings and management planning with their patients. Basic <span class="hlt">imaging</span> techniques, common findings and terminology are presented using some illustrative case examples.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA270596','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA270596"><span><span class="hlt">Image</span> Understanding Architecture</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1991-09-01</p>
         <p>architecture to support real-time, knowledge -based <span class="hlt">image</span> understanding , and develop the software support environment that will be needed to utilize...NUMBER OF PAGES <span class="hlt">Image</span> Understanding Architecture, Knowledge -Based Vision, AI Real-Time Computer Vision, Software Simulator, Parallel Processor IL PRICE... information . In addition to sensory and knowledge -based processing it is useful to introduce a level of symbolic processing. Thus, vision researchers</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1326791','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1326791"><span><span class="hlt">Image</span> processing occupancy sensor</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Brackney, Larry J.</p>
         <p>2016-09-27</p>
         <p>A system and method of detecting occupants in a building automation system environment using <span class="hlt">image</span> based occupancy detection and position determinations. In one example, the system includes an <span class="hlt">image</span> processing occupancy sensor that detects the number and position of occupants within a space that has controllable building elements such as lighting and ventilation diffusers. Based on the position and location of the occupants, the system can finely control the elements to optimize conditions for the occupants, optimize energy usage, among other advantages.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADB286590','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADB286590"><span>Ultrasound <span class="hlt">Imaging</span> Initiative</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2003-01-01</p>
         <p>texture mapping hardware,&#34 IEEE Tranactions on Information Technology in Biomedicine, Submitted. [14] C.R. Castro Pareja , J.M. Jagadeesh and R. Shekhar...modulation in real-time three-dimensional sparse synthetic aperture ultrasound <span class="hlt">imaging</span> systems &#34* Carlos R. Castro Pareja , Masters of Science, The Ohio...C.R. Castro Pareja , &#34An architecture for real-time <span class="hlt">image</span> registration,&#34 M.S. Thesis, The Ohio State University, March 2002. 14. C.R. Castro Pareja , R</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA110746','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA110746"><span><span class="hlt">Image</span> Understanding Research</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1981-09-30</p>
         <p>to perform a variety of local arithmetic operations. Our initial task will be to use it for computing 5X5 convolutions common to many low level...report presents the results of applying our relaxation based scene matching systein I1] to a new domain - automatic matching of pairs of <span class="hlt">images</span>. The task...objects (corners of buildings) within the large <span class="hlt">image</span>. But we did demonstrate the ability of our system to automatically segment, describe, and match</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/5605738-diagnostic-imaging-infertility','SCIGOV-STC'); return false;" href="https://www.osti.gov/biblio/5605738-diagnostic-imaging-infertility"><span>Diagnostic <span class="hlt">imaging</span> of infertility</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Winfield, A.C.; Wentz, A.C.</p>
         <p>1987-01-01</p>
         <p>This text presents a review of all the <span class="hlt">imaging</span> modalities available in the diagnosis of infertility. This book integrates the perspectives of experts in ob/gyn, radiology, reproductive endocrinology, and urology. It's a one-of-a-kind ''how to'' guide to hysterosalpinography and infertility evaluation, providing complete clinical information on the techniques, pitfalls, problems encountered and differential diagnosis. Detailed descriptions accompany numerous high-quality illustrations to help correlate findings and give meaning to the radiographic and ultrasound <span class="hlt">images</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2003JGRE..108.8065H','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2003JGRE..108.8065H"><span>Athena Microscopic <span class="hlt">Imager</span> investigation</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Herkenhoff, K. E.; Squyres, S. W.; Bell, J. F.; Maki, J. N.; Arneson, H. M.; Bertelsen, P.; Brown, D. I.; Collins, S. A.; Dingizian, A.; Elliott, S. T.; Goetz, W.; Hagerott, E. C.; Hayes, A. G.; Johnson, M. J.; Kirk, R. L.; McLennan, S.; Morris, R. V.; Scherr, L. M.; Schwochert, M. A.; Shiraishi, L. R.; Smith, G. H.; Soderblom, L. A.; Sohl-Dickstein, J. N.; Wadsworth, M. V.</p>
         <p>2003-11-01</p>
         <p>The Athena science payload on the Mars Exploration Rovers (MER) includes the Microscopic <span class="hlt">Imager</span> (MI). The MI is a fixed-focus camera mounted on the end of an extendable instrument arm, the Instrument Deployment Device (IDD). The MI was designed to acquire <span class="hlt">images</span> at a spatial resolution of 30 microns/pixel over a broad spectral range (400-700 nm). The MI uses the same electronics design as the other MER cameras but has optics that yield a field of view of 31 × 31 mm across a 1024 × 1024 pixel CCD <span class="hlt">image</span>. The MI acquires <span class="hlt">images</span> using only solar or skylight illumination of the target surface. A contact sensor is used to place the MI slightly closer to the target surface than its best focus distance (about 66 mm), allowing concave surfaces to be <span class="hlt">imaged</span> in good focus. Coarse focusing (~2 mm precision) is achieved by moving the IDD away from a rock target after the contact sensor has been activated. The MI optics are protected from the Martian environment by a retractable dust cover. The dust cover includes a Kapton window that is tinted orange to restrict the spectral bandpass to 500-700 nm, allowing color information to be obtained by taking <span class="hlt">images</span> with the dust cover open and closed. MI data will be used to place other MER instrument data in context and to aid in petrologic and geologic interpretations of rocks and soils on Mars.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016SPIE10013E..0KL','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016SPIE10013E..0KL"><span>High speed multiphoton <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Li, Yongxiao; Brustle, Anne; Gautam, Vini; Cockburn, Ian; Gillespie, Cathy; Gaus, Katharina; Lee, Woei Ming</p>
         <p>2016-12-01</p>
         <p>Intravital multiphoton microscopy has emerged as a powerful technique to visualize cellular processes in-vivo. Real time processes revealed through live <span class="hlt">imaging</span> provided many opportunities to capture cellular activities in living animals. The typical parameters that determine the performance of multiphoton microscopy are speed, field of view, 3D <span class="hlt">imaging</span> and <span class="hlt">imaging</span> depth; many of these are important to achieving data from in-vivo. Here, we provide a full exposition of the flexible polygon mirror based high speed laser scanning multiphoton <span class="hlt">imaging</span> system, PCI-6110 card (National Instruments) and high speed analog frame grabber card (Matrox Solios eA/XA), which allows for rapid adjustments between frame rates i.e. 5 Hz to 50 Hz with 512 × 512 pixels. Furthermore, a motion correction algorithm is also used to mitigate motion artifacts. A customized control software called Pscan 1.0 is developed for the system. This is then followed by calibration of the <span class="hlt">imaging</span> performance of the system and a series of quantitative in-vitro and in-vivo <span class="hlt">imaging</span> in neuronal tissues and mice.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA05381.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA05381.html"><span>Saturn Methane <span class="hlt">Image</span></span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2004-03-05</p>
         <p>NASA's Cassini narrow angle camera took this <span class="hlt">image</span> of Saturn on Feb. 16, 2004, from a distance of 66.1 million kilometers (41.1 million miles) in a special filter which reveals clouds and haze high in the atmosphere. The <span class="hlt">image</span> scale is 397 kilometers (247 miles) per pixel. The MT2 spectral filter samples a near-infrared region of the electromagnetic spectrum where methane gas absorbs light at a wavelength of 727 nanometers. In the <span class="hlt">image</span>, methane gas is uniformly mixed with hydrogen, the main gas in Saturn's atmosphere. Dark locales are places of strong methane absorption, relatively free of high clouds; the bright areas are places with high, thick clouds which shield the methane below. <span class="hlt">Image</span> details reveal a high, thick equatorial cloud and a relatively deep or thin haze encircling the pole, as well as several distinct latitude bands with different cloud height attributes. It also shows a high atmospheric disturbance, just south of the equator, which has persisted throughout the 1990s in <span class="hlt">images</span> returned by NASA's Hubble Space Telescope. Four of Saturn's moons are visible (clockwise from above right): Enceladus (499 kilometers, or 310 miles across); Mimas (396 kilometers, or 245 miles across); Tethys (1,060 kilometers, or 659 miles across); and Rhea (1,528 kilometers, or 949 miles across). The <span class="hlt">imaging</span> team enhanced the brightness of Mimas and Enceladus by a factor of three. http://photojournal.jpl.nasa.gov/catalog/PIA05381</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20020086977','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20020086977"><span>Cardiac <span class="hlt">Imaging</span> System</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1990-01-01</p>
         <p>Although not available to all patients with narrowed arteries, balloon angioplasty has expanded dramatically since its introduction with an estimated further growth to 562,000 procedures in the U.S. alone by 1992. Growth has fueled demand for higher quality <span class="hlt">imaging</span> systems that allow the cardiologist to be more accurate and increase the chances of a successful procedure. A major advance is the Digital Cardiac <span class="hlt">Imaging</span> (DCI) System designed by Philips Medical Systems International, Best, The Netherlands and marketed in the U.S. by Philips Medical Systems North America Company. The key benefit is significantly improved real-time <span class="hlt">imaging</span> and the ability to employ <span class="hlt">image</span> enhancement techniques to bring out added details. Using a cordless control unit, the cardiologist can manipulate <span class="hlt">images</span> to make immediate assessment, compare live x-ray and roadmap <span class="hlt">images</span> by placing them side-by-side on monitor screens, or compare pre-procedure and post procedure conditions. The Philips DCI improves the cardiologist's precision by expanding the information available to him.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/29602728','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/29602728"><span><span class="hlt">Imaging</span> the ovary.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Feng, Yi; Tamadon, Amin; Hsueh, Aaron J W</p>
         <p>2018-05-01</p>
         <p>During each reproductive cycle, the ovary exhibits tissue remodelling and cyclic vasculature changes associated with hormonally regulated folliculogenesis, follicle rupture, luteal formation and regression. However, the relationships among different types of follicles and corpora lutea are unclear, and the role of ovarian vasculature in folliculogenesis and luteal dynamics has not been extensively investigated. Understanding of ovarian physiology and pathophysiology relies upon elucidation of ovarian morphology and architecture. This paper summarizes the literature on traditional approaches to the <span class="hlt">imaging</span> of ovarian structures and discusses recent advances in ovarian <span class="hlt">imaging</span>. Traditional in-vivo ultrasound, together with histological and electron microscopic approaches provide detailed views of the ovary at organ, tissue and molecular levels. However, in-vivo <span class="hlt">imaging</span> is limited to antral and larger follicles whereas histological <span class="hlt">imaging</span> is mainly two-dimensional in nature. Also discussed are emerging approaches in the use of near-infrared fluorophores to <span class="hlt">image</span> follicles in live animals to detect preantral follicles as well as visualizing ovarian structures using CLARITY in fixed whole ovaries to elucidate three-dimensional interrelationships among follicles, corpora lutea and ovarian vasculature. Advances in ovarian <span class="hlt">imaging</span> techniques provide new understanding of ovarian physiology and allow for the development of better tools to diagnose ovarian pathophysiology. Copyright © 2018 Reproductive Healthcare Ltd. All rights reserved.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://pubs.er.usgs.gov/publication/70025049','USGSPUBS'); return false;" href="https://pubs.er.usgs.gov/publication/70025049"><span>Athena microscopic <span class="hlt">Imager</span> investigation</span></a></p>
      <p><a target="_blank" href="http://pubs.er.usgs.gov/pubs/index.jsp?view=adv">USGS Publications Warehouse</a></p>
      <p>Herkenhoff, K. E.; Squyres, S. W.; Bell, J.F.; Maki, J.N.; Arneson, H.M.; Bertelsen, P.; Brown, D.I.; Collins, S.A.; Dingizian, A.; Elliott, S.T.; Goetz, W.; Hagerott, E.C.; Hayes, A.G.; Johnson, M.J.; Kirk, R.L.; McLennan, S.; Morris, R.V.; Scherr, L.M.; Schwochert, M.A.; Shiraishi, L.R.; Smith, G.H.; Soderblom, L.A.; Sohl-Dickstein, J. N.; Wadsworth, M.V.</p>
         <p>2003-01-01</p>
         <p>The Athena science payload on the Mars Exploration Rovers (MER) includes the Microscopic <span class="hlt">Imager</span> (MI). The MI is a fixed-focus camera mounted on the end of an extendable instrument arm, the Instrument Deployment Device (IDD). The MI was designed to acquire <span class="hlt">images</span> at a spatial resolution of 30 microns/pixel over a broad spectral range (400-700 nm). The MI uses the same electronics design as the other MER cameras but has optics that yield a field of view of 31 ?? 31 mm across a 1024 ?? 1024 pixel CCD <span class="hlt">image</span>. The MI acquires <span class="hlt">images</span> using only solar or skylight illumination of the target surface. A contact sensor is used to place the MI slightly closer to the target surface than its best focus distance (about 66 mm), allowing concave surfaces to be <span class="hlt">imaged</span> in good focus. Coarse focusing (???2 mm precision) is achieved by moving the IDD away from a rock target after the contact sensor has been activated. The MI optics are protected from the Martian environment by a retractable dust cover. The dust cover includes a Kapton window that is tinted orange to restrict the spectral bandpass to 500-700 nm, allowing color information to be obtained by taking <span class="hlt">images</span> with the dust cover open and closed. MI data will be used to place other MER instrument data in context and to aid in petrologic and geologic interpretations of rocks and soils on Mars. Copyright 2003 by the American Geophysical Union.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2006MeScT..17E...1A','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2006MeScT..17E...1A"><span>EDITORIAL: Molecular <span class="hlt">Imaging</span> Technology</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Asai, Keisuke; Okamoto, Koji</p>
         <p>2006-06-01</p>
         <p>'Molecular <span class="hlt">Imaging</span> Technology' focuses on <span class="hlt">image</span>-based techniques using nanoscale molecules as sensor probes to measure spatial variations of various species (molecular oxygen, singlet oxygen, carbon dioxide, nitric monoxide, etc) and physical properties (pressure, temperature, skin friction, velocity, mechanical stress, etc). This special feature, starting on page 1237, contains selected papers from The International Workshop on Molecular <span class="hlt">Imaging</span> for Interdisciplinary Research, sponsored by the Ministry of Education, Culture, Sports, Science and Technology (MEXT) in Japan, which was held at the Sendai Mediatheque, Sendai, Japan, on 8 9 November 2004. The workshop was held as a sequel to the MOSAIC International Workshop that was held in Tokyo in 2003, to summarize the outcome of the 'MOSAIC Project', a five-year interdisciplinary project supported by Techno-Infrastructure Program, the Special Coordination Fund for Promotion of Science Technology to develop molecular sensor technology for aero-thermodynamic research. The workshop focused on molecular <span class="hlt">imaging</span> technology and its applications to interdisciplinary research areas. More than 110 people attended this workshop from various research fields such as aerospace engineering, automotive engineering, radiotechnology, fluid dynamics, bio-science/engineering and medical engineering. The purpose of this workshop is to stimulate intermixing of these interdisciplinary fields for further development of molecular sensor and <span class="hlt">imaging</span> technology. It is our pleasure to publish the seven papers selected from our workshop as a special feature in Measurement and Science Technology. We will be happy if this issue inspires people to explore the future direction of molecular <span class="hlt">imaging</span> technology for interdisciplinary research.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_17");'>17</a></li>
      <li><a href="#" onclick='return showDiv("page_18");'>18</a></li>
      <li class="active"><span>19</span></li>
   <li><a href="#" onclick='return showDiv("page_20");'>20</a></li>
      <li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_19 -->
   <div id="page_20" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_18");'>18</a></li>
      <li><a href="#" onclick='return showDiv("page_19");'>19</a></li>
      <li class="active"><span>20</span></li>
   <li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li><a href="#" onclick='return showDiv("page_22");'>22</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="381">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2013SPIE.8664E..0MF','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2013SPIE.8664E..0MF"><span>Tangible <span class="hlt">imaging</span> systems</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Ferwerda, James A.</p>
         <p>2013-03-01</p>
         <p>We are developing tangible <span class="hlt">imaging</span> systems1-4 that enable natural interaction with virtual objects. Tangible <span class="hlt">imaging</span> systems are based on consumer mobile devices that incorporate electronic displays, graphics hardware, accelerometers, gyroscopes, and digital cameras, in laptop or tablet-shaped form-factors. Custom software allows the orientation of a device and the position of the observer to be tracked in real-time. Using this information, realistic <span class="hlt">images</span> of threedimensional objects with complex textures and material properties are rendered to the screen, and tilting or moving in front of the device produces realistic changes in surface lighting and material appearance. Tangible <span class="hlt">imaging</span> systems thus allow virtual objects to be observed and manipulated as naturally as real ones with the added benefit that object properties can be modified under user control. In this paper we describe four tangible <span class="hlt">imaging</span> systems we have developed: the tangiBook - our first implementation on a laptop computer; tangiView - a more refined implementation on a tablet device; tangiPaint - a tangible digital painting application; and phantoView - an application that takes the tangible <span class="hlt">imaging</span> concept into stereoscopic 3D.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20100014160','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20100014160"><span>Satellite <span class="hlt">Image</span> Mosaic Engine</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Plesea, Lucian</p>
         <p>2006-01-01</p>
         <p>A computer program automatically builds large, full-resolution mosaics of multispectral <span class="hlt">images</span> of Earth landmasses from <span class="hlt">images</span> acquired by Landsat 7, complete with matching of colors and blending between adjacent scenes. While the code has been used extensively for Landsat, it could also be used for other data sources. A single mosaic of as many as 8,000 scenes, represented by more than 5 terabytes of data and the largest set produced in this work, demonstrated what the code could do to provide global coverage. The program first statistically analyzes input <span class="hlt">images</span> to determine areas of coverage and data-value distributions. It then transforms the input <span class="hlt">images</span> from their original universal transverse Mercator coordinates to other geographical coordinates, with scaling. It applies a first-order polynomial brightness correction to each band in each scene. It uses a data-mask <span class="hlt">image</span> for selecting data and blending of input scenes. Under control by a user, the program can be made to operate on small parts of the output <span class="hlt">image</span> space, with check-point and restart capabilities. The program runs on SGI IRIX computers. It is capable of parallel processing using shared-memory code, large memories, and tens of central processing units. It can retrieve input data and store output data at locations remote from the processors on which it is executed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/18033979','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/18033979"><span>Body <span class="hlt">image</span> and transsexualism.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Kraemer, Bernd; Delsignore, Aba; Schnyder, Ulrich; Hepp, Urs</p>
         <p>2008-01-01</p>
         <p>To achieve a detailed view of the body <span class="hlt">image</span> of transsexual patients, an assessment of perception, attitudes and experiences about one's own body is necessary. To date, research on the body <span class="hlt">image</span> of transsexual patients has mostly covered body dissatisfaction with respect to body perception. We investigated 23 preoperative (16 male-to-female and 7 female-to-male transsexual patients) and 22 postoperative (14 male-to-female and 8 female-to-male) transsexual patients using a validated psychological measure for body <span class="hlt">image</span> variables. We found that preoperative transsexual patients were insecure and felt unattractive because of concerns about their body <span class="hlt">image</span>. However, postoperative transsexual patients scored high on attractiveness and self-confidence. Furthermore, postoperative transsexual patients showed low scores for insecurity and concerns about their body. Our results indicate an improvement of body <span class="hlt">image</span> concerns for transsexual patients following standards of care for gender identity disorder. Follow-up studies are recommended to confirm the assumed positive outcome of standards of care on body <span class="hlt">image</span>. (c) 2007 S. Karger AG, Basel.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1082877','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1082877"><span><span class="hlt">Image</span> portion identification methods, <span class="hlt">image</span> parsing methods, <span class="hlt">image</span> parsing systems, and articles of manufacture</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Lassahn, Gordon D.; Lancaster, Gregory D.; Apel, William A.; Thompson, Vicki S.</p>
         <p>2013-01-08</p>
         <p><span class="hlt">Image</span> portion identification methods, <span class="hlt">image</span> parsing methods, <span class="hlt">image</span> parsing systems, and articles of manufacture are described. According to one embodiment, an <span class="hlt">image</span> portion identification method includes accessing data regarding an <span class="hlt">image</span> depicting a plurality of biological substrates corresponding to at least one biological sample and indicating presence of at least one biological indicator within the biological sample and, using processing circuitry, automatically identifying a portion of the <span class="hlt">image</span> depicting one of the biological substrates but not others of the biological substrates.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2008SPIE.6916E..1IW','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2008SPIE.6916E..1IW"><span>Whole mouse cryo-<span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Wilson, David; Roy, Debashish; Steyer, Grant; Gargesha, Madhusudhana; Stone, Meredith; McKinley, Eliot</p>
         <p>2008-03-01</p>
         <p>The Case cryo-<span class="hlt">imaging</span> system is a section and <span class="hlt">image</span> system which allows one to acquire micron-scale, information rich, whole mouse color bright field and molecular fluorescence <span class="hlt">images</span> of an entire mouse. Cryo-<span class="hlt">imaging</span> is used in a variety of applications, including mouse and embryo anatomical phenotyping, drug delivery, <span class="hlt">imaging</span> agents, metastastic cancer, stem cells, and very high resolution vascular <span class="hlt">imaging</span>, among many. Cryo-<span class="hlt">imaging</span> fills the gap between whole animal in vivo <span class="hlt">imaging</span> and histology, allowing one to <span class="hlt">image</span> a mouse along the continuum from the mouse -> organ -> tissue structure -> cell -> sub-cellular domains. In this overview, we describe the technology and a variety of exciting applications. Enhancements to the system now enable tiled acquisition of high resolution <span class="hlt">images</span> to cover an entire mouse. High resolution fluorescence <span class="hlt">imaging</span>, aided by a novel subtraction processing algorithm to remove sub-surface fluorescence, makes it possible to detect fluorescently-labeled single cells. Multi-modality experiments in Magnetic Resonance <span class="hlt">Imaging</span> and Cryo-<span class="hlt">imaging</span> of a whole mouse demonstrate superior resolution of cryo-<span class="hlt">images</span> and efficiency of registration techniques. The 3D results demonstrate the novel true-color volume visualization tools we have developed and the inherent advantage of cryo-<span class="hlt">imaging</span> in providing unlimited depth of field and spatial resolution. The recent results continue to demonstrate the value cryo-<span class="hlt">imaging</span> provides in the field of small animal <span class="hlt">imaging</span> research.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19950015120&hterms=image+processing&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D50%26Ntt%3Dimage%2Bprocessing','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19950015120&hterms=image+processing&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D50%26Ntt%3Dimage%2Bprocessing"><span>Combining <span class="hlt">image</span>-processing and <span class="hlt">image</span> compression schemes</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Greenspan, H.; Lee, M.-C.</p>
         <p>1995-01-01</p>
         <p>An investigation into the combining of <span class="hlt">image</span>-processing schemes, specifically an <span class="hlt">image</span> enhancement scheme, with existing compression schemes is discussed. Results are presented on the pyramid coding scheme, the subband coding scheme, and progressive transmission. Encouraging results are demonstrated for the combination of <span class="hlt">image</span> enhancement and pyramid <span class="hlt">image</span> coding schemes, especially at low bit rates. Adding the enhancement scheme to progressive <span class="hlt">image</span> transmission allows enhanced visual perception at low resolutions. In addition, further progressing of the transmitted <span class="hlt">images</span>, such as edge detection schemes, can gain from the added <span class="hlt">image</span> resolution via the enhancement.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/12318099','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/12318099"><span>Recasting <span class="hlt">image</span> of contraceptives.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Rimon Jg; Kiragu, K</p>
         <p>1993-03-01</p>
         <p>Even though contraceptives are linked to sex which, along with sensuality and peer acceptance, is used to market consumer goods, contraceptives are promoted in a hygienic, clinical way. Glamorous <span class="hlt">images</span> which divert from adverse health effects are used to sell unhealthy goods, e.g., alcohol and cigarettes, but technical and intimidating promotion techniques centering on risks are used to promote family planning (FP) products and services which actually save the lives of mothers and children and improve their health. Until recently, only the medical system provided FP products and services so consumers identified them with illness and a help-seeking behavior. The <span class="hlt">image</span> of contraceptives must be remolded to gain people's attention. To avoid instilling mistrust of a method in consumers, even those who believe in birth spacing, it is important for <span class="hlt">images</span> to be positive and to reflect accurate information. In Indonesia, the Dualima condom has been linked to responsible fatherhood thereby creating a positive <span class="hlt">image</span> and removing the negative <span class="hlt">image</span> of a condom being linked to illicit sex. In the US, condom adds show the user in control, especially in reference to AIDS. Prior to promotion of any contraceptive, complete, clear communication and marketing plans are needed to identify and to focus on consumers' perceived needs. A survey in Egypt shows that the most important attributes of a contraceptive are ease of use, healthiness, and effectiveness and that Egyptians considered IUDs to best fit these attributes. <span class="hlt">Images</span> of contraceptive users often determine whether potential users do choose to use contraceptives. For example, in Cameroon and the Philippines, female users are considered to be smart, rich, educated, confident and in control of their lives. In the Philippines, male users are perceived to be loving, caring, and considerate husbands. The mass medias can improve providers' public <span class="hlt">image</span> as was the case in Turkey and Egypt.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2017OptLE..90..196K','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2017OptLE..90..196K"><span>An algorithm for encryption of secret <span class="hlt">images</span> into meaningful <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Kanso, A.; Ghebleh, M.</p>
         <p>2017-03-01</p>
         <p><span class="hlt">Image</span> encryption algorithms typically transform a plain <span class="hlt">image</span> into a noise-like cipher <span class="hlt">image</span>, whose appearance is an indication of encrypted content. Bao and Zhou [<span class="hlt">Image</span> encryption: Generating visually meaningful encrypted <span class="hlt">images</span>, Information Sciences 324, 2015] propose encrypting the plain <span class="hlt">image</span> into a visually meaningful cover <span class="hlt">image</span>. This improves security by masking existence of encrypted content. Following their approach, we propose a lossless visually meaningful <span class="hlt">image</span> encryption scheme which improves Bao and Zhou's algorithm by making the encrypted content, i.e. distortions to the cover <span class="hlt">image</span>, more difficult to detect. Empirical results are presented to show high quality of the resulting <span class="hlt">images</span> and high security of the proposed algorithm. Competence of the proposed scheme is further demonstrated by means of comparison with Bao and Zhou's scheme.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2012SPIE.8527E..0BM','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2012SPIE.8527E..0BM"><span>Quality evaluation of pansharpened hyperspectral <span class="hlt">images</span> generated using multispectral <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Matsuoka, Masayuki; Yoshioka, Hiroki</p>
         <p>2012-11-01</p>
         <p>Hyperspectral remote sensing can provide a smooth spectral curve of a target by using a set of higher spectral resolution detectors. The spatial resolution of the hyperspectral <span class="hlt">images</span>, however, is generally much lower than that of multispectral <span class="hlt">images</span> due to the lower energy of incident radiation. Pansharpening is an <span class="hlt">image</span>-fusion technique that generates higher spatial resolution multispectral <span class="hlt">images</span> by combining lower resolution multispectral <span class="hlt">images</span> with higher resolution panchromatic <span class="hlt">images</span>. In this study, higher resolution hyperspectral <span class="hlt">images</span> were generated by pansharpening of simulated lower hyperspectral and higher multispectral data. Spectral and spatial qualities of pansharpened <span class="hlt">images</span>, then, were accessed in relation to the spectral bands of multispectral <span class="hlt">images</span>. Airborne hyperspectral data of AVIRIS was used in this study, and it was pansharpened using six methods. Quantitative evaluations of pansharpened <span class="hlt">image</span> are achieved using two frequently used indices, ERGAS, and the Q index.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/AD1000408','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/AD1000408"><span><span class="hlt">Image</span> Processing for Cameras with Fiber Bundle <span class="hlt">Image</span> Relay</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p></p>
         <p>length. Optical fiber bundles have been used to couple between this focal surface and planar <span class="hlt">image</span> sensors . However, such fiber-coupled <span class="hlt">imaging</span> systems...coupled to six discrete CMOS focal planes. We characterize the locally space-variant system impulse response at various stages: monocentric lens <span class="hlt">image</span>...vignetting, and stitch together the <span class="hlt">image</span> data from discrete sensors into a single panorama. We compare processed <span class="hlt">images</span> from the prototype to those taken with</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2018MS%26E..336a2012P','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2018MS%26E..336a2012P"><span>Brain Tumor <span class="hlt">Image</span> Segmentation in MRI <span class="hlt">Image</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Peni Agustin Tjahyaningtijas, Hapsari</p>
         <p>2018-04-01</p>
         <p>Brain tumor segmentation plays an important role in medical <span class="hlt">image</span> processing. Treatment of patients with brain tumors is highly dependent on early detection of these tumors. Early detection of brain tumors will improve the patient’s life chances. Diagnosis of brain tumors by experts usually use a manual segmentation that is difficult and time consuming because of the necessary automatic segmentation. Nowadays automatic segmentation is very populer and can be a solution to the problem of tumor brain segmentation with better performance. The purpose of this paper is to provide a review of MRI-based brain tumor segmentation methods. There are number of existing review papers, focusing on traditional methods for MRI-based brain tumor <span class="hlt">image</span> segmentation. this paper, we focus on the recent trend of automatic segmentation in this field. First, an introduction to brain tumors and methods for brain tumor segmentation is given. Then, the state-of-the-art algorithms with a focus on recent trend of full automatic segmentaion are discussed. Finally, an assessment of the current state is presented and future developments to standardize MRI-based brain tumor segmentation methods into daily clinical routine are addressed.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/12817420','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/12817420"><span>PACS for <span class="hlt">imaging</span> centers.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Farnsworth, T J</p>
         <p>2003-01-01</p>
         <p>PACS can be a difficult and confusing decision for any radiology provider, but it can be an even more dynamic question for an outpatient <span class="hlt">imaging</span> center. Every center represents a unique situation and requires a specialized solution. Typically, most of what is said and discussed about PACS concentrates on solutions and requirements for hospital radiology facilities. Administrators of <span class="hlt">imaging</span> centers have different problems from hospital administrators, and they need different answers. For <span class="hlt">imaging</span> centers, the financial justification for PACS may be less immediate than for hospitals. The first thing that must be understood is that no PAC system can make a typical <span class="hlt">imaging</span> center completely filmless, at least not for quite a while. A hospital has the ability to dictate to its internal referring physicians how a radiological study is delivered, whereas in an <span class="hlt">imaging</span> center environment, the roles are very much reversed. Once the justification are made for the financial viability of PACS in an <span class="hlt">imaging</span> center, the next question is how to finance the acquisition of PACS. The decision will depend on how you cost justify your PACS, as well as the shape of your business model, and it will come to a decision between capital purchase or contracting with an application service provider, or ASP. Historically, in the hospital-dominated marketplace, PAC systems have been treated as capital acquisitions. However, for most <span class="hlt">imaging</span> center, owning the system is more of a problem than a benefit. ASPs increasingly represent a successful alternative for <span class="hlt">imaging</span> centers. One of the biggest things to consider with PACS is how to store all of those <span class="hlt">images</span>. There are typically two options, on-site and off-site, with a new "hybrid" option surfacing more recently. Each option has benefits for the user, but the benefits of off-site storage are increasing as the technology advances. Some of the benefits are data security and access. Other issues to address are HIPAA compliance, standardized</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2011SPIE.7967E..0GD','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2011SPIE.7967E..0GD"><span>Mobile medical <span class="hlt">image</span> retrieval</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Duc, Samuel; Depeursinge, Adrien; Eggel, Ivan; Müller, Henning</p>
         <p>2011-03-01</p>
         <p><span class="hlt">Images</span> are an integral part of medical practice for diagnosis, treatment planning and teaching. <span class="hlt">Image</span> retrieval has gained in importance mainly as a research domain over the past 20 years. Both textual and visual retrieval of <span class="hlt">images</span> are essential. In the process of mobile devices becoming reliable and having a functionality equaling that of formerly desktop clients, mobile computing has gained ground and many applications have been explored. This creates a new field of mobile information search & access and in this context <span class="hlt">images</span> can play an important role as they often allow understanding complex scenarios much quicker and easier than free text. Mobile information retrieval in general has skyrocketed over the past year with many new applications and tools being developed and all sorts of interfaces being adapted to mobile clients. This article describes constraints of an information retrieval system including visual and textual information retrieval from the medical literature of BioMedCentral and of the RSNA journals Radiology and Radiographics. Solutions for mobile data access with an example on an iPhone in a web-based environment are presented as iPhones are frequently used and the operating system is bound to become the most frequent smartphone operating system in 2011. A web-based scenario was chosen to allow for a use by other smart phone platforms such as Android as well. Constraints of small screens and navigation with touch screens are taken into account in the development of the application. A hybrid choice had to be taken to allow for taking pictures with the cell phone camera and upload them for visual similarity search as most producers of smart phones block this functionality to web applications. Mobile information access and in particular access to <span class="hlt">images</span> can be surprisingly efficient and effective on smaller screens. <span class="hlt">Images</span> can be read on screen much faster and relevance of documents can be identified quickly through the use of <span class="hlt">images</span> contained in</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=PIA02321&hterms=photo+image&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D20%26Ntt%3Dphoto%2Bimage','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=PIA02321&hterms=photo+image&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D20%26Ntt%3Dphoto%2Bimage"><span>Single Still <span class="hlt">Image</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1999-01-01</p>
         <p><p/>This narrow angle <span class="hlt">image</span> taken by Cassini's camera system of the Moon is one of the best of a sequence of narrow angle frames taken as the spacecraft passed by the Moon on the way to its closest approach with Earth on August 17, 1999. The 80 millisecond exposure was taken through a spectral filter centered at 0.33 microns; the filter bandpass was 85 Angstroms wide. The spatial scale of the <span class="hlt">image</span> is about 1.4 miles per pixel (about 2.3 kilometers). The <span class="hlt">imaging</span> data were processed and released by the Cassini <span class="hlt">Imaging</span> Central Laboratory for Operations (CICLOPS) at the University of Arizona's Lunar and Planetary Laboratory, Tucson, AZ. <p/>Photo Credit: NASA/JPL/Cassini <span class="hlt">Imaging</span> Team/University of Arizona <p/>Cassini, launched in 1997, is a joint mission of NASA, the European Space Agency and Italian Space Agency. The mission is managed by NASA's Jet Propulsion Laboratory, Pasadena, CA, for NASA's Office of Space Science, Washington DC. JPL is a division of the California Institute of Technology, Pasadena, CA.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19860009796','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19860009796"><span>Planetary <span class="hlt">image</span> conversion task</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Martin, M. D.; Stanley, C. L.; Laughlin, G.</p>
         <p>1985-01-01</p>
         <p>The Planetary <span class="hlt">Image</span> Conversion Task group processed 12,500 magnetic tapes containing raw <span class="hlt">imaging</span> data from JPL planetary missions and produced an <span class="hlt">image</span> data base in consistent format on 1200 fully packed 6250-bpi tapes. The output tapes will remain at JPL. A copy of the entire tape set was delivered to US Geological Survey, Flagstaff, Ariz. A secondary task converted computer datalogs, which had been stored in project specific MARK IV File Management System data types and structures, to flat-file, text format that is processable on any modern computer system. The conversion processing took place at JPL's <span class="hlt">Image</span> Processing Laboratory on an IBM 370-158 with existing software modified slightly to meet the needs of the conversion task. More than 99% of the original digital <span class="hlt">image</span> data was successfully recovered by the conversion task. However, processing data tapes recorded before 1975 was destructive. This discovery is of critical importance to facilities responsible for maintaining digital archives since normal periodic random sampling techniques would be unlikely to detect this phenomenon, and entire data sets could be wiped out in the act of generating seemingly positive sampling results. Reccomended follow-on activities are also included.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=GL-2002-001556&hterms=digital+transformation&qs=N%3D0%26Ntk%3DAll%26Ntx%3Dmode%2Bmatchall%26Ntt%3Ddigital%2Btransformation','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=GL-2002-001556&hterms=digital+transformation&qs=N%3D0%26Ntk%3DAll%26Ntx%3Dmode%2Bmatchall%26Ntt%3Ddigital%2Btransformation"><span><span class="hlt">Image</span> Transformations-Montserrat</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>2002-01-01</p>
         <p>A slightly oblique digital photograph of Montserrat taken from the International Space Station was posted to Earth Observatory in December 2001. An Earth Observatory reader used widely available software to correct the oblique perspective and adjust the color. The story of how he modified the <span class="hlt">image</span> includes step-by-step instructions that can be applied to other photographs. Photographs of Earth taken by astronauts have shaped our view of the Earth and are part of our popular culture because NASA makes them easily accessible to the public. Read the Transformations Story for more information. The original <span class="hlt">image</span> was digital photograph number ISS002-E-9309, taken on July 9, 2001, from the International Space Station and was provided by the Earth Sciences and <span class="hlt">Image</span> Analysis Laboratory at Johnson Space Center. Additional <span class="hlt">images</span> taken by astronauts and cosmonauts can be viewed at the NASA-JSC Gateway to Astronaut Photography of Earth. Bill Innanen provided the transformed <span class="hlt">image</span> and the story of how he did it.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20120006575','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20120006575"><span>Ring <span class="hlt">Image</span> Analyzer</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Strekalov, Dmitry V.</p>
         <p>2012-01-01</p>
         <p>Ring <span class="hlt">Image</span> Analyzer software analyzes <span class="hlt">images</span> to recognize elliptical patterns. It determines the ellipse parameters (axes ratio, centroid coordinate, tilt angle). The program attempts to recognize elliptical fringes (e.g., Newton Rings) on a photograph and determine their centroid position, the short-to-long-axis ratio, and the angle of rotation of the long axis relative to the horizontal direction on the photograph. These capabilities are important in interferometric <span class="hlt">imaging</span> and control of surfaces. In particular, this program has been developed and applied for determining the rim shape of precision-machined optical whispering gallery mode resonators. The program relies on a unique <span class="hlt">image</span> recognition algorithm aimed at recognizing elliptical shapes, but can be easily adapted to other geometric shapes. It is robust against non-elliptical details of the <span class="hlt">image</span> and against noise. Interferometric analysis of precision-machined surfaces remains an important technological instrument in hardware development and quality analysis. This software automates and increases the accuracy of this technique. The software has been developed for the needs of an R&TD-funded project and has become an important asset for the future research proposal to NASA as well as other agencies.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3408896','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3408896"><span>Wavefront <span class="hlt">image</span> sensor chip</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Cui, Xiquan; Ren, Jian; Tearney, Guillermo J.; Yang, Changhuei</p>
         <p>2010-01-01</p>
         <p>We report the implementation of an <span class="hlt">image</span> sensor chip, termed wavefront <span class="hlt">image</span> sensor chip (WIS), that can measure both intensity/amplitude and phase front variations of a light wave separately and quantitatively. By monitoring the tightly confined transmitted light spots through a circular aperture grid in a high Fresnel number regime, we can measure both intensity and phase front variations with a high sampling density (11 µm) and high sensitivity (the sensitivity of normalized phase gradient measurement is 0.1 mrad under the typical working condition). By using WIS in a standard microscope, we can collect both bright-field (transmitted light intensity) and normalized phase gradient <span class="hlt">images</span>. Our experiments further demonstrate that the normalized phase gradient <span class="hlt">images</span> of polystyrene microspheres, unstained and stained starfish embryos, and strongly birefringent potato starch granules are improved versions of their corresponding differential interference contrast (DIC) microscope <span class="hlt">images</span> in that they are artifact-free and quantitative. Besides phase microscopy, WIS can benefit machine recognition, object ranging, and texture assessment for a variety of applications. PMID:20721059</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/24664924','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/24664924"><span>Direct <span class="hlt">imaging</span> of exoplanets.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Lagrange, Anne-Marie</p>
         <p>2014-04-28</p>
         <p>Most of the exoplanets known today have been discovered by indirect techniques, based on the study of the host star radial velocity or photometric temporal variations. These detections allowed the study of the planet populations in the first 5-8 AU from the central stars and have provided precious information on the way planets form and evolve at such separations. Direct <span class="hlt">imaging</span> on 8-10 m class telescopes allows the detection of giant planets at larger separations (currently typically more than 5-10 AU) complementing the indirect techniques. So far, only a few planets have been <span class="hlt">imaged</span> around young stars, but each of them provides an opportunity for unique dedicated studies of their orbital, physical and atmospheric properties and sometimes also on the interaction with the 'second-generation', debris discs. These few detections already challenge formation theories. In this paper, I present the results of direct <span class="hlt">imaging</span> surveys obtained so far, and what they already tell us about giant planet (GP) formation and evolution. Individual and emblematic cases are detailed; they illustrate what future instruments will routinely deliver for a much larger number of stars. I also point out the limitations of this approach, as well as the needs for further work in terms of planet formation modelling. I finally present the progress expected in direct <span class="hlt">imaging</span> in the near future, thanks in particular to forthcoming planet <span class="hlt">imagers</span> on 8-10 m class telescopes.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19910017780','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19910017780"><span>Photographic <span class="hlt">image</span> enhancement</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Hite, Gerald E.</p>
         <p>1990-01-01</p>
         <p>Deblurring capabilities would significantly improve the scientific return from Space Shuttle crew-acquired <span class="hlt">images</span> of the Earth and the safety of Space Shuttle missions. Deblurring techniques were developed and demonstrated on two digitized <span class="hlt">images</span> that were blurred in different ways. The first was blurred by a Gaussian blurring function analogous to that caused by atmospheric turbulence, while the second was blurred by improper focussing. It was demonstrated, in both cases, that the nature of the blurring (Gaussian and Airy) and the appropriate parameters could be obtained from the Fourier transformation of their <span class="hlt">images</span>. The difficulties posed by the presence of noise necessitated special consideration. It was demonstrated that a modified Wiener frequency filter judiciously constructed to avoid over emphasis of frequency regions dominated by noise resulted in substantially improved <span class="hlt">images</span>. Several important areas of future research were identified. Two areas of particular promise are the extraction of blurring information directly from the spatial <span class="hlt">images</span> and improved noise abatement form investigations of select spatial regions and the elimination of spike noise.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_18");'>18</a></li>
      <li><a href="#" onclick='return showDiv("page_19");'>19</a></li>
      <li class="active"><span>20</span></li>
   <li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li><a href="#" onclick='return showDiv("page_22");'>22</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_20 -->
   <div id="page_21" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_19");'>19</a></li>
      <li><a href="#" onclick='return showDiv("page_20");'>20</a></li>
      <li class="active"><span>21</span></li>
   <li><a href="#" onclick='return showDiv("page_22");'>22</a></li>
      <li><a href="#" onclick='return showDiv("page_23");'>23</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="401">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19900012927','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19900012927"><span>Pyramid <span class="hlt">image</span> codes</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Watson, Andrew B.</p>
         <p>1990-01-01</p>
         <p>All vision systems, both human and machine, transform the spatial <span class="hlt">image</span> into a coded representation. Particular codes may be optimized for efficiency or to extract useful <span class="hlt">image</span> features. Researchers explored <span class="hlt">image</span> codes based on primary visual cortex in man and other primates. Understanding these codes will advance the art in <span class="hlt">image</span> coding, autonomous vision, and computational human factors. In cortex, imagery is coded by features that vary in size, orientation, and position. Researchers have devised a mathematical model of this transformation, called the Hexagonal oriented Orthogonal quadrature Pyramid (HOP). In a pyramid code, features are segregated by size into layers, with fewer features in the layers devoted to large features. Pyramid schemes provide scale invariance, and are useful for coarse-to-fine searching and for progressive transmission of <span class="hlt">images</span>. The HOP Pyramid is novel in three respects: (1) it uses a hexagonal pixel lattice, (2) it uses oriented features, and (3) it accurately models most of the prominent aspects of primary visual cortex. The transform uses seven basic features (kernels), which may be regarded as three oriented edges, three oriented bars, and one non-oriented blob. Application of these kernels to non-overlapping seven-pixel neighborhoods yields six oriented, high-pass pyramid layers, and one low-pass (blob) layer.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2003SPIE.4955..145K','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2003SPIE.4955..145K"><span>Animal <span class="hlt">imaging</span> using immersion</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Kalogerakis, Konstantinos S.; Kotz, Kenneth T.; Rand, Kendra; Faris, Gregory W.</p>
         <p>2003-07-01</p>
         <p>We are using rodent animal models to study and compare contrast mechanisms for detection of breast cancer. These measurements are performed with the animals immersed in a matching scattering medium. The matching scattering medium or liquid tissue phantom comprises a mixture of Ropaque (hollow acrylic/styrene microspheres) and ink. We have previously applied matched <span class="hlt">imaging</span> to <span class="hlt">imaging</span> in humans. Surrounding the <span class="hlt">imaged</span> region with a matched tissue phantom compensates for variations in tissue thickness and geometry, provides more uniform illumination, and allows better use of the dynamic range of the <span class="hlt">imaging</span> system. If the match is good, the boundaries of the <span class="hlt">imaged</span> region should almost vanish, enhancing the contrast from internal structure as compared to contrast from the boundaries and surface topography. For our measurements in animals, the immersion plays two additional roles. First, we can readily study tumors through tissue thickness similar to that of a human breast. Although the heterogeneity of the breast is lost, this is a practical method to study the detection of small tumors and monitor changes as they grow. Second, the immersion enhances our ability to quantify the contrast mechanisms for peripheral tumors on the animal because the boundary effects on photon migration are eliminated. We are currently developing two systems for these measurements. One is a continuous-wave (CW) system based on near-infrared LED illumination and a CCD (charge-coupled device) camera. The second system, a frequency domain system, can help quantify the changes observed with the CW system.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2017LaPhL..14e5701O','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2017LaPhL..14e5701O"><span>Scalp <span class="hlt">imaging</span> techniques</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Otberg, Nina; Shapiro, Jerry; Lui, Harvey; Wu, Wen-Yu; Alzolibani, Abdullateef; Kang, Hoon; Richter, Heike; Lademann, Jürgen</p>
         <p>2017-05-01</p>
         <p>Scalp <span class="hlt">imaging</span> techniques are necessary tools for the trichological practice and for visualization of permeation, penetration and absorption processes into and through the scalp and for the research on drug delivery and toxicology. The present letter reviews different scalp <span class="hlt">imaging</span> techniques and discusses their utility. Moreover, two different studies on scalp <span class="hlt">imaging</span> techniques are presented in this letter: (1) scalp <span class="hlt">imaging</span> with phototrichograms in combination with laser scanning microscopy, and (2) follicular measurements with cyanoacrylate surface replicas and light microscopy in combination with laser scanning microscopy. The experiments compare different methods for the determination of hair density on the scalp and different follicular measures. An average terminal hair density of 132 hairs cm-2 was found in 6 Caucasian volunteers and 135 hairs cm-2 in 6 Asian volunteers. The area of the follicular orifices accounts to 16.3% of the skin surface on average measured with laser scanning microscopy <span class="hlt">images</span>. The potential volume of the follicular infundibulum was calculated based on the laser scanning measurements and is found to be 4.63 mm3 per cm2 skin on average. The experiments show that hair follicles are quantitatively relevant pathways and potential reservoirs for topically applied drugs and cosmetics.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016Natur.540..100K','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016Natur.540..100K"><span>Ghost <span class="hlt">imaging</span> with atoms</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Khakimov, R. I.; Henson, B. M.; Shin, D. K.; Hodgman, S. S.; Dall, R. G.; Baldwin, K. G. H.; Truscott, A. G.</p>
         <p>2016-12-01</p>
         <p>Ghost <span class="hlt">imaging</span> is a counter-intuitive phenomenon—first realized in quantum optics—that enables the <span class="hlt">image</span> of a two-dimensional object (mask) to be reconstructed using the spatio-temporal properties of a beam of particles with which it never interacts. Typically, two beams of correlated photons are used: one passes through the mask to a single-pixel (bucket) detector while the spatial profile of the other is measured by a high-resolution (multi-pixel) detector. The second beam never interacts with the mask. Neither detector can reconstruct the mask independently, but temporal cross-correlation between the two beams can be used to recover a ‘ghost’ <span class="hlt">image</span>. Here we report the realization of ghost <span class="hlt">imaging</span> using massive particles instead of photons. In our experiment, the two beams are formed by correlated pairs of ultracold, metastable helium atoms, which originate from s-wave scattering of two colliding Bose-Einstein condensates. We use higher-order Kapitza-Dirac scattering to generate a large number of correlated atom pairs, enabling the creation of a clear ghost <span class="hlt">image</span> with submillimetre resolution. Future extensions of our technique could lead to the realization of ghost interference, and enable tests of Einstein-Podolsky-Rosen entanglement and Bell’s inequalities with atoms.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2017JBO....22g6005C','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2017JBO....22g6005C"><span>Photothermal strain <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Choi, Changhoon; Ahn, Joongho; Jeon, Seungwan; Kim, Chulhong</p>
         <p>2017-07-01</p>
         <p>Vulnerable plaques are the major cause of cardiovascular disease, but they are difficult to detect with conventional intravascular <span class="hlt">imaging</span> techniques. Techniques are needed to identify plaque vulnerability based on the presence of lipids in plaque. Thermal strain <span class="hlt">imaging</span> (TSI) is an <span class="hlt">imaging</span> technique based on ultrasound (US) wave propagation speed, which varies with the medium temperature. In TSI, the strain that occurs during tissue temperature change can be used for lipid detection because it has a different tendency depending on the type of tissue. Here, we demonstrate photothermal strain <span class="hlt">imaging</span> (pTSI) using an intravascular ultrasound catheter. pTSI is performed by slightly and selectively heating lipid using a relatively inexpensive continuous laser source. We applied a speckle-tracking algorithm to US B-mode <span class="hlt">images</span> for strain calculations. As a result, the strain produced in porcine fat was different from the strain produced in water-bearing gelatin phantom, which made it possible to distinguish the two. This suggests that pTSI could potentially be a way of differentiating lipids in coronary artery.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1990SPIE.1232..150W','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1990SPIE.1232..150W"><span>Transparent volume <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Wixson, Steve E.</p>
         <p>1990-07-01</p>
         <p>Transparent Volume <span class="hlt">Imaging</span> began with the stereo xray in 1895 and ended for most investigators when radiation safety concerns eliminated the second view. Today, similiar <span class="hlt">images</span> can be generated by the computer without safety hazards providing improved perception and new means of <span class="hlt">image</span> quantification. A volumetric workstation is under development based on an operational prototype. The workstation consists of multiple symbolic and numeric processors, binocular stereo color display generator with large <span class="hlt">image</span> memory and liquid crystal shutter, voice input and output, a 3D pointer that uses projection lenses so that structures in 3 space can be touched directly, 3D hard copy using vectograph and lenticular printing, and presentation facilities using stereo 35mm slide and stereo video tape projection. Volumetric software includes a volume window manager, Mayo Clinic's Analyze program and our Digital Stereo Microscope (DSM) algorithms. The DSM uses stereo xray-like projections, rapidly oscillating motion and focal depth cues such that detail can be studied in the spatial context of the entire set of data. Focal depth cues are generated with a lens and apeture algorithm that generates a plane of sharp focus, and multiple stereo pairs each with a different plane of sharp focus are generated and stored in the large memory for interactive selection using a physical or symbolic depth selector. More recent work is studying non-linear focussing. Psychophysical studies are underway to understand how people perce ive <span class="hlt">images</span> on a volumetric display and how accurately 3 dimensional structures can be quantitated from these displays.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19930062441&hterms=Increased+entropy&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D50%26Ntt%3DIncreased%2Bentropy','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19930062441&hterms=Increased+entropy&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D50%26Ntt%3DIncreased%2Bentropy"><span>Bayesian <span class="hlt">image</span> reconstruction - The pixon and optimal <span class="hlt">image</span> modeling</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Pina, R. K.; Puetter, R. C.</p>
         <p>1993-01-01</p>
         <p>In this paper we describe the optimal <span class="hlt">image</span> model, maximum residual likelihood method (OptMRL) for <span class="hlt">image</span> reconstruction. OptMRL is a Bayesian <span class="hlt">image</span> reconstruction technique for removing point-spread function blurring. OptMRL uses both a goodness-of-fit criterion (GOF) and an '<span class="hlt">image</span> prior', i.e., a function which quantifies the a priori probability of the <span class="hlt">image</span>. Unlike standard maximum entropy methods, which typically reconstruct the <span class="hlt">image</span> on the data pixel grid, OptMRL varies the <span class="hlt">image</span> model in order to find the optimal functional basis with which to represent the <span class="hlt">image</span>. We show how an optimal basis for <span class="hlt">image</span> representation can be selected and in doing so, develop the concept of the 'pixon' which is a generalized <span class="hlt">image</span> cell from which this basis is constructed. By allowing both the <span class="hlt">image</span> and the <span class="hlt">image</span> representation to be variable, the OptMRL method greatly increases the volume of solution space over which the <span class="hlt">image</span> is optimized. Hence the likelihood of the final reconstructed <span class="hlt">image</span> is greatly increased. For the goodness-of-fit criterion, OptMRL uses the maximum residual likelihood probability distribution introduced previously by Pina and Puetter (1992). This GOF probability distribution, which is based on the spatial autocorrelation of the residuals, has the advantage that it ensures spatially uncorrelated <span class="hlt">image</span> reconstruction residuals.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/19125357','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/19125357"><span><span class="hlt">Image</span> manipulation as research misconduct.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Parrish, Debra; Noonan, Bridget</p>
         <p>2009-06-01</p>
         <p>A growing number of research misconduct cases handled by the Office of Research Integrity involve <span class="hlt">image</span> manipulations. Manipulations may include simple <span class="hlt">image</span> enhancements, misrepresenting an <span class="hlt">image</span> as something different from what it is, and altering specific features of an <span class="hlt">image</span>. Through a study of specific cases, the misconduct findings associated with <span class="hlt">image</span> manipulation, detection methods and those likely to identify such manipulations, are discussed. This article explores sanctions imposed against guilty researchers and the factors that resulted in no misconduct finding although relevant <span class="hlt">images</span> clearly were flawed. Although new detection tools are available for universities and journals to detect questionable <span class="hlt">images</span>, this article explores why these tools have not been embraced.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19940021010','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19940021010"><span>Enhanced <span class="hlt">image</span> capture through fusion</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Burt, Peter J.; Hanna, Keith; Kolczynski, Raymond J.</p>
         <p>1993-01-01</p>
         <p><span class="hlt">Image</span> fusion may be used to combine <span class="hlt">images</span> from different sensors, such as IR and visible cameras, to obtain a single composite with extended information content. Fusion may also be used to combine multiple <span class="hlt">images</span> from a given sensor to form a composite <span class="hlt">image</span> in which information of interest is enhanced. We present a general method for performing <span class="hlt">image</span> fusion and show that this method is effective for diverse fusion applications. We suggest that fusion may provide a powerful tool for enhanced <span class="hlt">image</span> capture with broad utility in <span class="hlt">image</span> processing and computer vision.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1996SPIE10283E..0AE','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1996SPIE10283E..0AE"><span><span class="hlt">Imaging</span> standards for smart cards</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Ellson, Richard N.; Ray, Lawrence A.</p>
         <p>1996-02-01</p>
         <p>"Smart cards" are plastic cards the size of credit cards which contain integrated circuits for the storage of digital information. The applications of these cards for <span class="hlt">image</span> storage has been growing as card data capacities have moved from tens of bytes to thousands of bytes. This has prompted the recommendation of standards by the X3B10 committee of ANSI for inclusion in ISO standards for card <span class="hlt">image</span> storage of a variety of <span class="hlt">image</span> data types including digitized signatures and color portrait <span class="hlt">images</span>. This paper will review <span class="hlt">imaging</span> requirements of the smart card industry, challenges of <span class="hlt">image</span> storage for small memory devices, card <span class="hlt">image</span> communications, and the present status of standards. The paper will conclude with recommendations for the evolution of smart card <span class="hlt">image</span> standards towards <span class="hlt">image</span> formats customized to the <span class="hlt">image</span> content and more optimized for smart card memory constraints.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1996SPIE.CR61..176E','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1996SPIE.CR61..176E"><span><span class="hlt">Imaging</span> standards for smart cards</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Ellson, Richard N.; Ray, Lawrence A.</p>
         <p>1996-01-01</p>
         <p>'Smart cards' are plastic cards the size of credit cards which contain integrated circuits for the storage of digital information. The applications of these cards for <span class="hlt">image</span> storage has been growing as card data capacities have moved from tens of bytes to thousands of bytes. This has prompted the recommendation of standards by the X3B10 committee of ANSI for inclusion in ISO standards for card <span class="hlt">image</span> storage of a variety of <span class="hlt">image</span> data types including digitized signatures and color portrait <span class="hlt">images</span>. This paper reviews <span class="hlt">imaging</span> requirements of the smart card industry, challenges of <span class="hlt">image</span> storage for small memory devices, card <span class="hlt">image</span> communications, and the present status of standards. The paper concludes with recommendations for the evolution of smart card <span class="hlt">image</span> standards towards <span class="hlt">image</span> formats customized to the <span class="hlt">image</span> content and more optimized for smart card memory constraints.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2017SPIE10395E..0JG','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2017SPIE10395E..0JG"><span>Restoration of motion blurred <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Gaxiola, Leopoldo N.; Juarez-Salazar, Rigoberto; Diaz-Ramirez, Victor H.</p>
         <p>2017-08-01</p>
         <p><span class="hlt">Image</span> restoration is a classic problem in <span class="hlt">image</span> processing. <span class="hlt">Image</span> degradations can occur due to several reasons, for instance, imperfections of <span class="hlt">imaging</span> systems, quantization errors, atmospheric turbulence, relative motion between camera or objects, among others. Motion blur is a typical degradation in dynamic <span class="hlt">imaging</span> systems. In this work, we present a method to estimate the parameters of linear motion blur degradation from a captured blurred <span class="hlt">image</span>. The proposed method is based on analyzing the frequency spectrum of a captured <span class="hlt">image</span> in order to firstly estimate the degradation parameters, and then, to restore the <span class="hlt">image</span> with a linear filter. The performance of the proposed method is evaluated by processing synthetic and real-life <span class="hlt">images</span>. The obtained results are characterized in terms of accuracy of <span class="hlt">image</span> restoration given by an objective criterion.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2001IJMPC..12..481F','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2001IJMPC..12..481F"><span>Medical <span class="hlt">Images</span> Remote Consultation</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Ferraris, Maurizio; Frixione, Paolo; Squarcia, Sandro</p>
         <p></p>
         <p>Teleconsultation of digital <span class="hlt">images</span> among different medical centers is now a reality. The problem to be solved is how to interconnect all the clinical diagnostic devices in a hospital in order to allow physicians and health physicists, working in different places, to discuss on interesting clinical cases visualizing the same diagnostic <span class="hlt">images</span> at the same time. Applying World Wide Web technologies, the proposed system can be easily used by people with no specific computer knowledge providing a verbose help to guide the user through the right steps of execution. Diagnostic <span class="hlt">images</span> are retrieved from a relational database or from a standard DICOM-PACS through the DICOM-WWW gateway allowing connection of the usual Web browsers to DICOM applications via the HTTP protocol. The system, which is proposed for radiotherapy implementation, where radiographies play a fundamental role, can be easily converted to different field of medical applications where a remote access to secure data are compulsory.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20000025506','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20000025506"><span>Stereo <span class="hlt">Imaging</span> Velocimetry</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>McDowell, Mark (Inventor); Glasgow, Thomas K. (Inventor)</p>
         <p>1999-01-01</p>
         <p>A system and a method for measuring three-dimensional velocities at a plurality of points in a fluid employing at least two cameras positioned approximately perpendicular to one another. The cameras are calibrated to accurately represent <span class="hlt">image</span> coordinates in world coordinate system. The two-dimensional views of the cameras are recorded for <span class="hlt">image</span> processing and centroid coordinate determination. Any overlapping particle clusters are decomposed into constituent centroids. The tracer particles are tracked on a two-dimensional basis and then stereo matched to obtain three-dimensional locations of the particles as a function of time so that velocities can be measured therefrom The stereo <span class="hlt">imaging</span> velocimetry technique of the present invention provides a full-field. quantitative, three-dimensional map of any optically transparent fluid which is seeded with tracer particles.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2012SPIE.8365E..0IE','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2012SPIE.8365E..0IE"><span>Progressive compressive <span class="hlt">imager</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Evladov, Sergei; Levi, Ofer; Stern, Adrian</p>
         <p>2012-06-01</p>
         <p>We have designed and built a working automatic progressive sampling <span class="hlt">imaging</span> system based on the vector sensor concept, which utilizes a unique sampling scheme of Radon projections. This sampling scheme makes it possible to progressively add information resulting in tradeoff between compression and the quality of reconstruction. The uniqueness of our sampling is that in any moment of the acquisition process the reconstruction can produce a reasonable version of the <span class="hlt">image</span>. The advantage of the gradual addition of the samples is seen when the sparsity rate of the object is unknown, and thus the number of needed measurements. We have developed the iterative algorithm OSO (Ordered Sets Optimization) which employs our sampling scheme for creation of nearly uniform distributed sets of samples, which allows the reconstruction of Mega-Pixel <span class="hlt">images</span>. We present the good quality reconstruction from compressed data ratios of 1:20.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19870019717','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19870019717"><span>Video <span class="hlt">image</span> processing</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Murray, N. D.</p>
         <p>1985-01-01</p>
         <p>Current technology projections indicate a lack of availability of special purpose computing for Space Station applications. Potential functions for video <span class="hlt">image</span> special purpose processing are being investigated, such as smoothing, enhancement, restoration and filtering, data compression, feature extraction, object detection and identification, pixel interpolation/extrapolation, spectral estimation and factorization, and vision synthesis. Also, architectural approaches are being identified and a conceptual design generated. Computationally simple algorithms will be research and their <span class="hlt">image</span>/vision effectiveness determined. Suitable algorithms will be implimented into an overall architectural approach that will provide <span class="hlt">image</span>/vision processing at video rates that are flexible, selectable, and programmable. Information is given in the form of charts, diagrams and outlines.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/1033971-microbial-cell-imaging','SCIGOV-STC'); return false;" href="https://www.osti.gov/biblio/1033971-microbial-cell-imaging"><span>Microbial Cell <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Doktycz, Mitchel John; Sullivan, Claretta; Mortensen, Ninell P</p>
         <p></p>
         <p>Atomic force microscopy (AFM) is finding increasing application in a variety of fields including microbiology. Until the emergence of AFM, techniques for ivnestigating processes in single microbes were limited. From a biologist's perspective, the fact that AFM can be used to generate high-resolution <span class="hlt">images</span> in buffers or media is its most appealing feature as live-cell <span class="hlt">imaging</span> can be pursued. <span class="hlt">Imaging</span> living cells by AFM allows dynamic biological events to be studied, at the nanoscale, in real time. Few areas of biological research have as much to gain as microbiology from the application of AFM. Whereas the scale of microbes placesmore » them near the limit of resolution for light microscopy. AFM is well suited for the study of structures on the order of a micron or less. Although electron microscopy techniques have been the standard for high-resolution <span class="hlt">imaging</span> of microbes, AFM is quickly gaining favor for several reasons. First, fixatives that impair biological activity are not required. Second, AFM is capable of detecting forces in the pN range, and precise control of the force applied to the cantilever can be maintained. This combination facilitates the evaluation of physical characteristics of microbes. Third, rather than yielding the composite, statistical average of cell populations, as is the case with many biochemical assays, the behavior of single cells can be monitored. Despite the potential of AFM in microbiology, there are several limitations that must be considered. For example, the time required to record an <span class="hlt">image</span> allows for the study of gross events such as cell division or membrane degradation from an antibiotic but precludes the evaluation of biological reactions and events that happen in just fractions of a second. Additionally, the AFM is a topographical tool and is restricted to <span class="hlt">imaging</span> surfaces. Therefore, it cannot be used to look inside cells as with opticla and transmission electron microscopes. other practical considerations are the</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1084242','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/1084242"><span>Variable waveband infrared <span class="hlt">imager</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Hunter, Scott R.</p>
         <p>2013-06-11</p>
         <p>A waveband <span class="hlt">imager</span> includes an <span class="hlt">imaging</span> pixel that utilizes photon tunneling with a thermally actuated bimorph structure to convert infrared radiation to visible radiation. Infrared radiation passes through a transparent substrate and is absorbed by a bimorph structure formed with a pixel plate. The absorption generates heat which deflects the bimorph structure and pixel plate towards the substrate and into an evanescent electric field generated by light propagating through the substrate. Penetration of the bimorph structure and pixel plate into the evanescent electric field allows a portion of the visible wavelengths propagating through the substrate to tunnel through the substrate, bimorph structure, and/or pixel plate as visible radiation that is proportional to the intensity of the incident infrared radiation. This converted visible radiation may be superimposed over visible wavelengths passed through the <span class="hlt">imaging</span> pixel.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2797462','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=2797462"><span>Meninges in cancer <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Chong, V.</p>
         <p>2009-01-01</p>
         <p>Abstract Primary malignant tumours arising from the meninges are distinctly uncommon, and when they occur, they are usually sarcomas. In contrast, metastatic meningeal involvement is increasingly seen as advances in cancer therapy have changed the natural history of malignant disease and prolonged the life span of cancer patients. The meninges can either be infiltrated by contiguous extension of primary tumours of the central nervous system, paranasal sinuses and skull base origin or can be diffusely infiltrated from haematogenous dissemination from distant primary malignancies. <span class="hlt">Imaging</span> in these patients provides crucial information in planning management. This article reviews the pertinent anatomy that underlies <span class="hlt">imaging</span> findings, discusses the mechanism of meningeal metastasis and highlights different <span class="hlt">imaging</span> patterns of meningeal carcinomatosis and the pitfalls. PMID:19965290</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/19965290','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/19965290"><span>Meninges in cancer <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Mahendru, G; Chong, V</p>
         <p>2009-10-02</p>
         <p>Primary malignant tumours arising from the meninges are distinctly uncommon, and when they occur, they are usually sarcomas. In contrast, metastatic meningeal involvement is increasingly seen as advances in cancer therapy have changed the natural history of malignant disease and prolonged the life span of cancer patients. The meninges can either be infiltrated by contiguous extension of primary tumours of the central nervous system, paranasal sinuses and skull base origin or can be diffusely infiltrated from haematogenous dissemination from distant primary malignancies. <span class="hlt">Imaging</span> in these patients provides crucial information in planning management. This article reviews the pertinent anatomy that underlies <span class="hlt">imaging</span> findings, discusses the mechanism of meningeal metastasis and highlights different <span class="hlt">imaging</span> patterns of meningeal carcinomatosis and the pitfalls.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_19");'>19</a></li>
      <li><a href="#" onclick='return showDiv("page_20");'>20</a></li>
      <li class="active"><span>21</span></li>
   <li><a href="#" onclick='return showDiv("page_22");'>22</a></li>
      <li><a href="#" onclick='return showDiv("page_23");'>23</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_21 -->
   <div id="page_22" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_20");'>20</a></li>
      <li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li class="active"><span>22</span></li>
   <li><a href="#" onclick='return showDiv("page_23");'>23</a></li>
      <li><a href="#" onclick='return showDiv("page_24");'>24</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="421">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2014NatMa..13..125W','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2014NatMa..13..125W"><span><span class="hlt">Imaging</span> macrophages with nanoparticles</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Weissleder, Ralph; Nahrendorf, Matthias; Pittet, Mikael J.</p>
         <p>2014-02-01</p>
         <p>Nanomaterials have much to offer, not only in deciphering innate immune cell biology and tracking cells, but also in advancing personalized clinical care by providing diagnostic and prognostic information, quantifying treatment efficacy and designing better therapeutics. This Review presents different types of nanomaterial, their biological properties and their applications for <span class="hlt">imaging</span> macrophages in human diseases, including cancer, atherosclerosis, myocardial infarction, aortic aneurysm, diabetes and other conditions. We anticipate that future needs will include the development of nanomaterials that are specific for immune cell subsets and can be used as <span class="hlt">imaging</span> surrogates for nanotherapeutics. New in vivo <span class="hlt">imaging</span> clinical tools for noninvasive macrophage quantification are thus ultimately expected to become relevant to predicting patients' clinical outcome, defining treatment options and monitoring responses to therapy.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/872414','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/872414"><span>Multispectral <span class="hlt">imaging</span> probe</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Sandison, David R.; Platzbecker, Mark R.; Descour, Michael R.; Armour, David L.; Craig, Marcus J.; Richards-Kortum, Rebecca</p>
         <p>1999-01-01</p>
         <p>A multispectral <span class="hlt">imaging</span> probe delivers a range of wavelengths of excitation light to a target and collects a range of expressed light wavelengths. The multispectral <span class="hlt">imaging</span> probe is adapted for mobile use and use in confined spaces, and is sealed against the effects of hostile environments. The multispectral <span class="hlt">imaging</span> probe comprises a housing that defines a sealed volume that is substantially sealed from the surrounding environment. A beam splitting device mounts within the sealed volume. Excitation light is directed to the beam splitting device, which directs the excitation light to a target. Expressed light from the target reaches the beam splitting device along a path coaxial with the path traveled by the excitation light from the beam splitting device to the target. The beam splitting device directs expressed light to a collection subsystem for delivery to a detector.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/6171114','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/biblio/6171114"><span>Multispectral <span class="hlt">imaging</span> probe</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Sandison, D.R.; Platzbecker, M.R.; Descour, M.R.; Armour, D.L.; Craig, M.J.; Richards-Kortum, R.</p>
         <p>1999-07-27</p>
         <p>A multispectral <span class="hlt">imaging</span> probe delivers a range of wavelengths of excitation light to a target and collects a range of expressed light wavelengths. The multispectral <span class="hlt">imaging</span> probe is adapted for mobile use and use in confined spaces, and is sealed against the effects of hostile environments. The multispectral <span class="hlt">imaging</span> probe comprises a housing that defines a sealed volume that is substantially sealed from the surrounding environment. A beam splitting device mounts within the sealed volume. Excitation light is directed to the beam splitting device, which directs the excitation light to a target. Expressed light from the target reaches the beam splitting device along a path coaxial with the path traveled by the excitation light from the beam splitting device to the target. The beam splitting device directs expressed light to a collection subsystem for delivery to a detector. 8 figs.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19980203084','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19980203084"><span>Flame <span class="hlt">Imaging</span> System</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Barnes, Heidi L. (Inventor); Smith, Harvey S. (Inventor)</p>
         <p>1998-01-01</p>
         <p>A system for <span class="hlt">imaging</span> a flame and the background scene is discussed. The flame <span class="hlt">imaging</span> system consists of two charge-coupled-device (CCD) cameras. One camera uses a 800 nm long pass filter which during overcast conditions blocks sufficient background light so the hydrogen flame is brighter than the background light, and the second CCD camera uses a 1100 nm long pass filter, which blocks the solar background in full sunshine conditions such that the hydrogen flame is brighter than the solar background. Two electronic viewfinders convert the signal from the cameras into a visible <span class="hlt">image</span>. The operator can select the appropriate filtered camera to use depending on the current light conditions. In addition, a narrow band pass filtered InGaAs sensor at 1360 nm triggers an audible alarm and a flashing LED if the sensor detects a flame, providing additional flame detection so the operator does not overlook a small flame.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/11813680','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/11813680"><span>Dental digital radiographic <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Mauriello, S M; Platin, E</p>
         <p>2001-01-01</p>
         <p>Radiographs are an important adjunct to providing oral health care for the total patient. Historically, radiographic <span class="hlt">images</span> have been produced using film-based systems. However, in recent years, with the arrival of new technologies, many practitioners have begun to incorporate digital radiographic <span class="hlt">imaging</span> into their practices. Since dental hygienists are primarily responsible for exposing and processing radiographs in the provision of dental hygiene care, it is imperative that they become knowledgeable on the use and application of digital <span class="hlt">imaging</span> in patient care and record keeping. The purpose of this course is to provide a comprehensive overview of digital radiography in dentistry. Specific components addressed are technological features, diagnostic software, advantages and disadvantages, technique procedures, and legal implications.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19970028544','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19970028544"><span>Soot Volume Fraction <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Greenberg, Paul S.; Ku, Jerry C.</p>
         <p>1994-01-01</p>
         <p>A new technique is described for the full-field determination of soot volume fractions via laser extinction measurements. This technique differs from previously reported point-wise methods in that a two-dimensional array (i.e., <span class="hlt">image</span>) of data is acquired simultaneously. In this fashion, the net data rate is increased, allowing the study of time-dependent phenomena and the investigation of spatial and temporal correlations. A telecentric <span class="hlt">imaging</span> configuration is employed to provide depth-invariant magnification and to permit the specification of the collection angle for scattered light. To improve the threshold measurement sensitivity, a method is employed to suppress undesirable coherent <span class="hlt">imaging</span> effects. A discussion of the tomographic inversion process is provided, including the results obtained from numerical simulation. Results obtained with this method from an ethylene diffusion flame are shown to be in close agreement with those previously obtained by sequential point-wise interrogation.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20125.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20125.html"><span>Dawn HAMO <span class="hlt">Image</span> 63</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-11-19</p>
         <p>This <span class="hlt">image</span> of Ceres from NASA's Dawn spacecraft shows hummocky terrain -- a surface covered in low, rounded hills -- with numerous impact craters of varying sizes. The two biggest craters display central peaks and many places where masses of material have collapsed and slid downward along their walls and floors -- a phenomenon geologists call "mass wasting". The sharp crater at upper right is surrounded by smooth ejecta with a streaky texture to the south. A graben -- what geologists call a linear feature where terrain has dropped -- measuring 2 to 5 miles (3 to 8 kilometers) in width, and two prominent scarps, or linear, cliff-like slopes, are located in the southeastern (lower right) part of the <span class="hlt">image</span>. Dawn took this <span class="hlt">image</span> on Oct. 5, 2015, from an altitude of 915 miles (1,470 kilometers). It has a resolution of 450 feet (140 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20125</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1989sntm.work...90F','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1989sntm.work...90F"><span>A multicolor <span class="hlt">imaging</span> pyrometer</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Frish, Michael B.; Frank, Jonathan H.</p>
         <p>1989-06-01</p>
         <p>A multicolor <span class="hlt">imaging</span> pyrometer was designed for accurately and precisely measuring the temperature distribution histories of small moving samples. The device projects six different color <span class="hlt">images</span> of the sample onto a single charge coupled device array that provides an RS-170 video signal to a computerized frame grabber. The computer automatically selects which one of the six <span class="hlt">images</span> provides useful data, and converts that information to a temperature map. By measuring the temperature of molten aluminum heated in a kiln, a breadboard version of the device was shown to provide high accuracy in difficult measurement situations. It is expected that this pyrometer will ultimately find application in measuring the temperature of materials undergoing radiant heating in a microgravity acoustic levitation furnace.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19900008589','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19900008589"><span>A multicolor <span class="hlt">imaging</span> pyrometer</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Frish, Michael B.; Frank, Jonathan H.</p>
         <p>1989-01-01</p>
         <p>A multicolor <span class="hlt">imaging</span> pyrometer was designed for accurately and precisely measuring the temperature distribution histories of small moving samples. The device projects six different color <span class="hlt">images</span> of the sample onto a single charge coupled device array that provides an RS-170 video signal to a computerized frame grabber. The computer automatically selects which one of the six <span class="hlt">images</span> provides useful data, and converts that information to a temperature map. By measuring the temperature of molten aluminum heated in a kiln, a breadboard version of the device was shown to provide high accuracy in difficult measurement situations. It is expected that this pyrometer will ultimately find application in measuring the temperature of materials undergoing radiant heating in a microgravity acoustic levitation furnace.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20070019311','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20070019311"><span>Spatial Phase <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>2006-01-01</p>
         <p>Frequently, scientists grow crystals by dissolving a protein in a specific liquid solution, and then allowing that solution to evaporate. The methods used next have been, variously, invasive (adding a dye that is absorbed by the protein), destructive (crushing protein/salt-crystal mixtures and observing differences between the crushing of salt and protein), or costly and time-consuming (X-ray crystallography). In contrast to these methods, a new technology for monitoring protein growth, developed in part through NASA Small Business Innovation Research (SBIR) funding from Marshall Space Flight Center, is noninvasive, nondestructive, rapid, and more cost effective than X-ray analysis. The partner for this SBIR, Photon-X, Inc., of Huntsville, Alabama, developed spatial phase <span class="hlt">imaging</span> technology that can monitor crystal growth in real time and in an automated mode. Spatial phase <span class="hlt">imaging</span> scans for flaws quickly and produces a 3-D structured <span class="hlt">image</span> of a crystal, showing volumetric growth analysis for future automated growth.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/29629796','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/29629796"><span>Advances in Pancreatic CT <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Almeida, Renata R; Lo, Grace C; Patino, Manuel; Bizzo, Bernardo; Canellas, Rodrigo; Sahani, Dushyant V</p>
         <p>2018-07-01</p>
         <p>The purpose of this article is to discuss the advances in CT acquisition and <span class="hlt">image</span> postprocessing as they apply to <span class="hlt">imaging</span> the pancreas and to conceptualize the role of radiogenomics and machine learning in pancreatic <span class="hlt">imaging</span>. CT is the preferred <span class="hlt">imaging</span> modality for assessment of pancreatic diseases. Recent advances in CT (dual-energy CT, CT perfusion, CT volumetry, and radiogenomics) and emerging computational algorithms (machine learning) have the potential to further increase the value of CT in pancreatic <span class="hlt">imaging</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016SoftX...5..101B','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016SoftX...5..101B"><span>Light-Field <span class="hlt">Imaging</span> Toolkit</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Bolan, Jeffrey; Hall, Elise; Clifford, Chris; Thurow, Brian</p>
         <p></p>
         <p>The Light-Field <span class="hlt">Imaging</span> Toolkit (LFIT) is a collection of MATLAB functions designed to facilitate the rapid processing of raw light field <span class="hlt">images</span> captured by a plenoptic camera. An included graphical user interface streamlines the necessary post-processing steps associated with plenoptic <span class="hlt">images</span>. The generation of perspective shifted views and computationally refocused <span class="hlt">images</span> is supported, in both single <span class="hlt">image</span> and animated formats. LFIT performs necessary calibration, interpolation, and structuring steps to enable future applications of this technology.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20150000508','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20150000508"><span>Position Estimation Using <span class="hlt">Image</span> Derivative</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Mortari, Daniele; deDilectis, Francesco; Zanetti, Renato</p>
         <p>2015-01-01</p>
         <p>This paper describes an <span class="hlt">image</span> processing algorithm to process Moon and/or Earth <span class="hlt">images</span>. The theory presented is based on the fact that Moon hard edge points are characterized by the highest values of the <span class="hlt">image</span> derivative. Outliers are eliminated by two sequential filters. Moon center and radius are then estimated by nonlinear least-squares using circular sigmoid functions. The proposed <span class="hlt">image</span> processing has been applied and validated using real and synthetic Moon <span class="hlt">images</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=20040001430&hterms=SEWAGE&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D30%26Ntt%3DSEWAGE','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=20040001430&hterms=SEWAGE&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D30%26Ntt%3DSEWAGE"><span>Coastal Research <span class="hlt">Imaging</span> Spectrometer</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>2002-01-01</p>
         <p>The Coastal Research <span class="hlt">Imaging</span> Spectrometer (CRIS) is an airborne remote-sensing system designed specifically for research on the physical, chemical, and biological characteristics of coastal waters. The CRIS includes a visible-light hyperspectral <span class="hlt">imaging</span> subsystem for measuring the color of water, which contains information on the biota, sediment, and nutrient contents of the water. The CRIS also includes an infrared <span class="hlt">imaging</span> subsystem, which provides information on the temperature of the water. The combination of measurements enables investigation of biological effects of both natural and artificial flows of water from land into the ocean, including diffuse and point-source flows that may contain biological and/or chemical pollutants. Temperature is an important element of such measurements because temperature contrasts can often be used to distinguish among flows from different sources: for example, a sewage outflow could manifest itself in spectral <span class="hlt">images</span> as a local high-temperature anomaly.anomaly. Both the visible and infrared subsystems scan in "pushbroom" mode: that is, an aircraft carrying the system moves along a ground track, the system is aimed downward, and <span class="hlt">image</span> data are acquired in acrosstrack linear arrays of pixels. Both subsystems operate at a frame rate of 30 Hz. The infrared and visible-light optics are adjusted so that both subsystems are aimed at the same moving swath, which has across-track angular width of 15. Data from the infrared and visible <span class="hlt">imaging</span> subsystems are stored in the same file along with aircraft-position data acquired by a Global Positioning System receiver. The combination of the three sets of data is used to construct infrared and hyperspectral maps of scanned areas shown.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/11003510','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/11003510"><span>Direct <span class="hlt">imaging</span> of explosives.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Knapp, E A; Moler, R B; Saunders, A W; Trower, W P</p>
         <p>2000-01-01</p>
         <p>Any technique that can detect nitrogen concentrations can screen for concealed explosives. However, such a technique would have to be insensitive to metal, both encasing and incidental. If <span class="hlt">images</span> of the nitrogen concentrations could be captured, then, since form follows function, a robust screening technology could be developed. However these <span class="hlt">images</span> would have to be sensitive to the surface densities at or below that of the nitrogen contained in buried anti-personnel mines or of the SEMTEX that brought down Pan Am 103, approximately 200 g. Although the ability to <span class="hlt">image</span> in three-dimensions would somewhat reduce false positives, capturing collateral <span class="hlt">images</span> of carbon and oxygen would virtually assure that nitrogenous non-explosive material like fertilizer, Melmac dinnerware, and salami could be eliminated. We are developing such an instrument, the Nitrogen Camera, which has met experimentally these criteria with the exception of providing oxygen <span class="hlt">images</span>, which awaits the availability of a sufficiently energetic light source. Our Nitrogen Camera technique uses an electron accelerator to produce photonuclear reactions whose unique decays it registers. Clearly if our Nitrogen Camera is made mobile, it could be effective in detecting buried mines, either in an active battlefield situation or in the clearing of abandoned military munitions. Combat operations require that a swathe the width of an armored vehicle, 5 miles deep, be screened in an hour, which is within our camera's scanning speed. Detecting abandoned munitions is technically easier as it is free from the onerous speed requirement. We describe here our Nitrogen Camera and show its 180 pixel intensity <span class="hlt">images</span> of elemental nitrogen in a 200 g mine simulant and in a 125 g stick of SEMTEX. We also report on our progress in creating a lorry transportable 70 MeV electron racetrack microtron, the principal enabling technology that will allow our Nitrogen Camera to be deployed in the field.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20130.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20130.html"><span>Dawn HAMO <span class="hlt">Image</span> 67</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-12-01</p>
         <p>The tall, cone-shaped mountain Ahuna Mons is seen in this <span class="hlt">image</span> taken by NASA's Dawn spacecraft. Ahuna Mons, named for the traditional post-harvest festival of the Sumi tribe of Nagaland in India, is about 4 miles (6 kilometers) tall and 12 miles (20 kilometers) in diameter. Dawn took this <span class="hlt">image</span> on Oct. 14, 2015, from an altitude of 915 miles (1,470 kilometers). It has a resolution of 450 feet (140 meters) per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20130</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1998SPIE.3299..615S','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1998SPIE.3299..615S"><span>Visual exploration of <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Suaste-Gomez, Ernesto; Leybon, Jaime I.; Rodriguez, D.</p>
         <p>1998-07-01</p>
         <p>Visual scanpath has been an important work applied in neuro- ophthalmic and psychological studies. This is because it has been working like a tool to validate some pathologies such as visual perception in color or black/white <span class="hlt">images</span>; color blindness; etc. On the other hand, this tool has reached a big field of applications such as marketing. The scanpath over a specific picture, shows the observer interest in color, shapes, letter size, etc.; even tough the picture be among a group of <span class="hlt">images</span>, this tool has demonstrated to be helpful to catch people interest over a specific advertisement.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/871063','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/servlets/purl/871063"><span>Magnetic <span class="hlt">imager</span> and method</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Powell, James; Reich, Morris; Danby, Gordon</p>
         <p>1997-07-22</p>
         <p>A magnetic <span class="hlt">imager</span> 10 includes a generator 18 for practicing a method of applying a background magnetic field over a concealed object, with the object being effective to locally perturb the background field. The <span class="hlt">imager</span> 10 also includes a sensor 20 for measuring perturbations of the background field to detect the object. In one embodiment, the background field is applied quasi-statically. And, the magnitude or rate of change of the perturbations may be measured for determining location, size, and/or condition of the object.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20122.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20122.html"><span>Dawn HAMO <span class="hlt">Image</span> 60</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-11-16</p>
         <p>Dantu crater on Ceres, seen here at left, reveals structures hinting at tectonic processes that formed the dwarf planet's surface. Linear structures are spread over the crater floor. Outside the crater's rim, the occurrence of linear structures continues the in form of scarps (linear, cliff-like slopes) and ridges. Dantu's diameter is 78 miles (125 kilometers). The <span class="hlt">image</span> was taken by NASA's Dawn spacecraft on Oct. 3, 2015, from an altitude of 915 miles (1,470 kilometers). It has a resolution of 450 feet (140 meters) per pixel. The <span class="hlt">image</span> is located at 31 degrees north latitude, 149 degrees east longitude. http://photojournal.jpl.nasa.gov/catalog/PIA20122</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA245661','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA245661"><span>Symbolic <span class="hlt">Image</span> Understanding</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1991-11-01</p>
         <p>publication. APPROVED: a LEE A. UVANNI Project Engineer FOR THE COMMANDER: GARRY W. BARRINGER Technical Director Intelligence & Reconnaissance...f Od1cAtl nd ir-&#8217bm a UNl tofU~rtaw .&#34t Pu&#8217 o scrxr± ing twra fa revrl r Jt,= seagrg d un zla souLces gahwtW&#34 r T , iUm rm , rruk4 c adiwvctws coa w...1990j matches straight lines extracted from an <span class="hlt">image</span> with model lines r projected to the <span class="hlt">image</span> plane using an assumed location of the camera. This</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_20");'>20</a></li>
      <li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li class="active"><span>22</span></li>
   <li><a href="#" onclick='return showDiv("page_23");'>23</a></li>
      <li><a href="#" onclick='return showDiv("page_24");'>24</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_22 -->
   <div id="page_23" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li><a href="#" onclick='return showDiv("page_22");'>22</a></li>
      <li class="active"><span>23</span></li>
   <li><a href="#" onclick='return showDiv("page_24");'>24</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>25</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="441">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19930007523','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19930007523"><span>Advanced <span class="hlt">imaging</span> system</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1992-01-01</p>
         <p>This document describes the Advanced <span class="hlt">Imaging</span> System CCD based camera. The AIS1 camera system was developed at Photometric Ltd. in Tucson, Arizona as part of a Phase 2 SBIR contract No. NAS5-30171 from the NASA/Goddard Space Flight Center in Greenbelt, Maryland. The camera project was undertaken as a part of the Space Telescope <span class="hlt">Imaging</span> Spectrograph (STIS) project. This document is intended to serve as a complete manual for the use and maintenance of the camera system. All the different parts of the camera hardware and software are discussed and complete schematics and source code listings are provided.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA110811','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA110811"><span><span class="hlt">Image</span> Compression Research.</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1981-08-01</p>
         <p>FOR THE COI4MANDER:9P 4 4:JOHN P. HUSS Acting Chief, Plans Office If your address has changed or if you wish to be removed from the .RADC mailing list...these transformations are not sensitive to changes in local <span class="hlt">image</span> characteristics, and so may work much better on some <span class="hlt">image</span> blocks than on others. The...i,j=l i n i j=1 ij The feature u(x) is a good measure of bloc&#8217, &#34 busyness &#34 and for this reason provides a high correlation with block information</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5594016','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5594016"><span><span class="hlt">Imaging</span> of shoulder instability</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Martínez Martínez, Alberto; Tomás Muñoz, Pablo; Pozo Sánchez, José; Zarza Pérez, Antonio</p>
         <p>2017-01-01</p>
         <p>This extended review tries to cover the <span class="hlt">imaging</span> findings of the wide range of shoulder injuries secondary to shoulder joint instability. Usefulness of the different <span class="hlt">imaging</span> methods is stressed, including radiography, computed tomography (CT) and magnetic resonance. The main topics to be covered include traumatic, atraumatic and minor instability syndromes. Radiography may show bone abnormalities associated to instability, including developmental and post-traumatic changes. CT is the best technique depicting and quantifying skeletal changes. MR-arthrography is the main tool in diagnosing the shoulder instability injuries. PMID:28932699</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20080040682','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20080040682"><span>Neutron <span class="hlt">Imaging</span> Camera</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Hunter, Stanley; deNolfo, G. A.; Barbier, L. M.; Link, J. T.; Son, S.; Floyd, S. R.; Guardala, N.; Skopec, M.; Stark, B.</p>
         <p>2008-01-01</p>
         <p>The Neutron <span class="hlt">Imaging</span> Camera (NIC) is based on the Three-dimensional Track <span class="hlt">Imager</span> (3DTI) technology developed at GSFC for gamma-ray astrophysics applications. The 3-DTI, a large volume time-projection chamber, provides accurate, approximately 0.4 mm resolution, 3-D tracking of charged particles. The incident direction of fast neutrons, En > 0.5 MeV, are reconstructed from the momenta and energies of the proton and triton fragments resulting from (sup 3)He(n,p) (sup 3)H interactions in the 3-DTI volume. The performance of the NIC from laboratory and accelerator tests is presented.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/9274093','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/9274093"><span><span class="hlt">Imaging</span> of digital neuromas.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Baron, R L; Galinski, A W; Vlahos, M</p>
         <p>1997-08-01</p>
         <p>In this case presentation, an incision was made on the nonweightbearing surface of the hallux directly over the mass in question, just long enough to allow for the isolation of the entire mass. This permitted easier identification of the mass and enabled dissection of the abnormal tissue and excision of only the tumor with a minimum of tissue trauma. Healing was uneventful and expedient largely because of the reduced tissue handling. Prior to the advent of magnetic resonance <span class="hlt">imaging</span>, this type of preoperative detailed surgical mapping would not have been possible. Continuing improvements in magnetic resonance <span class="hlt">imaging</span> hold great and increasing promise.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=20040161529&hterms=blackhole&qs=N%3D0%26Ntk%3DAll%26Ntx%3Dmode%2Bmatchall%26Ntt%3Dblackhole','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=20040161529&hterms=blackhole&qs=N%3D0%26Ntk%3DAll%26Ntx%3Dmode%2Bmatchall%26Ntt%3Dblackhole"><span>MAXIM: The Blackhole <span class="hlt">Imager</span></span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Gendreau, Keith; Cash, Webster; Gorenstein, Paul; Windt, David; Kaaret, Phil; Reynolds, Chris</p>
         <p>2004-01-01</p>
         <p>The Beyond Einstein Program in NASA's Office of Space Science Structure and Evolution of the Universe theme spells out the top level scientific requirements for a Black Hole <span class="hlt">Imager</span> in its strategic plan. The MAXIM mission will provide better than one tenth of a microarcsecond <span class="hlt">imaging</span> in the X-ray band in order to satisfy these requirements. We will overview the driving requirements to achieve these goals and ultimately resolve the event horizon of a supermassive black hole. We will present the current status of this effort that includes a study of a baseline design as well as two alternative approaches.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA576914','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA576914"><span>Photoacoustic <span class="hlt">Imaging</span> of Epilepsy</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>2013-04-01</p>
         <p>mouse brain with the skin and skull intact,” Opt. Lett. 28(19), 1739–1741 (2003). 5. Q. Zhang, Z. Liu, P. R. Carney, Z. Yuan, H. Chen, S. N. Roper, and...<span class="hlt">imaging</span> at centimeter scale depths. To date PAT has been applied to the detection of breast cancer, skin cancer and osteoarthritis in humans [1–3...the hemodynamic changes and reveal the 3D structures in the rat brain. Two small rats (~40g) were <span class="hlt">imaged</span> with intact skull and skin but hairs on the</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/29157545','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/29157545"><span>Temporomandibular Joint <span class="hlt">Imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Tamimi, Dania; Jalali, Elnaz; Hatcher, David</p>
         <p>2018-01-01</p>
         <p>The temporomandibular joint (TMJ) is an anatomically and biomechanically complex structure. Understanding how this structure grows and functions is essential to accurate radiographic evaluation. This article discusses the anatomy, function, and growth and development of the TMJ and how growth changes can affect the morphology of the craniofacial structures. Accordingly, the radiographic appearance of the entities that may alter the TMJ are discussed, including developmental, degenerative, inflammatory, and traumatic changes. Both osseous <span class="hlt">imaging</span> and soft tissue <span class="hlt">imaging</span> are shown. Copyright © 2017 Elsevier Inc. All rights reserved.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19840008322','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19840008322"><span>Spaceborne <span class="hlt">Imaging</span> Radar Symposium</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Elachi, C.</p>
         <p>1983-01-01</p>
         <p>An overview of the present state of the art in the different scientific and technological fields related to spaceborne <span class="hlt">imaging</span> radars was presented. The data acquired with the SEASAT SAR (1978) and Shuttle <span class="hlt">Imaging</span> Radar, SIR-A (1981) clearly demonstrated the important emphasis in the 80's is going to be on in-depth research investigations conducted with the more flexible and sophisticated SIR series instruments and on long term monitoring of geophysical phenomena conducted from free-flying platforms such as ERS-1 and RADARSAT.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/5722333-ear-diagnostic-imaging','SCIGOV-STC'); return false;" href="https://www.osti.gov/biblio/5722333-ear-diagnostic-imaging"><span>The ear: Diagnostic <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Vignaud, J.; Jardin, C.; Rosen, L.</p>
         <p>1986-01-01</p>
         <p>This is an English translation of volume 17-1 of Traite de radiodiagnostic and represents a reasonably complete documentation of the diseases of the temporal bone that have <span class="hlt">imaging</span> manifestations. The book begins with chapters on embryology, anatomy and radiography anatomy; it continues with blood supply and an overview of temporal bone pathology. Subsequent chapters cover malformations, trauma, infections, tumors, postoperative changes, glomus tumors, vertebasilar insufficiency, and facial nerve canal lesions. A final chapter demonstrates and discusses magnetic resonance <span class="hlt">images</span> of the ear and cerebellopontine angle.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1990SPIE.1232..258M','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1990SPIE.1232..258M"><span>Multimodality <span class="hlt">image</span> display station</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Myers, H. Joseph</p>
         <p>1990-07-01</p>
         <p>The Multi-modality <span class="hlt">Image</span> Display Station (MIDS) is designed for the use of physicians outside of the radiology department. Connected to a local area network or a host computer, it provides speedy access to digitized radiology <span class="hlt">images</span> and written diagnostics needed by attending and consulting physicians near the patient bedside. Emphasis has been placed on low cost, high performance and ease of use. The work is being done as a joint study with the University of Texas Southwestern Medical Center at Dallas, and as part of a joint development effort with the Mayo Clinic. MIDS is a prototype, and should not be assumed to be an IBM product.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://www.dtic.mil/docs/citations/ADA034748','DTIC-ST'); return false;" href="http://www.dtic.mil/docs/citations/ADA034748"><span><span class="hlt">Image</span> Processing Research</span></a></p>
      <p><a target="_blank" href="http://www.dtic.mil/">DTIC Science & Technology</a></p>
      <p></p>
         <p>1975-09-30</p>
         <p>systems a linear model results in an object f being mappad into an <span class="hlt">image</span> _ by a point spread function matrix H. Thus with noise j +Hf +n (1) The simplest... linear models for <span class="hlt">imaging</span> systems are given by space invariant point spread functions (SIPSF) in which case H is block circulant. If the linear model is...Ij,...,k-IM1 is a set of two dimensional indices each distinct and prior to k. Modeling Procedare: To derive the linear predictor (block LP of figure</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/biblio/516941','DOE-PATENT-XML'); return false;" href="https://www.osti.gov/biblio/516941"><span>Magnetic <span class="hlt">imager</span> and method</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/doepatents">DOEpatents</a></p>
      <p>Powell, J.; Reich, M.; Danby, G.</p>
         <p>1997-07-22</p>
         <p>A magnetic <span class="hlt">imager</span> includes a generator for practicing a method of applying a background magnetic field over a concealed object, with the object being effective to locally perturb the background field. The <span class="hlt">imager</span> also includes a sensor for measuring perturbations of the background field to detect the object. In one embodiment, the background field is applied quasi-statically. And, the magnitude or rate of change of the perturbations may be measured for determining location, size, and/or condition of the object. 25 figs.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/27837126','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/27837126"><span>Medical <span class="hlt">Imaging</span> and Infertility.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Peterson, Rebecca</p>
         <p>2016-11-01</p>
         <p>Infertility affects many couples, and medical <span class="hlt">imaging</span> plays a vital role in its diagnosis and treatment. Radiologic technologists benefit from having a broad understanding of infertility risk factors and causes. This article describes the typical structure and function of the male and female reproductive systems, as well as congenital and acquired conditions that could lead to a couple's inability to conceive. Medical <span class="hlt">imaging</span> procedures performed for infertility diagnosis are discussed, as well as common interventional options available to patients. © 2016 American Society of Radiologic Technologists.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/27717679','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/27717679"><span>Cancer heterogeneity and <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>O'Connor, James P B</p>
         <p>2017-04-01</p>
         <p>There is interest in identifying and quantifying tumor heterogeneity at the genomic, tissue pathology and clinical <span class="hlt">imaging</span> scales, as this may help better understand tumor biology and may yield useful biomarkers for guiding therapy-based decision making. This review focuses on the role and value of using x-ray, CT, MRI and PET based <span class="hlt">imaging</span> methods that identify, measure and map tumor heterogeneity. In particular we highlight the potential value of these techniques and the key challenges required to validate and qualify these biomarkers for clinical use. Copyright © 2016. Published by Elsevier Ltd.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2018OptCo.410...35T','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2018OptCo.410...35T"><span><span class="hlt">Image</span> reconstruction of dynamic infrared single-pixel <span class="hlt">imaging</span> system</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Tong, Qi; Jiang, Yilin; Wang, Haiyan; Guo, Limin</p>
         <p>2018-03-01</p>
         <p>Single-pixel <span class="hlt">imaging</span> technique has recently received much attention. Most of the current single-pixel <span class="hlt">imaging</span> is aimed at relatively static targets or the <span class="hlt">imaging</span> system is fixed, which is limited by the number of measurements received through the single detector. In this paper, we proposed a novel dynamic compressive <span class="hlt">imaging</span> method to solve the <span class="hlt">imaging</span> problem, where exists <span class="hlt">imaging</span> system motion behavior, for the infrared (IR) rosette scanning system. The relationship between adjacent target <span class="hlt">images</span> and scene is analyzed under different system movement scenarios. These relationships are used to build dynamic compressive <span class="hlt">imaging</span> models. Simulation results demonstrate that the proposed method can improve the reconstruction quality of IR <span class="hlt">image</span> and enhance the contrast between the target and the background in the presence of system movement.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1992SPIE.1652..443C','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1992SPIE.1652..443C"><span><span class="hlt">Image</span> processing on the <span class="hlt">image</span> with pixel noise bits removed</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Chuang, Keh-Shih; Wu, Christine</p>
         <p>1992-06-01</p>
         <p>Our previous studies used statistical methods to assess the noise level in digital <span class="hlt">images</span> of various radiological modalities. We separated the pixel data into signal bits and noise bits and demonstrated visually that the removal of the noise bits does not affect the <span class="hlt">image</span> quality. In this paper we apply <span class="hlt">image</span> enhancement techniques on noise-bits-removed <span class="hlt">images</span> and demonstrate that the removal of noise bits has no effect on the <span class="hlt">image</span> property. The <span class="hlt">image</span> processing techniques used are gray-level look up table transformation, Sobel edge detector, and 3-D surface display. Preliminary results show no noticeable difference between original <span class="hlt">image</span> and noise bits removed <span class="hlt">image</span> using look up table operation and Sobel edge enhancement. There is a slight enhancement of the slicing artifact in the 3-D surface display of the noise bits removed <span class="hlt">image</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1988ITASS..36.1313K','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1988ITASS..36.1313K"><span>Robust <span class="hlt">image</span> modeling techniques with an <span class="hlt">image</span> restoration application</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Kashyap, Rangasami L.; Eom, Kie-Bum</p>
         <p>1988-08-01</p>
         <p>A robust parameter-estimation algorithm for a nonsymmetric half-plane (NSHP) autoregressive model, where the driving noise is a mixture of a Gaussian and an outlier process, is presented. The convergence of the estimation algorithm is proved. An algorithm to estimate parameters and original <span class="hlt">image</span> intensity simultaneously from the impulse-noise-corrupted <span class="hlt">image</span>, where the model governing the <span class="hlt">image</span> is not available, is also presented. The robustness of the parameter estimates is demonstrated by simulation. Finally, an algorithm to restore realistic <span class="hlt">images</span> is presented. The entire <span class="hlt">image</span> generally does not obey a simple <span class="hlt">image</span> model, but a small portion (e.g., 8 x 8) of the <span class="hlt">image</span> is assumed to obey an NSHP model. The original <span class="hlt">image</span> is divided into windows and the robust estimation algorithm is applied for each window. The restoration algorithm is tested by comparing it to traditional methods on several different <span class="hlt">images</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2010AIPC.1324..261B','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2010AIPC.1324..261B"><span><span class="hlt">Image</span> Viewer using Digital <span class="hlt">Imaging</span> and Communications in Medicine (DICOM)</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Baraskar, Trupti N.</p>
         <p>2010-11-01</p>
         <p>Digital <span class="hlt">Imaging</span> and Communications in Medicine is a standard for handling, storing, printing, and transmitting information in medical <span class="hlt">imaging</span>. The National Electrical Manufacturers Association holds the copyright to this standard. It was developed by the DICOM Standards committee. The other <span class="hlt">image</span> viewers cannot collectively store the <span class="hlt">image</span> details as well as the patient's information. So the <span class="hlt">image</span> may get separated from the details, but DICOM file format stores the patient's information and the <span class="hlt">image</span> details. Main objective is to develop a DICOM <span class="hlt">image</span> viewer. The <span class="hlt">image</span> viewer will open .dcm i.e. DICOM <span class="hlt">image</span> file and also will have additional features such as zoom in, zoom out, black and white inverter, magnifier, blur, B/W inverter, horizontal and vertical flipping, sharpening, contrast, brightness and .gif converter are incorporated.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5554542','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5554542"><span>NIH <span class="hlt">Image</span> to <span class="hlt">Image</span>J: 25 years of <span class="hlt">Image</span> Analysis</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Schneider, Caroline A.; Rasband, Wayne S.; Eliceiri, Kevin W.</p>
         <p>2017-01-01</p>
         <p>For the past twenty five years the NIH family of <span class="hlt">imaging</span> software, NIH <span class="hlt">Image</span> and <span class="hlt">Image</span>J have been pioneers as open tools for scientific <span class="hlt">image</span> analysis. We discuss the origins, challenges and solutions of these two programs, and how their history can serve to advise and inform other software projects. PMID:22930834</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li><a href="#" onclick='return showDiv("page_22");'>22</a></li>
      <li class="active"><span>23</span></li>
   <li><a href="#" onclick='return showDiv("page_24");'>24</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>25</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_23 -->
   <div id="page_24" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li><a href="#" onclick='return showDiv("page_22");'>22</a></li>
      <li><a href="#" onclick='return showDiv("page_23");'>23</a></li>
      <li class="active"><span>24</span></li>
   <li><a href="#" onclick='return showDiv("page_25");'>25</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="461">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2010SPIE.7723E..0DM','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2010SPIE.7723E..0DM"><span>Matching rendered and real world <span class="hlt">images</span> by digital <span class="hlt">image</span> processing</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Mitjà, Carles; Bover, Toni; Bigas, Miquel; Escofet, Jaume</p>
         <p>2010-05-01</p>
         <p>Recent advances in computer-generated <span class="hlt">images</span> (CGI) have been used in commercial and industrial photography providing a broad scope in product advertising. Mixing real world <span class="hlt">images</span> with those rendered from virtual space software shows a more or less visible mismatching between corresponding <span class="hlt">image</span> quality performance. Rendered <span class="hlt">images</span> are produced by software which quality performance is only limited by the resolution output. Real world <span class="hlt">images</span> are taken with cameras with some amount of <span class="hlt">image</span> degradation factors as lens residual aberrations, diffraction, sensor low pass anti aliasing filters, color pattern demosaicing, etc. The effect of all those <span class="hlt">image</span> quality degradation factors can be characterized by the system Point Spread Function (PSF). Because the <span class="hlt">image</span> is the convolution of the object by the system PSF, its characterization shows the amount of <span class="hlt">image</span> degradation added to any taken picture. This work explores the use of <span class="hlt">image</span> processing to degrade the rendered <span class="hlt">images</span> following the parameters indicated by the real system PSF, attempting to match both virtual and real world <span class="hlt">image</span> qualities. The system MTF is determined by the slanted edge method both in laboratory conditions and in the real picture environment in order to compare the influence of the working conditions on the device performance; an approximation to the system PSF is derived from the two measurements. The rendered <span class="hlt">images</span> are filtered through a Gaussian filter obtained from the taking system PSF. Results with and without filtering are shown and compared measuring the contrast achieved in different final <span class="hlt">image</span> regions.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/17281951','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/17281951"><span><span class="hlt">Image</span> registration: enabling technology for <span class="hlt">image</span> guided surgery and therapy.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Sauer, Frank</p>
         <p>2005-01-01</p>
         <p><span class="hlt">Imaging</span> looks inside the patient's body, exposing the patient's anatomy beyond what is visible on the surface. Medical <span class="hlt">imaging</span> has a very successful history for medical diagnosis. It also plays an increasingly important role as enabling technology for minimally invasive procedures. Interventional procedures (e.g. catheter based cardiac interventions) are traditionally supported by intra-procedure <span class="hlt">imaging</span> (X-ray fluoro, ultrasound). There is realtime feedback, but the <span class="hlt">images</span> provide limited information. Surgical procedures are traditionally supported with pre-operative <span class="hlt">images</span> (CT, MR). The <span class="hlt">image</span> quality can be very good; however, the link between <span class="hlt">images</span> and patient has been lost. For both cases, <span class="hlt">image</span> registration can play an essential role -augmenting intra-op <span class="hlt">images</span> with pre-op <span class="hlt">images</span>, and mapping pre-op <span class="hlt">images</span> to the patient's body. We will present examples of both approaches from an application oriented perspective, covering electrophysiology, radiation therapy, and neuro-surgery. Ultimately, as the boundaries between interventional radiology and surgery are becoming blurry, also the different methods for <span class="hlt">image</span> guidance will merge. <span class="hlt">Image</span> guidance will draw upon a combination of pre-op and intra-op <span class="hlt">imaging</span> together with magnetic or optical tracking systems, and enable precise minimally invasive procedures. The information is registered into a common coordinate system, and allows advanced methods for visualization such as augmented reality or advanced methods for therapy delivery such as robotics.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4631490','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=4631490"><span>Stereoscopic Integrated <span class="hlt">Imaging</span> Goggles for Multimodal Intraoperative <span class="hlt">Image</span> Guidance</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Mela, Christopher A.; Patterson, Carrie; Thompson, William K.; Papay, Francis; Liu, Yang</p>
         <p>2015-01-01</p>
         <p>We have developed novel stereoscopic wearable multimodal intraoperative <span class="hlt">imaging</span> and display systems entitled Integrated <span class="hlt">Imaging</span> Goggles for guiding surgeries. The prototype systems offer real time stereoscopic fluorescence <span class="hlt">imaging</span> and color reflectance <span class="hlt">imaging</span> capacity, along with in vivo handheld microscopy and ultrasound <span class="hlt">imaging</span>. With the Integrated <span class="hlt">Imaging</span> Goggle, both wide-field fluorescence <span class="hlt">imaging</span> and in vivo microscopy are provided. The real time ultrasound <span class="hlt">images</span> can also be presented in the goggle display. Furthermore, real time goggle-to-goggle stereoscopic video sharing is demonstrated, which can greatly facilitate telemedicine. In this paper, the prototype systems are described, characterized and tested in surgeries in biological tissues ex vivo. We have found that the system can detect fluorescent targets with as low as 60 nM indocyanine green and can resolve structures down to 0.25 mm with large FOV stereoscopic <span class="hlt">imaging</span>. The system has successfully guided simulated cancer surgeries in chicken. The Integrated <span class="hlt">Imaging</span> Goggle is novel in 4 aspects: it is (a) the first wearable stereoscopic wide-field intraoperative fluorescence <span class="hlt">imaging</span> and display system, (b) the first wearable system offering both large FOV and microscopic <span class="hlt">imaging</span> simultaneously, (c) the first wearable system that offers both ultrasound <span class="hlt">imaging</span> and fluorescence <span class="hlt">imaging</span> capacities, and (d) the first demonstration of goggle-to-goggle communication to share stereoscopic views for medical guidance. PMID:26529249</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5904451','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=5904451"><span>Vaccine <span class="hlt">Images</span> on Twitter: Analysis of What <span class="hlt">Images</span> are Shared</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Dredze, Mark</p>
         <p>2018-01-01</p>
         <p>Background Visual imagery plays a key role in health communication; however, there is little understanding of what aspects of vaccine-related <span class="hlt">images</span> make them effective communication aids. Twitter, a popular venue for discussions related to vaccination, provides numerous <span class="hlt">images</span> that are shared with tweets. Objective The objectives of this study were to understand how <span class="hlt">images</span> are used in vaccine-related tweets and provide guidance with respect to the characteristics of vaccine-related <span class="hlt">images</span> that correlate with the higher likelihood of being retweeted. Methods We collected more than one million vaccine <span class="hlt">image</span> messages from Twitter and characterized various properties of these <span class="hlt">images</span> using automated <span class="hlt">image</span> analytics. We fit a logistic regression model to predict whether or not a vaccine <span class="hlt">image</span> tweet was retweeted, thus identifying characteristics that correlate with a higher likelihood of being shared. For comparison, we built similar models for the sharing of vaccine news on Facebook and for general <span class="hlt">image</span> tweets. Results Most vaccine-related <span class="hlt">images</span> are duplicates (125,916/237,478; 53.02%) or taken from other sources, not necessarily created by the author of the tweet. Almost half of the <span class="hlt">images</span> contain embedded text, and many include <span class="hlt">images</span> of people and syringes. The visual content is highly correlated with a tweet’s textual topics. Vaccine <span class="hlt">image</span> tweets are twice as likely to be shared as nonimage tweets. The sentiment of an <span class="hlt">image</span> and the objects shown in the <span class="hlt">image</span> were the predictive factors in determining whether an <span class="hlt">image</span> was retweeted. Conclusions We are the first to study vaccine <span class="hlt">images</span> on Twitter. Our findings suggest future directions for the study and use of vaccine imagery and may inform communication strategies around vaccination. Furthermore, our study demonstrates an effective study methodology for <span class="hlt">image</span> analysis. PMID:29615386</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/29615386','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/29615386"><span>Vaccine <span class="hlt">Images</span> on Twitter: Analysis of What <span class="hlt">Images</span> are Shared.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Chen, Tao; Dredze, Mark</p>
         <p>2018-04-03</p>
         <p>Visual imagery plays a key role in health communication; however, there is little understanding of what aspects of vaccine-related <span class="hlt">images</span> make them effective communication aids. Twitter, a popular venue for discussions related to vaccination, provides numerous <span class="hlt">images</span> that are shared with tweets. The objectives of this study were to understand how <span class="hlt">images</span> are used in vaccine-related tweets and provide guidance with respect to the characteristics of vaccine-related <span class="hlt">images</span> that correlate with the higher likelihood of being retweeted. We collected more than one million vaccine <span class="hlt">image</span> messages from Twitter and characterized various properties of these <span class="hlt">images</span> using automated <span class="hlt">image</span> analytics. We fit a logistic regression model to predict whether or not a vaccine <span class="hlt">image</span> tweet was retweeted, thus identifying characteristics that correlate with a higher likelihood of being shared. For comparison, we built similar models for the sharing of vaccine news on Facebook and for general <span class="hlt">image</span> tweets. Most vaccine-related <span class="hlt">images</span> are duplicates (125,916/237,478; 53.02%) or taken from other sources, not necessarily created by the author of the tweet. Almost half of the <span class="hlt">images</span> contain embedded text, and many include <span class="hlt">images</span> of people and syringes. The visual content is highly correlated with a tweet's textual topics. Vaccine <span class="hlt">image</span> tweets are twice as likely to be shared as nonimage tweets. The sentiment of an <span class="hlt">image</span> and the objects shown in the <span class="hlt">image</span> were the predictive factors in determining whether an <span class="hlt">image</span> was retweeted. We are the first to study vaccine <span class="hlt">images</span> on Twitter. Our findings suggest future directions for the study and use of vaccine imagery and may inform communication strategies around vaccination. Furthermore, our study demonstrates an effective study methodology for <span class="hlt">image</span> analysis. ©Tao Chen, Mark Dredze. Originally published in the Journal of Medical Internet Research (http://www.jmir.org), 03.04.2018.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=competitive+AND+intelligence&pg=3&id=EJ550911','ERIC'); return false;" href="https://eric.ed.gov/?q=competitive+AND+intelligence&pg=3&id=EJ550911"><span>Moving Multimedia: The Information Value in <span class="hlt">Images</span>.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Berinstein, Paula</p>
         <p>1997-01-01</p>
         <p>Discusses the value and use of <span class="hlt">images</span> as information. Topics include the information in <span class="hlt">images</span> versus text; a taxonomy of <span class="hlt">image</span> types; resources related to <span class="hlt">images</span>; and the use of <span class="hlt">images</span> in architecture, engineering, advertising, and competitive intelligence. (LRW)</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2018SPIE10696E..19F','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2018SPIE10696E..19F"><span>Toward a perceptual <span class="hlt">image</span> quality assessment of color quantized <span class="hlt">images</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Frackiewicz, Mariusz; Palus, Henryk</p>
         <p>2018-04-01</p>
         <p>Color <span class="hlt">image</span> quantization is an important operation in the field of color <span class="hlt">image</span> processing. In this paper, we consider new perceptual <span class="hlt">image</span> quality metrics for assessment of quantized <span class="hlt">images</span>. These types of metrics, e.g. DSCSI, MDSIs, MDSIm and HPSI achieve the highest correlation coefficients with MOS during tests on the six publicly available <span class="hlt">image</span> databases. Research was limited to <span class="hlt">images</span> distorted by two types of compression: JPG and JPG2K. Statistical analysis of correlation coefficients based on the Friedman test and post-hoc procedures showed that the differences between the four new perceptual metrics are not statistically significant.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/28720986','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/28720986"><span><span class="hlt">Imaging</span> investigations in Spine Trauma: The value of commonly used <span class="hlt">imaging</span> modalities and emerging <span class="hlt">imaging</span> modalities.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Tins, Bernhard J</p>
         <p>2017-01-01</p>
         <p>Traumatic spine injuries can be devastating for patients affected and for health care professionals if preventable neurological deterioration occurs. This review discusses the <span class="hlt">imaging</span> options for the diagnosis of spinal trauma. It lays out when <span class="hlt">imaging</span> is appropriate and when it is not. It discusses strength and weakness of available <span class="hlt">imaging</span> modalities. Advanced techniques for spinal injury <span class="hlt">imaging</span> will be explored. The review concludes with a review of <span class="hlt">imaging</span> protocols adjusted to clinical circumstances.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/21129633','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/21129633"><span>Basic concepts of MR <span class="hlt">imaging</span>, diffusion MR <span class="hlt">imaging</span>, and diffusion tensor <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>de Figueiredo, Eduardo H M S G; Borgonovi, Arthur F N G; Doring, Thomas M</p>
         <p>2011-02-01</p>
         <p>MR <span class="hlt">image</span> contrast is based on intrinsic tissue properties and specific pulse sequences and parameter adjustments. A growing number of MRI <span class="hlt">imaging</span> applications are based on diffusion properties of water. To better understand MRI diffusion-weighted <span class="hlt">imaging</span>, a brief overview of MR physics is presented in this article followed by physics of the evolving techniques of diffusion MR <span class="hlt">imaging</span> and diffusion tensor <span class="hlt">imaging</span>. Copyright © 2011. Published by Elsevier Inc.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=quantum+AND+medicine&id=EJ522196','ERIC'); return false;" href="https://eric.ed.gov/?q=quantum+AND+medicine&id=EJ522196"><span><span class="hlt">Imaging</span> the Working Brain.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Swithenby, S. J.</p>
         <p>1996-01-01</p>
         <p>Very sensitive SQUID (superconducting quantum interference device) detectors are used in the technique known as magnetoencephalography to provide dynamic <span class="hlt">images</span> of the brain. This can help our fundamental understanding of the way the brain works and may be of particular use in treating disorders such as epilepsy. (Author/MKR)</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/27917527','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/27917527"><span>Simultaneous orthogonal plane <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Mickevicius, Nikolai J; Paulson, Eric S</p>
         <p>2017-11-01</p>
         <p>Intrafraction motion can result in a smearing of planned external beam radiation therapy dose distributions, resulting in an uncertainty in dose actually deposited in tissue. The purpose of this paper is to present a pulse sequence that is capable of <span class="hlt">imaging</span> a moving target at a high frame rate in two orthogonal planes simultaneously for MR-guided radiotherapy. By balancing the zero gradient moment on all axes, slices in two orthogonal planes may be spatially encoded simultaneously. The orthogonal slice groups may be acquired with equal or nonequal echo times. A Cartesian spoiled gradient echo simultaneous orthogonal plane <span class="hlt">imaging</span> (SOPI) sequence was tested in phantom and in vivo. Multiplexed SOPI acquisitions were performed in which two parallel slices were <span class="hlt">imaged</span> along two orthogonal axes simultaneously. An autocalibrating phase-constrained 2D-SENSE-GRAPPA (generalized autocalibrating partially parallel acquisition) algorithm was implemented to reconstruct the multiplexed data. SOPI <span class="hlt">images</span> without intraslice motion artifacts were reconstructed at a maximum frame rate of 8.16 Hz. The 2D-SENSE-GRAPPA reconstruction separated the parallel slices aliased along each orthogonal axis. The high spatiotemporal resolution provided by SOPI has the potential to be beneficial for intrafraction motion management during MR-guided radiation therapy or other MRI-guided interventions. Magn Reson Med 78:1700-1710, 2017. © 2016 International Society for Magnetic Resonance in Medicine. © 2016 International Society for Magnetic Resonance in Medicine.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=Egan&pg=4&id=EJ821429','ERIC'); return false;" href="https://eric.ed.gov/?q=Egan&pg=4&id=EJ821429"><span>Imagination without <span class="hlt">Images</span></span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Manna, Concettina; Minichiello, Giuliano</p>
         <p>2005-01-01</p>
         <p>In the context of learning theories the problem of the passage from the psychological dimension governed by <span class="hlt">images</span> to the "scientific" dimension dominated by concepts needs to be reformulated. The starting point of the question should be recognition that at a "deep" level, between the two dimensions, there is a bridge, the design of which can be…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20080012382','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20080012382"><span>CCD <span class="hlt">imaging</span> sensors</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Janesick, James R. (Inventor); Elliott, Stythe T. (Inventor)</p>
         <p>1989-01-01</p>
         <p>A method for promoting quantum efficiency (QE) of a CCD <span class="hlt">imaging</span> sensor for UV, far UV and low energy x-ray wavelengths by overthinning the back side beyond the interface between the substrate and the photosensitive semiconductor material, and flooding the back side with UV prior to using the sensor for <span class="hlt">imaging</span>. This UV flooding promotes an accumulation layer of positive states in the oxide film over the thinned sensor to greatly increase QE for either frontside or backside illumination. A permanent or semipermanent <span class="hlt">image</span> (analog information) may be stored in a frontside SiO.sub.2 layer over the photosensitive semiconductor material using implanted ions for a permanent storage and intense photon radiation for a semipermanent storage. To read out this stored information, the gate potential of the CCD is biased more negative than that used for normal <span class="hlt">imaging</span>, and excess charge current thus produced through the oxide is integrated in the pixel wells for subsequent readout by charge transfer from well to well in the usual manner.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3404706','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3404706"><span><span class="hlt">Imaging</span> lung perfusion</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Wielpütz, Mark O.; Kauczor, Hans-Ulrich</p>
         <p>2012-01-01</p>
         <p>From the first measurements of the distribution of pulmonary blood flow using radioactive tracers by West and colleagues (J Clin Invest 40: 1–12, 1961) allowing gravitational differences in pulmonary blood flow to be described, the <span class="hlt">imaging</span> of pulmonary blood flow has made considerable progress. The researcher employing modern <span class="hlt">imaging</span> techniques now has the choice of several techniques, including magnetic resonance <span class="hlt">imaging</span> (MRI), computerized tomography (CT), positron emission tomography (PET), and single photon emission computed tomography (SPECT). These techniques differ in several important ways: the resolution of the measurement, the type of contrast or tag used to <span class="hlt">image</span> flow, and the amount of ionizing radiation associated with each measurement. In addition, the techniques vary in what is actually measured, whether it is capillary perfusion such as with PET and SPECT, or larger vessel information in addition to capillary perfusion such as with MRI and CT. Combined, these issues affect quantification and interpretation of data as well as the type of experiments possible using different techniques. The goal of this review is to give an overview of the techniques most commonly in use for physiological experiments along with the issues unique to each technique. PMID:22604884</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2007RPPh...70.1325C','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2007RPPh...70.1325C"><span><span class="hlt">Imaging</span> with terahertz radiation</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Chan, Wai Lam; Deibel, Jason; Mittleman, Daniel M.</p>
         <p>2007-08-01</p>
         <p>Within the last several years, the field of terahertz science and technology has changed dramatically. Many new advances in the technology for generation, manipulation, and detection of terahertz radiation have revolutionized the field. Much of this interest has been inspired by the promise of valuable new applications for terahertz <span class="hlt">imaging</span> and sensing. Among a long list of proposed uses, one finds compelling needs such as security screening and quality control, as well as whimsical notions such as counting the almonds in a bar of chocolate. This list has grown in parallel with the development of new technologies and new paradigms for <span class="hlt">imaging</span> and sensing. Many of these proposed applications exploit the unique capabilities of terahertz radiation to penetrate common packaging materials and provide spectroscopic information about the materials within. Several of the techniques used for terahertz <span class="hlt">imaging</span> have been borrowed from other, more well established fields such as x-ray computed tomography and synthetic aperture radar. Others have been developed exclusively for the terahertz field, and have no analogies in other portions of the spectrum. This review provides a comprehensive description of the various techniques which have been employed for terahertz <span class="hlt">image</span> formation, as well as discussing numerous examples which illustrate the many exciting potential uses for these emerging technologies.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=19940000017&hterms=image+processing&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D60%26Ntt%3Dimage%2Bprocessing','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=19940000017&hterms=image+processing&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D60%26Ntt%3Dimage%2Bprocessing"><span><span class="hlt">Image</span>-Processing Program</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Roth, D. J.; Hull, D. R.</p>
         <p>1994-01-01</p>
         <p>IMAGEP manipulates digital <span class="hlt">image</span> data to effect various processing, analysis, and enhancement functions. It is keyboard-driven program organized into nine subroutines. Within subroutines are sub-subroutines also selected via keyboard. Algorithm has possible scientific, industrial, and biomedical applications in study of flows in materials, analysis of steels and ores, and pathology, respectively.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=jungle&pg=5&id=EJ442828','ERIC'); return false;" href="https://eric.ed.gov/?q=jungle&pg=5&id=EJ442828"><span><span class="hlt">Images</span> of Leadership.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Bolman, Lee G.; Deal, Terrence E.</p>
         <p>1992-01-01</p>
         <p>Research suggests four different metaphors of schools as organizations: (1) factory; (2) family; (3) jungle; and (4) cathedral. Each implies a leadership role and an ethical responsibility for board members. Integrating all four <span class="hlt">images</span> might help provide a better map of the board's governance task and improve the board's ability to make wise…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://medlineplus.gov/ency/imagepages/9818.htm','NIH-MEDLINEPLUS'); return false;" href="https://medlineplus.gov/ency/imagepages/9818.htm"><span>Renal arteries (<span class="hlt">image</span>)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>... then injected into the renal artery through the catheter, and <span class="hlt">images</span> of the vessels of the kidney are taken. The test is a useful aid in evaluating kidney function and diagnosing any narrowing of the arteries, blood clots, tumors or aneurysms.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2012AIPC.1463..194Z','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2012AIPC.1463..194Z"><span>Wavelets in medical <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Zahra, Noor e.; Sevindir, Hulya Kodal; Aslan, Zafer; Siddiqi, A. H.</p>
         <p>2012-07-01</p>
         <p>The aim of this study is to provide emerging applications of wavelet methods to medical signals and <span class="hlt">images</span>, such as electrocardiogram, electroencephalogram, functional magnetic resonance <span class="hlt">imaging</span>, computer tomography, X-ray and mammography. Interpretation of these signals and <span class="hlt">images</span> are quite important. Nowadays wavelet methods have a significant impact on the science of medical <span class="hlt">imaging</span> and the diagnosis of disease and screening protocols. Based on our initial investigations, future directions include neurosurgical planning and improved assessment of risk for individual patients, improved assessment and strategies for the treatment of chronic pain, improved seizure localization, and improved understanding of the physiology of neurological disorders. We look ahead to these and other emerging applications as the benefits of this technology become incorporated into current and future patient care. In this chapter by applying Fourier transform and wavelet transform, analysis and denoising of one of the important biomedical signals like EEG is carried out. The presence of rhythm, template matching, and correlation is discussed by various method. Energy of EEG signal is used to detect seizure in an epileptic patient. We have also performed denoising of EEG signals by SWT.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2000SPIE.4132..356T','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2000SPIE.4132..356T"><span>Hyperspectral fundus <span class="hlt">imager</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Truitt, Paul W.; Soliz, Peter; Meigs, Andrew D.; Otten, Leonard John, III</p>
         <p>2000-11-01</p>
         <p>A Fourier Transform hyperspectral <span class="hlt">imager</span> was integrated onto a standard clinical fundus camera, a Zeiss FF3, for the purposes of spectrally characterizing normal anatomical and pathological features in the human ocular fundus. To develop this instrument an existing FDA approved retinal camera was selected to avoid the difficulties of obtaining new FDA approval. Because of this, several unusual design constraints were imposed on the optical configuration. Techniques to calibrate the sensor and to define where the hyperspectral pushbroom stripe was located on the retina were developed, including the manufacturing of an artificial eye with calibration features suitable for a spectral <span class="hlt">imager</span>. In this implementation the Fourier transform hyperspectral <span class="hlt">imager</span> can collect over a hundred 86 cm-1 spectrally resolved bands with 12 micro meter/pixel spatial resolution within the 1050 nm to 450 nm band. This equates to 2 nm to 8 nm spectral resolution depending on the wavelength. For retinal observations the band of interest tends to lie between 475 nm and 790 nm. The instrument has been in use over the last year successfully collecting hyperspectral <span class="hlt">images</span> of the optic disc, retinal vessels, choroidal vessels, retinal backgrounds, and macula diabetic macular edema, and lesions of age-related macular degeneration.</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li><a href="#" onclick='return showDiv("page_22");'>22</a></li>
      <li><a href="#" onclick='return showDiv("page_23");'>23</a></li>
      <li class="active"><span>24</span></li>
   <li><a href="#" onclick='return showDiv("page_25");'>25</a></li>
      <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_24 -->
   <div id="page_25" class="hiddenDiv">
       <div class="row">
            <div class="col-sm-12">
              <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li><a href="#" onclick='return showDiv("page_22");'>22</a></li>
      <li><a href="#" onclick='return showDiv("page_23");'>23</a></li>
      <li><a href="#" onclick='return showDiv("page_24");'>24</a></li>
      <li class="active"><span>25</span></li>
   <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div>
           </div>
           <div class="row">
             <div class="col-sm-12">
               <ol class="result-class" start="481">
      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2000AIPC..509..847Y','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2000AIPC..509..847Y"><span>Ultrasound shear wave <span class="hlt">imaging</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Ye, Shigong; Wu, Junru</p>
         <p>2000-05-01</p>
         <p>Shear wave propagation properties including phase velocity and attenuation coefficient are indispensable information in materials characterization and nondestructive evaluation. A computer controlled scanning shear-wave ultrasonic <span class="hlt">imaging</span> system has been developed. It consists of a pair of focusing broadband pvdf transducers of central frequency of 50 MHz immersed in distilled water. Shear waves in a solid specimen are generated by mode-conversion. When ultrasonic waves generated by one of the pvdf transducers impinge upon a solid specimen from water with angle of incidence of θ that is greater than θcr, the critical angle of the longitudinal wave in the solid, only shear waves can propagate in the solid and longitudinal waves become evanescent waves. The shear waves pass through the specimen and received by the other pvdf transducer. Meanwhile, the specimen was scanned by a stepped motor of a step of 10 μm. The system was used to generated shear waves amplitude and phase velocity <span class="hlt">images</span> of bone specimen of 1280 μm and they are compared with their longitudinal wave counterparts. The results have shown shear wave <span class="hlt">images</span> can provide additional shear modulus and shear viscous information that longitudinal waves can't provide. The lateral resolution of 60 μm was achieved using shear wave <span class="hlt">imaging</span> technique applied in bone sample.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA20138.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA20138.html"><span>Dawn HAMO <span class="hlt">Image</span> 75</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-12-11</p>
         <p>This view from NASA Dawn spacecraft shows high northern latitudes on Ceres. Dawn acquired the <span class="hlt">image</span> on Oct. 17, 2015, from an altitude of 915 miles 1,470 kilometers. It has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA20138</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA19979.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA19979.html"><span>Dawn HAMO <span class="hlt">Image</span> 37</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2015-10-14</p>
         <p>This <span class="hlt">image</span>, taken by NASA Dawn spacecraft on Sept. 20, 2015, shows a portion of the northern hemisphere of dwarf planet Ceres from an altitude of 915 miles 1,470 kilometers, and has a resolution of 450 feet 140 meters per pixel. http://photojournal.jpl.nasa.gov/catalog/PIA19979</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/12413555','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/12413555"><span><span class="hlt">Imaging</span> optical sensor arrays.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Walt, David R</p>
         <p>2002-10-01</p>
         <p><span class="hlt">Imaging</span> optical fibres have been etched to prepare microwell arrays. These microwells have been loaded with sensing materials such as bead-based sensors and living cells to create high-density sensor arrays. The extremely small sizes and volumes of the wells enable high sensitivity and high information content sensing capabilities.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://ntrs.nasa.gov/search.jsp?R=20000027670&hterms=images+mars&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D50%26Ntt%3Dimages%2Bmars','NASA-TRS'); return false;" href="https://ntrs.nasa.gov/search.jsp?R=20000027670&hterms=images+mars&qs=Ntx%3Dmode%2Bmatchall%26Ntk%3DAll%26N%3D0%26No%3D50%26Ntt%3Dimages%2Bmars"><span>MGS <span class="hlt">images</span> of Mars</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p></p>
         <p>1999-01-01</p>
         <p>The Mars Global Surveyor (MGS) camera captured <span class="hlt">images</span> of a pit formed when a straight-walled trough collapsed. The heart shaped pit is about 2.3 kilometers (1.4 miles) wide. It is located on the east flank of the Alba Patera volcano in northern Tharsis.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://images.nasa.gov/#/details-PIA02751.html','SCIGOVIMAGE-NASA'); return false;" href="https://images.nasa.gov/#/details-PIA02751.html"><span>Radar <span class="hlt">Image</span>, Hokkaido, Japan</span></a></p>
      <p><a target="_blank" href="https://images.nasa.gov/">NASA Image and Video Library</a></p>
      <p></p>
         <p>2000-05-18</p>
         <p>The southeast part of the island of Hokkaido, Japan, is an area dominated by volcanoes and volcanic caldera. The active Usu Volcano is at the lower right edge of the circular Lake Toya-Ko and near the center of the <span class="hlt">image</span>.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=use+AND+%22remote+sensing%22&pg=4&id=EJ474996','ERIC'); return false;" href="https://eric.ed.gov/?q=use+AND+%22remote+sensing%22&pg=4&id=EJ474996"><span><span class="hlt">Image</span> Processing for Teaching.</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Greenberg, R.; And Others</p>
         <p>1993-01-01</p>
         <p>The <span class="hlt">Image</span> Processing for Teaching project provides a powerful medium to excite students about science and mathematics, especially children from minority groups and others whose needs have not been met by traditional teaching. Using professional-quality software on microcomputers, students explore a variety of scientific data sets, including…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/29685554','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/29685554"><span>Overdiagnosis in <span class="hlt">imaging</span>.</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Díaz Candamio, M J; Jha, S; Martel Villagrán, J</p>
         <p>2018-04-21</p>
         <p>Overdiagnosis, more than an error regarding the diagnosis, is an error regarding the prognosis. We cannot know what consequences some lesions that we detect by <span class="hlt">imaging</span> would have on our patients' lives if they were left untreated. As long as it is not possible for <span class="hlt">imaging</span> techniques to differentiate between lesions that will have an indolent course from those that will have an aggressive course, there will be overdiagnosis. Advanced <span class="hlt">imaging</span> techniques, radiomics, and radiogenomics, together with artificial intelligence, promise advances in this sense. In the meantime, it is important that radiologists be careful to ensure that only strictly necessary <span class="hlt">imaging</span> tests are done. Moreover, we need to participate, together with patients, in making multidisciplinary decisions about diagnosis and clinical management. Finally, of course, we need to continue to contribute to the technological and scientific advance of our profession, so that we can continue to improve the diagnosis and early detection of abnormalities, especially those that require treatment. Copyright © 2018 SERAM. Publicado por Elsevier España, S.L.U. All rights reserved.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.osti.gov/servlets/purl/1044085','SCIGOV-STC'); return false;" href="https://www.osti.gov/servlets/purl/1044085"><span><span class="hlt">Image</span> processing and reconstruction</span></a></p>
      <p><a target="_blank" href="http://www.osti.gov/scitech">SciTech Connect</a></p>
      <p>Chartrand, Rick</p>
         <p>2012-06-15</p>
         <p>This talk will examine some mathematical methods for <span class="hlt">image</span> processing and the solution of underdetermined, linear inverse problems. The talk will have a tutorial flavor, mostly accessible to undergraduates, while still presenting research results. The primary approach is the use of optimization problems. We will find that relaxing the usual assumption of convexity will give us much better results.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/1997SPD....28.1202S','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/1997SPD....28.1202S"><span><span class="hlt">Image</span> Improvement Techniques</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Shine, R. A.</p>
         <p>1997-05-01</p>
         <p>Over the last decade, a repertoire of techniques have been developed and/or refined to improve the quality of high spatial resolution solar movies taken from ground based observatories. These include real time <span class="hlt">image</span> motion corrections, frame selection, phase diversity measurements of the wavefront, and extensive post processing to partially remove atmospheric distortion. Their practical application has been made possible by the increasing availability and decreasing cost of large CCD's with fast digital readouts and high speed computer workstations with large memories. Most successful have been broad band (0.3 to 10 nm) filtergram movies which can use exposure times of 10 to 30 ms, short enough to ``freeze'' atmospheric motions. Even so, only a handful of movies with excellent <span class="hlt">image</span> quality for more than a hour have been obtained to date. Narrowband filtergrams (about 0.01 nm), such as those required for constructing magnetograms and Dopplergrams, have been more challenging although some single <span class="hlt">images</span> approach the quality of the best continuum <span class="hlt">images</span>. Some promising new techniques and instruments, together with persistence and good luck, should continue the progress made in the last several years.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2016SPIE.9819E..0MC','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2016SPIE.9819E..0MC"><span>Pyxis handheld polarimetric <span class="hlt">imager</span></span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Chenault, David B.; Pezzaniti, J. Larry; Vaden, Justin P.</p>
         <p>2016-05-01</p>
         <p>The instrumentation for measuring infrared polarization signatures has seen significant advancement over the last decade. Previous work has shown the value of polarimetric imagery for a variety of target detection scenarios including detection of manmade targets in clutter and detection of ground and maritime targets while recent work has shown improvements in contrast for aircraft detection and biometric markers. These data collection activities have generally used laboratory or prototype systems with limitations on the allowable amount of target motion or the sensor platform and usually require an attached computer for data acquisition and processing. Still, performance and sensitivity have been steadily getting better while size, weight, and power requirements have been getting smaller enabling polarimetric <span class="hlt">imaging</span> for a greater or real world applications. In this paper, we describe Pyxis®, a microbolometer based <span class="hlt">imaging</span> polarimeter that produces live polarimetric video of conventional, polarimetric, and fused <span class="hlt">image</span> products. A polarization microgrid array integrated in the optical system captures all polarization states simultaneously and makes the system immune to motion artifacts of either the sensor or the scene. The system is battery operated, rugged, and weighs about a quarter pound, and can be helmet mounted or handheld. On board processing of polarization and fused <span class="hlt">image</span> products enable the operator to see polarimetric signatures in real time. Both analog and digital outputs are possible with sensor control available through a tablet interface. A top level description of Pyxis® is given followed by performance characteristics and representative data.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://familydoctor.org/magnetic-resonance-imaging-mri/?adfree=true','NIH-MEDLINEPLUS'); return false;" href="https://familydoctor.org/magnetic-resonance-imaging-mri/?adfree=true"><span>Magnetic Resonance <span class="hlt">Imaging</span> (MRI)</span></a></p>
      <p><a target="_blank" href="http://medlineplus.gov/">MedlinePlus</a></p>
      <p></p>
         <p></p>
         <p>... MoreBMI Calculator Complete Blood Count (CBC)Blood Test: Lipid PanelRapid Strep TestPelvic UltrasoundAbdominal UltrasoundCT Head ScanPap Smear ( ... because it can provide <span class="hlt">images</span> of internal body structures. It is more like a CT scan than ...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/19950002787','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/19950002787"><span>Picosecond <span class="hlt">imaging</span> of sprays</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Breisacher, Kevin; Liou, Larry; Wang, L.; Liang, X.; Galland, P.; Ho, P. P.; Alfano, R. R.</p>
         <p>1994-01-01</p>
         <p>Preliminary results from applying a Kerr-Fourier <span class="hlt">imaging</span> system to a water/air spray produced by a shear coaxial element are presented. The physics behind ultrafast time-gated optical techniques is discussed briefly. A typical setup of a Kerr-Fourier time gating system is presented.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3760773','PMC'); return false;" href="https://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pmcentrez&artid=3760773"><span><span class="hlt">Imaging</span> the Alzheimer Brain</span></a></p>
      <p><a target="_blank" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pmc">PubMed Central</a></p>
      <p>Ashford, J. Wesson; Salehi, Ahmad; Furst, Ansgar; Bayley, Peter; Frisoni, Giovanni B.; Jack, Clifford R.; Sabri, Osama; Adamson, Maheen M.; Coburn, Kerry L.; Olichney, John; Schuff, Norbert; Spielman, Daniel; Edland, Steven D.; Black, Sandra; Rosen, Allyson; Kennedy, David; Weiner, Michael; Perry, George</p>
         <p>2013-01-01</p>
         <p>This supplement to the Journal of Alzheimer's Disease contains more than half of the chapters from The Handbook of <span class="hlt">Imaging</span> the Alzheimer Brain, which was first presented at the International Conference on Alzheimer's Disease in Paris, in July, 2011. While the Handbook contains 27 chapters that are modified articles from 2009, 2010, and 2011 issues of the Journal of Alzheimer's Disease, this supplement contains the 31 new chapters of that book and an introductory article drawn from the introductions to each section of the book. The Handbook was designed to provide a multilevel overview of the full field of brain <span class="hlt">imaging</span> related to Alzheimer's disease (AD). The Handbook, as well as this supplement, contains both reviews of the basic concepts of <span class="hlt">imaging</span>, the latest developments in <span class="hlt">imaging</span>, and various discussions and perspectives of the problems of the field and promising directions. The Handbook was designed to be useful for students and clinicians interested in AD as well as scientists studying the brain and pathology related to AD. PMID:21971448</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=cinema&id=EJ1077428','ERIC'); return false;" href="https://eric.ed.gov/?q=cinema&id=EJ1077428"><span>Making <span class="hlt">Images</span> That Move</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Rennie, Richard</p>
         <p>2015-01-01</p>
         <p>The history of the moving <span class="hlt">image</span> (the cinema) is well documented in books and on the Internet. This article offers a number of activities that can easily be carried out in a science class. They make use of the phenomenon of "Persistence of Vision." The activities presented herein demonstrate the functionality of the phenakistoscope, the…</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://cfpub.epa.gov/si/si_public_record_report.cfm?dirEntryId=237789&Lab=NRMRL&keyword=example+AND+study+AND+applied+AND+research&actType=&TIMSType=+&TIMSSubTypeID=&DEID=&epaNumber=&ntisID=&archiveStatus=Both&ombCat=Any&dateBeginCreated=&dateEndCreated=&dateBeginPublishedPresented=&dateEndPublishedPresented=&dateBeginUpdated=&dateEndUpdated=&dateBeginCompleted=&dateEndCompleted=&personID=&role=Any&journalID=&publisherID=&sortBy=revisionDate&count=50','EPA-EIMS'); return false;" href="https://cfpub.epa.gov/si/si_public_record_report.cfm?dirEntryId=237789&Lab=NRMRL&keyword=example+AND+study+AND+applied+AND+research&actType=&TIMSType=+&TIMSSubTypeID=&DEID=&epaNumber=&ntisID=&archiveStatus=Both&ombCat=Any&dateBeginCreated=&dateEndCreated=&dateBeginPublishedPresented=&dateEndPublishedPresented=&dateBeginUpdated=&dateEndUpdated=&dateBeginCompleted=&dateEndCompleted=&personID=&role=Any&journalID=&publisherID=&sortBy=revisionDate&count=50"><span>Electrical Resistivity <span class="hlt">Imaging</span></span></a></p>
      <p><a target="_blank" href="http://oaspub.epa.gov/eims/query.page">EPA Science Inventory</a></p>
      <p></p>
         <p></p>
         <p>Electrical resistivity <span class="hlt">imaging</span> (ERI) is a geophysical method originally developed within the mining industry where it has been used for decades to explore for and characterize subsurface mineral deposits. It is one of the oldest geophysical methods with the first documented usag...</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://eric.ed.gov/?q=ghosts&id=EJ985118','ERIC'); return false;" href="https://eric.ed.gov/?q=ghosts&id=EJ985118"><span><span class="hlt">Images</span> in the Air</span></a></p>
      <p><a target="_blank" href="http://www.eric.ed.gov/ERICWebPortal/search/extended.jsp?_pageLabel=advanced">ERIC Educational Resources Information Center</a></p>
      <p>Riveros, H. G.; Rosenberger, Franz</p>
         <p>2012-01-01</p>
         <p>This article discusses two "magic tricks" in terms of underlying optical principles. The first trick is new and produces a "ghost" in the air, and the second is the classical real <span class="hlt">image</span> produced with two parabolic mirrors. (Contains 2 figure and 6 photos.)</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('https://www.ncbi.nlm.nih.gov/pubmed/23789334','PUBMED'); return false;" href="https://www.ncbi.nlm.nih.gov/pubmed/23789334"><span>[Progress in <span class="hlt">imaging</span> techniques].</span></a></p>
      <p><a target="_blank" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?DB=pubmed">PubMed</a></p>
      <p>Mishima, Kazuaki; Otsuka, Tsukasa</p>
         <p>2013-05-01</p>
         <p>Today it is common to perform real-time diagnosis and treatment via live broadcast as a method of education and to spread new technology for diagnosis and therapy in medical fields. Live medical broadcasts have developed along with broadcast technology. In the early days, live video feeds were sent from operating rooms to classrooms and lecture halls in universities and hospitals. However, the development of <span class="hlt">imaging</span> techniques and communication networks enabled live broadcasts that bi-directionally link operating rooms and meeting halls during scientific meetings and live demonstration courses. Live broadcasts therefore became an important method for education and the dissemination of new medical technologies. The development of <span class="hlt">imaging</span> techniques has contributed to more realistic live broadcasts through such innovative techniques as three-dimensional viewing and higher-definition 4K technology. In the future, live broadcasts will be transmitted on personal computers using regular Internet connections. In addition to the enhancement of <span class="hlt">image</span> delivery technology, it will also be necessary to examine the entire <span class="hlt">image</span> delivery environment carefully, including issues of security and privacy of personal information.</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://adsabs.harvard.edu/abs/2017SPIE10563E..55B','NASAADS'); return false;" href="http://adsabs.harvard.edu/abs/2017SPIE10563E..55B"><span>Wedge filter <span class="hlt">imaging</span> spectrometer</span></a></p>
      <p><a target="_blank" href="http://adsabs.harvard.edu/abstract_service.html">NASA Astrophysics Data System (ADS)</a></p>
      <p>Bernardi, Pernelle; Bonafous, M.; Motisi, M.; Reess, J.-M.; Tanrin, J.; Laubier, D.</p>
         <p>2017-11-01</p>
         <p>LESIA (Laboratoire d'Etudes Spatiales et d'Instrumentation en Astrophysique, Observatoire de Paris-Meudon) has an extensive experience in visible and infrared <span class="hlt">imaging</span> spectrometry with several instruments onboard planetary space missions (MarsExpress/OMEGA, VenusExpress/VIRTIS, Rosetta/VIRTIS).</p>
      </li>

      <li>
      <p><a target="_blank" onclick="trackOutboundLink('http://hdl.handle.net/2060/20080004241','NASA-TRS'); return false;" href="http://hdl.handle.net/2060/20080004241"><span>Spacecraft camera <span class="hlt">image</span> registration</span></a></p>
      <p><a target="_blank" href="http://ntrs.nasa.gov/search.jsp">NASA Technical Reports Server (NTRS)</a></p>
      <p>Kamel, Ahmed A. (Inventor); Graul, Donald W. (Inventor); Chan, Fred N. T. (Inventor); Gamble, Donald W. (Inventor)</p>
         <p>1987-01-01</p>
         <p>A system for achieving spacecraft camera (1, 2) <span class="hlt">image</span> registration comprises a portion external to the spacecraft and an <span class="hlt">image</span> motion compensation system (IMCS) portion onboard the spacecraft. Within the IMCS, a computer (38) calculates an <span class="hlt">image</span> registration compensation signal (60) which is sent to the scan control loops (84, 88, 94, 98) of the onboard cameras (1, 2). At the location external to the spacecraft, the long-term orbital and attitude perturbations on the spacecraft are modeled. Coefficients (K, A) from this model are periodically sent to the onboard computer (38) by means of a command unit (39). The coefficients (K, A) take into account observations of stars and landmarks made by the spacecraft cameras (1, 2) themselves. The computer (38) takes as inputs the updated coefficients (K, A) plus synchronization information indicating the mirror position (AZ, EL) of each of the spacecraft cameras (1, 2), operating mode, and starting and stopping status of the scan lines generated by these cameras (1, 2), and generates in response thereto the <span class="hlt">image</span> registration compensation signal (60). The sources of periodic thermal errors on the spacecraft are discussed. The system is checked by calculating measurement residuals, the difference between the landmark and star locations predicted at the external location and the landmark and star locations as measured by the spacecraft cameras (1, 2).</p>
      </li>

      </ol>  
       <div class="pull-right">
          <ul class="pagination">

<li><a href="#" onclick='return showDiv("page_1");'>&laquo;</a></li>

<li><a href="#" onclick='return showDiv("page_21");'>21</a></li>
      <li><a href="#" onclick='return showDiv("page_22");'>22</a></li>
      <li><a href="#" onclick='return showDiv("page_23");'>23</a></li>
      <li><a href="#" onclick='return showDiv("page_24");'>24</a></li>
      <li class="active"><span>25</span></li>
   <li><a href="#" onclick='return showDiv("page_25");'>&raquo;</a></li>

          </ul>
        </div>
</div><!-- col-sm-12 -->
     </div><!-- row -->
   </div><!-- page_25 -->
   <center>
    <div class="footer-extlink text-muted"><small>Some links on this page may take you to non-federal websites. Their policies may differ from this site.</small>
    </div>
    </center>

    <div id="footer-wrapper">
      <div class="footer-content">
        <div id="footerOSTI" class="">
          <div class="row">
            <div class="col-md-4 text-center col-md-push-4 footer-content-center"><small><a href="http://www.science.gov/disclaimer.html">Privacy and Security</a></small>
              <div class="visible-sm visible-xs push_footer"></div>
            </div>
            <div class="col-md-4 text-center col-md-pull-4 footer-content-left">
              <img src="https://www.osti.gov/images/DOE_SC31.png" alt="U.S. Department of Energy" usemap="#doe" height="31" width="177"><map style="display:none;" name="doe" id="doe"><area shape="rect" coords="1,3,107,30" href="http://www.energy.gov" alt="U.S. Deparment of Energy"><area shape="rect" coords="114,3,165,30" href="http://www.science.energy.gov" alt="Office of Science"></map>
	      <a ref="http://www.osti.gov" style="margin-left: 15px;"><img src="https://www.osti.gov/images/footerimages/ostigov53.png" alt="Office of Scientific and Technical Information" height="31" width="53"></a>
	      <div class="visible-sm visible-xs push_footer"></div>
            </div>
            <div class="col-md-4 text-center footer-content-right">
	      <a href="http://www.science.gov"><img src="https://www.osti.gov/images/footerimages/scigov77.png" alt="science.gov" height="31" width="98"></a>
	      <a href="http://worldwidescience.org"><img src="https://www.osti.gov/images/footerimages/wws82.png" alt="WorldWideScience.org" height="31" width="90"></a>
            </div>
          </div>
        </div>
      </div>
    </div>
    <p><br></p>
  </div><!-- container -->
  <script type="text/javascript"><!--
    // var lastDiv = "";
    function showDiv(divName) {
      // hide last div
      if (lastDiv) {
        document.getElementById(lastDiv).className = "hiddenDiv";
      }
      //if value of the box is not nothing and an object with that name exists, then change the class
      if (divName && document.getElementById(divName)) {
        document.getElementById(divName).className = "visibleDiv";
        lastDiv = divName;
      }
    }
  //-->
  </script>
<script>
  /**
  * Function that tracks a click on an outbound link in Google Analytics.
  * This function takes a valid URL string as an argument, and uses that URL string
  * as the event label.
  */
  var trackOutboundLink = function(url,collectionCode) {
     try {
        h = window.open(url); 
            setTimeout(function() { 
              ga('send', 'event', 'topic-page-click-through', collectionCode, url);
            }, 1000); 
         }
     catch(err){} 
  };
  </script>
  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-1122789-34', 'auto');
  ga('send', 'pageview');
  </script>
  <!-- End Google Analytics -->
<script>
showDiv('page_1')
</script>
</body>
</html>
